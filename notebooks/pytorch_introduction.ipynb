{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6oqGiIXvrMl"
      },
      "source": [
        "## NPFL129 PyTorch Tutorial\n",
        "(link to this notebook: http://bit.ly/3LyZ2uV)\n",
        "\n",
        "\n",
        "### Based on:\n",
        "* Dilara Soylu, Ethan Chi, \"CS224N: PyTorch Tutorial (Winter '24)\", (https://colab.research.google.com/github/ryanyuchen/NLP-Pytorch/blob/main/CS224N_PyTorch_Tutorial.ipynb)\n",
        "\n",
        "In this notebook, we will have a basic introduction to `PyTorch` and work on a toy NLP task. Following resources have been used in preparation of this notebook:\n",
        "* Official PyTorch Documentation on [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) by Soumith Chintala\n",
        "* PyTorch Tutorial Notebook, [Build Basic Generative Adversarial Networks (GANs) | Coursera](https://www.coursera.org/learn/build-basic-generative-adversarial-networks-gans) by Sharon Zhou, offered on Coursera"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Please make a copy into your own Drive!"
      ],
      "metadata": {
        "id": "s2jvK3g_2zrK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gk1UKaNvrMv"
      },
      "source": [
        "## Introduction\n",
        "[PyTorch](https://pytorch.org/) is a deep learning framework, one of the two main frameworks alongside [TensorFlow](https://www.tensorflow.org/) or [Theano](https://en.wikipedia.org/wiki/Theano_(software)). The installation can be done via Pip or Conda, as described [here](https://pytorch.org/). Let's start by importing PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0ukr7quvrMx"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Import pprint, module we use for making our print statements prettier\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k10ZRdcBwDP3"
      },
      "source": [
        "We are all set to start our tutorial. Let's dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLdSN9ZXvrM0"
      },
      "source": [
        "##Part 1: Tensors\n",
        "\n",
        "**Tensors** are PyTorch's most basic building block. Each tensor is a multi-dimensional matrix; for example, a 256x256 square image might be represented by a `3x256x256` tensor, where the first dimension represents color. Here's how to create a tensor:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_lists = [\n",
        "  [1, 2, 3],\n",
        "  [4, 5, 6],\n",
        "]\n",
        "print(list_of_lists)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXD7cTLTh4oF",
        "outputId": "c1ec3e5c-279e-4b9f-d2d0-c8c33205cca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3], [4, 5, 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing a tensor\n",
        "data = torch.tensor(list_of_lists)\n",
        "print(data)\n",
        "print(data.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2u7NOqWNJ7c",
        "outputId": "4ffd52b1-fded-4741-d245-423e895a17f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each tensor has a **data type**: the major data types you'll need to worry about are floats (`torch.float32`) and integers (`torch.int`). You can specify the data type explicitly when you create the tensor:"
      ],
      "metadata": {
        "id": "1i7vrR1_oO4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing a tensor with an explicit data type\n",
        "# Notice the dots after the numbers, which specify that they're floats\n",
        "data = torch.tensor(list_of_lists, dtype=torch.float32)\n",
        "print(data)\n",
        "print(data.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7nMdqgMoLOa",
        "outputId": "2a184b81-ea51-45e6-cd73-3fc6732fb35c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensor data type is chosen automatically to\n",
        "# Notice the dots after the numbers, which specify that they're floats\n",
        "other_list_of_lists = [\n",
        "                  [0.11111111, 1],\n",
        "                  [2, 3],\n",
        "                  [4, 5]\n",
        "                ]\n",
        "data = torch.tensor(other_list_of_lists)\n",
        "print(data)\n",
        "print(data.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4vr6EYbiOEJ",
        "outputId": "03bb7c2b-5d0d-4b73-8f3d-f4ffd57c20b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1111, 1.0000],\n",
            "        [2.0000, 3.0000],\n",
            "        [4.0000, 5.0000]])\n",
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You do not need to explicitly cast the  tensors, PyTorch handles that for you\n",
        "const = torch.tensor([5], dtype=torch.int32)\n",
        "result = data * const\n",
        "print(result)\n",
        "print(result.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnB8J23WiUTi",
        "outputId": "de300583-b31a-443b-b4fa-f084428d134d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.5556,  5.0000],\n",
            "        [10.0000, 15.0000],\n",
            "        [20.0000, 25.0000]])\n",
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can recast the result manually\n",
        "result_int = result.type(torch.int32)\n",
        "print(result_int)\n",
        "print(result_int.dtype)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2L6Mw5-JIr9",
        "outputId": "2d23ba28-b6e5-41cb-cafb-0b2321b2b5fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  5],\n",
            "        [10, 15],\n",
            "        [20, 25]], dtype=torch.int32)\n",
            "torch.int32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also cast to a dtype of an existing tensor\n",
        "result_float = result_int.type_as(data)\n",
        "print(result_float)\n",
        "print(result_float.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFIp-uEJJT0U",
        "outputId": "44272727-3de3-41b1-aa7a-8eabc33d5a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.,  5.],\n",
            "        [10., 15.],\n",
            "        [20., 25.]])\n",
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also inter-convert tensors with **NumPy arrays**:"
      ],
      "metadata": {
        "id": "GaykBuhoou3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# numpy.ndarray --> torch.Tensor:\n",
        "arr = np.array([[1, 0, 5]])\n",
        "data = torch.tensor(arr)\n",
        "print(\"This is a torch.tensor\", data)\n",
        "\n",
        "# torch.Tensor --> numpy.ndarray:\n",
        "new_arr = data.numpy()\n",
        "print(\"This is a np.ndarray\", new_arr)"
      ],
      "metadata": {
        "id": "ppYiPnlko1Ci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c7ef03-fb8d-46bd-9ff8-7204f86384f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a torch.tensor tensor([[1, 0, 5]])\n",
            "This is a np.ndarray [[1 0 5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility functions also exist to create tensors with given shapes and contents:"
      ],
      "metadata": {
        "id": "aiGCmTsrpkP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zeros = torch.zeros(2, 5)  # a tensor of all zeros\n",
        "print(zeros)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tn-N-z_qYj0",
        "outputId": "4647b723-1941-42ea-d422-ca765d7ece59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ones = torch.ones(3, 4)   # a tensor of all ones\n",
        "print(ones)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue26M5Npqoe3",
        "outputId": "6fd89975-0879-4597-e245-c5c2a0bbe299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rr = torch.arange(1, 10) # range from [1, 10)\n",
        "print(rr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdvaujtvqokI",
        "outputId": "1cd302da-c59c-4d49-e6a1-60a637930f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rr + 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK3q3LRHipHB",
        "outputId": "825abe20-913c-48cc-a5eb-51c99fe72868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 3,  4,  5,  6,  7,  8,  9, 10, 11])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rr ** 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDQ-v6AFiyco",
        "outputId": "39182058-ec5a-4bcf-9d24-6fa5e4bf1aa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1,  4,  9, 16, 25, 36, 49, 64, 81])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1, 2], [2, 3], [4, 5]])      # (3, 2)\n",
        "b = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])  # (2, 4)\n",
        "\n",
        "print(\"A is\", a)\n",
        "print(\"B is\", b)\n",
        "print(\"The product is\", a.matmul(b)) #(3, 4)\n",
        "print(\"The other product is\", a @ b) # +, -, *, @"
      ],
      "metadata": {
        "id": "OFcjwshvi3pC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c29fc9b-229d-4a36-cad9-8a7bd6d6e10b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A is tensor([[1, 2],\n",
            "        [2, 3],\n",
            "        [4, 5]])\n",
            "B is tensor([[1, 2, 3, 4],\n",
            "        [5, 6, 7, 8]])\n",
            "The product is tensor([[11, 14, 17, 20],\n",
            "        [17, 22, 27, 32],\n",
            "        [29, 38, 47, 56]])\n",
            "The other product is tensor([[11, 14, 17, 20],\n",
            "        [17, 22, 27, 32],\n",
            "        [29, 38, 47, 56]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **shape** of a matrix (which can be accessed by `.shape` attribute or the `.size()` method) is defined as the dimensions of the matrix. Here's some examples:"
      ],
      "metadata": {
        "id": "SCS5z1lip9lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matr_2d = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "# print the shape of a tensor\n",
        "print(matr_2d.shape)\n",
        "print(matr_2d.size())\n",
        "\n",
        "# print the size of a tensor dimension\n",
        "print(matr_2d.shape[0])\n",
        "print(matr_2d.size(0))\n",
        "\n",
        "print(matr_2d)"
      ],
      "metadata": {
        "id": "fMptIpZkq0da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "528b76c1-8b80-4837-aedf-3030981351e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3])\n",
            "torch.Size([2, 3])\n",
            "2\n",
            "2\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matr_3d = torch.tensor([[[1, 2, 3, 4], [-2, 5, 6, 9]], [[5, 6, 7, 2], [8, 9, 10, 4]], [[-3, 2, 2, 1], [4, 6, 5, 9]]])\n",
        "print(matr_3d)\n",
        "print(matr_3d.shape)"
      ],
      "metadata": {
        "id": "aWjD7WjPqC6t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be16eb84-a884-486e-803e-0e7de5bf0a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1,  2,  3,  4],\n",
            "         [-2,  5,  6,  9]],\n",
            "\n",
            "        [[ 5,  6,  7,  2],\n",
            "         [ 8,  9, 10,  4]],\n",
            "\n",
            "        [[-3,  2,  2,  1],\n",
            "         [ 4,  6,  5,  9]]])\n",
            "torch.Size([3, 2, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reshaping** tensors can be used to make batch operations easier (more on that later), but be careful that the data is reshaped in the order you expect:"
      ],
      "metadata": {
        "id": "7wKqP85rrF-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rr = torch.arange(1, 16)\n",
        "print(\"The shape is currently\", rr.shape)\n",
        "print(\"The contents are currently\", rr)\n",
        "print()\n",
        "rr = rr.view(5, 3)\n",
        "print(\"After reshaping, the shape is currently\", rr.shape)\n",
        "print(\"The contents are currently:\\n\", rr)"
      ],
      "metadata": {
        "id": "HmUcqHYUrMu1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88a6c306-5e10-404f-8bc9-5abfa004a0f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape is currently torch.Size([15])\n",
            "The contents are currently tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
            "\n",
            "After reshaping, the shape is currently torch.Size([5, 3])\n",
            "The contents are currently:\n",
            " tensor([[ 1,  2,  3],\n",
            "        [ 4,  5,  6],\n",
            "        [ 7,  8,  9],\n",
            "        [10, 11, 12],\n",
            "        [13, 14, 15]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we want to order the data in a vertical listing, we need a help of the .transpose() method\n",
        "rr = torch.arange(1, 16)\n",
        "print(\"1d contents\", rr)\n",
        "rr = rr.view(3, 5)\n",
        "print(\"Reshape to a 'transposed' shape:\\n\", rr)\n",
        "rr = rr.transpose(0, 1)\n",
        "print(\"Transpose to get the desired shape and ordering:\\n\", rr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7383eAwsK5QW",
        "outputId": "c6119634-6766-47e0-9e72-3ef6f1e6c3c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1d contents tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
            "Reshape to a 'transposed' shape:\n",
            " tensor([[ 1,  2,  3,  4,  5],\n",
            "        [ 6,  7,  8,  9, 10],\n",
            "        [11, 12, 13, 14, 15]])\n",
            "Transpose to get the desired shape and ordering:\n",
            " tensor([[ 1,  6, 11],\n",
            "        [ 2,  7, 12],\n",
            "        [ 3,  8, 13],\n",
            "        [ 4,  9, 14],\n",
            "        [ 5, 10, 15]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For a `N`-dimensional tensor, you only need to specify sizes of `N-1` dimensions\n",
        "rr = torch.arange(1, 17)\n",
        "print(\"1d contents\", rr)\n",
        "rr = rr.view(4, -1, 2)\n",
        "print(\"Reshaped contents\", rr)\n",
        "print(\"Reshaped shape\", rr.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClS5QFBnuz2O",
        "outputId": "e0e20a6f-13a5-4085-e3ca-53d62238aa39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1d contents tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])\n",
            "Reshaped contents tensor([[[ 1,  2],\n",
            "         [ 3,  4]],\n",
            "\n",
            "        [[ 5,  6],\n",
            "         [ 7,  8]],\n",
            "\n",
            "        [[ 9, 10],\n",
            "         [11, 12]],\n",
            "\n",
            "        [[13, 14],\n",
            "         [15, 16]]])\n",
            "Reshaped shape torch.Size([4, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the reasons why we use **tensors** is *vectorized operations*: operations that be conducted in parallel over a particular dimension of a tensor."
      ],
      "metadata": {
        "id": "hyv1l431q9yA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.arange(1, 36, dtype=torch.float32).view(5, 7)\n",
        "#data = torch.ones(5, 7)\n",
        "print(\"Data is:\", data)\n",
        "\n",
        "# We can perform operations like *sum* over each row...\n",
        "print(\"Taking the sum over rows:\")\n",
        "print(data.sum(dim=1))\n",
        "\n",
        "# or over each column.\n",
        "print(\"Taking thep sum over columns:\")\n",
        "print(data.sum(dim=0))\n",
        "\n",
        "# Other operations are available:\n",
        "print(\"Taking the stdev over rows:\")\n",
        "print(data.std(dim=1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kas2MEFDsJWk",
        "outputId": "daf5d9b6-735a-4e30-be8e-168c471ed259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data is: tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11., 12., 13., 14.],\n",
            "        [15., 16., 17., 18., 19., 20., 21.],\n",
            "        [22., 23., 24., 25., 26., 27., 28.],\n",
            "        [29., 30., 31., 32., 33., 34., 35.]])\n",
            "Taking the sum over rows:\n",
            "tensor([ 28.,  77., 126., 175., 224.])\n",
            "Taking thep sum over columns:\n",
            "tensor([ 75.,  80.,  85.,  90.,  95., 100., 105.])\n",
            "Taking the stdev over rows:\n",
            "tensor([2.1602, 2.1602, 2.1602, 2.1602, 2.1602])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.arange(1, 7, dtype=torch.float32).view(1, 2, 3)\n",
        "print(data.sum(dim=0).sum(dim=0))\n",
        "print(data.sum(dim=0).sum(dim=0).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1ByKJL_WYWv",
        "outputId": "c067ffa9-b338-49ac-a79d-5c3b425cd6eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5., 7., 9.])\n",
            "torch.Size([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.sum()"
      ],
      "metadata": {
        "id": "NPRy-xtuk2tK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10b55f63-e15e-4b96-9a92-7e7b8cf46bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(21.)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Write code that creates a `torch.tensor` with the following contents:\n",
        "$\\begin{bmatrix} 1 & 2.2 & 9.6 \\\\ 4 & -7.2 & 6.3 \\end{bmatrix}$\n",
        "\n",
        "Now compute the average of each row (`.mean()`) and each column.\n",
        "\n",
        "What's the shape of the results?\n",
        "\n"
      ],
      "metadata": {
        "id": "IJ8MjWEMxOVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = torch.tensor([[1, 2.2, 9.6], [4, -7.2, 6.3]])\n",
        "m.mean(1)"
      ],
      "metadata": {
        "id": "BK0YInGkn3Xy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ee172e-e7c2-4d73-b686-33c52319a97d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4.2667, 1.0333])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7BMktFFAkRA"
      },
      "source": [
        "**Indexing**\n",
        "\n",
        "You can access arbitrary elements of a tensor using the `[]` operator. A special `:` index can be used to get all elements from that specific dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRJN7ovWDsKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b59241a-7351-4124-843d-ac50d95a3c2f"
      },
      "source": [
        "# Initialize an example tensor\n",
        "x = torch.arange(1, 13, dtype=torch.float32).view(3, 2, 2)\n",
        "print(x)\n",
        "print(x.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.,  2.],\n",
            "         [ 3.,  4.]],\n",
            "\n",
            "        [[ 5.,  6.],\n",
            "         [ 7.,  8.]],\n",
            "\n",
            "        [[ 9., 10.],\n",
            "         [11., 12.]]])\n",
            "torch.Size([3, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guXKE7m8AX1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ebd42a5-e089-408c-854a-8510189ca2bf"
      },
      "source": [
        "# Access the 0th element, which is the first row\n",
        "print(x[0]) # Equivalent to x[0, :] or x[0, :, :]\n",
        "\n",
        "print(x[0, :])\n",
        "\n",
        "print(x[0, :, :])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n",
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n",
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8m8EyVvES4-"
      },
      "source": [
        "We can also index into multiple dimensions with `:`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we want to extract 0th element from the \"column\", we need to explicitly state to take all rows\n",
        "x[:, 0]"
      ],
      "metadata": {
        "id": "zn4pW2rkmXuj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee3772d4-accd-4cdf-a071-ebd4cfa32dac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.,  2.],\n",
              "        [ 5.,  6.],\n",
              "        [ 9., 10.]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z6GFUcuEL85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3475846e-df34-4387-f667-738bc9f785f5"
      },
      "source": [
        "# Get the top left element of each element in our tensor\n",
        "x[:, 0, 0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 5., 9.])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)\n",
        "print(x[:, :, :])  # this is same as print(x)"
      ],
      "metadata": {
        "id": "TRkMhiJNnWAt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "297007df-20d8-4306-b4c0-e622a2a184f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.,  2.],\n",
            "         [ 3.,  4.]],\n",
            "\n",
            "        [[ 5.,  6.],\n",
            "         [ 7.,  8.]],\n",
            "\n",
            "        [[ 9., 10.],\n",
            "         [11., 12.]]])\n",
            "tensor([[[ 1.,  2.],\n",
            "         [ 3.,  4.]],\n",
            "\n",
            "        [[ 5.,  6.],\n",
            "         [ 7.,  8.]],\n",
            "\n",
            "        [[ 9., 10.],\n",
            "         [11., 12.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm8vc3nuXaEw"
      },
      "source": [
        "We can also access arbitrary elements in each dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4xl6CW3RrEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c08a6d87-800a-47f6-ede0-0e29675e18e7"
      },
      "source": [
        "# Let's access the 0th and 1st elements, each twice\n",
        "# same as stacking x[0], x[0], x[1], x[1]\n",
        "i = torch.tensor([0, 0, 1, 1])\n",
        "x[i]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 2.],\n",
              "         [3., 4.]],\n",
              "\n",
              "        [[1., 2.],\n",
              "         [3., 4.]],\n",
              "\n",
              "        [[5., 6.],\n",
              "         [7., 8.]],\n",
              "\n",
              "        [[5., 6.],\n",
              "         [7., 8.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is similar to stacking of 4 tensors extracted by individual indices\n",
        "x_stacked = torch.stack([x[0], x[0], x[1], x[1]], axis=0)\n",
        "print(\"Stacked tensor:\\n\", (x_stacked))\n",
        "print(\"Stacked shape: \", x_stacked.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8Cm7Zi91Y10",
        "outputId": "63dde0fb-2e8c-4fcc-c161-9d049a711376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacked tensor:\n",
            " tensor([[[1., 2.],\n",
            "         [3., 4.]],\n",
            "\n",
            "        [[1., 2.],\n",
            "         [3., 4.]],\n",
            "\n",
            "        [[5., 6.],\n",
            "         [7., 8.]],\n",
            "\n",
            "        [[5., 6.],\n",
            "         [7., 8.]]])\n",
            "Stacked shape:  torch.Size([4, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also do a concatenation\n",
        "x_concat = torch.cat([x[0], x[0], x[1], x[1]], axis=0)\n",
        "print(\"Concatenated tensor:\\n\", (x_concat))\n",
        "print(\"Concatenated shape: \", x_concat.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf0vxSzk2Dai",
        "outputId": "44ca3357-d7a5-4ecd-f152-347311870a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Concatenated tensor:\n",
            " tensor([[1., 2.],\n",
            "        [3., 4.],\n",
            "        [1., 2.],\n",
            "        [3., 4.],\n",
            "        [5., 6.],\n",
            "        [7., 8.],\n",
            "        [5., 6.],\n",
            "        [7., 8.]])\n",
            "Concatenated shape:  torch.Size([8, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3QYZ8k7Wvqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41512406-9c4b-479d-ab66-4c9b98d40ede"
      },
      "source": [
        "# Let's access the 0th elements of the 1st and 2nd elements\n",
        "# This works thanks to broadcasting (index-array j has shape 1)\n",
        "\n",
        "i = torch.tensor([1, 2])\n",
        "j = torch.tensor([0])\n",
        "x[i, j]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5.,  6.],\n",
              "        [ 9., 10.]])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also use a matrix of indices instead of arrays.\n",
        "indices = [[1, 2], [0, 0]]\n",
        "print(\"Full index list:\\n\", x[tuple(indices)])\n",
        "\n",
        "indices = [[1, 2], [0]]\n",
        "print(\"Broadcasting over the second index list:\\n\", x[tuple(indices)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy6HL9sPQM-M",
        "outputId": "07358858-3656-4a69-f85a-6048e278f6ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full index list:\n",
            " tensor([[ 5.,  6.],\n",
            "        [ 9., 10.]])\n",
            "Broadcasting over the second index list:\n",
            " tensor([[ 5.,  6.],\n",
            "        [ 9., 10.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAELXC--IHS7"
      },
      "source": [
        "We can get a `Python` scalar value from a tensor with `.item()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM-ZujN2IGaQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90c9ada1-8da8-4b1a-faec-8f07e07fd6d4"
      },
      "source": [
        "x[0, 0, 0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NwxK7d_Ycgs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "577769b6-ecc8-4e03-e796-6082a55fd454"
      },
      "source": [
        "x[0, 0, 0].item()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise:\n",
        "\n",
        "Write code that creates a `torch.tensor` with the following contents:\n",
        "$\\begin{bmatrix} 1 & 2.2 & 9.6 \\\\ 4 & -7.2 & 6.3 \\end{bmatrix}$\n",
        "\n",
        "How do you get the second column? The first row?\n",
        "\n"
      ],
      "metadata": {
        "id": "bGod5kHa6OOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here..."
      ],
      "metadata": {
        "id": "TeGzFj9OR1Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re8xiL37eAja"
      },
      "source": [
        "## Autograd\n",
        "Pytorch is well-known for its automatic differentiation feature. We can call the `.backward()` method to ask `PyTorch` to calculate the gradients, which are then stored in the `grad` attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oEvBJHWfn8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0a6c758-b9e6-4ef8-aefa-127ed7f8b54c"
      },
      "source": [
        "# Create an example tensor\n",
        "# requires_grad parameter tells PyTorch to store gradients\n",
        "x = torch.tensor([2.], requires_grad=True)  # True by default\n",
        "print(x)\n",
        "\n",
        "# Print the gradient if it is calculated\n",
        "# Currently None since x is a scalar\n",
        "print(\"Tensor's current gradient value: \", x.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.], requires_grad=True)\n",
            "Tensor's current gradient value:  None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTJazZXkgthP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "029ab8de-9771-4c8d-b267-30229cc3d6fa"
      },
      "source": [
        "# Calculating the gradient of y with respect to x\n",
        "y = x * x * 3 # 3x^2\n",
        "y.backward()\n",
        "pp.pprint(x.grad) # d(y)/d(x) = d(3x^2)/d(x) = 6x = 12"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([12.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hqc2oM3iV6a"
      },
      "source": [
        "Let's run backprop from a different tensor again to see what happens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K--Az0Xiic_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1030188a-5869-4aac-d077-9749370f3408"
      },
      "source": [
        "z = (x ** 2) * 3 # 3x^2\n",
        "z.backward()\n",
        "pp.pprint(x.grad)\n",
        "\n",
        "z1 = (x ** 2) * 3 # 3x^2\n",
        "z1.backward()\n",
        "pp.pprint(x.grad)\n",
        "\n",
        "# Computing .backward() through the same graph throws a RuntimeError\n",
        "#z.backward()\n",
        "#pp.pprint(x.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([24.])\n",
            "tensor([36.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# When iterating through a next batch of data, you need to reset the gradients first\n",
        "x.grad = None\n",
        "z = x * x * 3 # 3x^2\n",
        "z.backward()\n",
        "# y = x * x * 3\n",
        "pp.pprint(x.grad)"
      ],
      "metadata": {
        "id": "4tIa4dw6Tmar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efaa9dd8-65dd-417d-ba6c-322b7e94ca9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([12.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad = None\n",
        "z = x * x * 3 # 3x^2\n",
        "z.backward()\n",
        "# y = x * x * 3\n",
        "pp.pprint(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbyr-P4rdJ9b",
        "outputId": "71270e86-be96-4135-c2d2-4d2331b85f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([12.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = x * x * 3 # 3x^2\n",
        "z.backward()\n",
        "# y = x * x * 3\n",
        "pp.pprint(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpvRq_wUdM5y",
        "outputId": "57ac24ee-3916-4578-cae4-749e115fd194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([24.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhjPkiE6i7ja"
      },
      "source": [
        "We can see that the `x.grad` is updated to be the sum of the gradients calculated so far. When we run backprop in a neural network, we sum up all the gradients for a particular neuron before making an update. This is exactly what is happening here! This is also the reason why we need to run `zero_grad()` in every training iteration (more on this later). Otherwise our gradients would keep building up from one training iteration to the other, which would cause our updates to be wrong."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customized Backward Function\n",
        "In some rare cases, you might want to design your own operators, or calculate higher order gradients that are not supported by Pytorch. In these cases you can define your own function with customized forward & backward computation. However, keep in mind that always check if something is already implemented by Pytorch (which is very likely) before customizing your own forward & backward computation. See more at https://pytorch.org/docs/stable/notes/extending.html."
      ],
      "metadata": {
        "id": "w-9I0zZOKtXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using GPUs\n",
        "\n",
        "With large models, you can significantly speed-up training (and to some extent, inference) by assigning GPU devices to model computation.\n",
        "\n",
        "NOTE: This Collab notebook runs on a machine without GPU resources, by default."
      ],
      "metadata": {
        "id": "_ST4C0buxmL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GPU, if available, otherwise, run on CPU\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
        "else:\n",
        "    print(\"No GPU available. Training will run on CPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcI9KkFWyA13",
        "outputId": "10de2e66-a6bb-4ed7-954d-182defb4051d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla T4 is available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In practice, you want to set a variable based on the GPU availability and assign this variable to parts of the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vBHToHjyKos",
        "outputId": "ba5e2515-fabb-41cf-b1cb-e00873e7733c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor on the CPU\n",
        "tensor = torch.randn((3, 3))\n",
        "print(\"This tensor is on device: \", tensor.device)\n",
        "# Move the tensor to the GPU\n",
        "tensor_gpu = tensor.to(device)\n",
        "print(\"This tensor is on device: \", tensor_gpu.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzf42texylRK",
        "outputId": "342db73f-27bf-48fd-ae4d-232533d46b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This tensor is on device:  cpu\n",
            "This tensor is on device:  cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYLWqKIoaOyd"
      },
      "source": [
        "## Neural Network Module\n",
        "\n",
        "So far we have looked into the tensors, their properties and basic operations on tensors. These are especially useful to get familiar with if we are building the layers of our network from scratch. We will utilize these in Assignment 3, but moving forward, we will use predefined blocks in the `torch.nn` module of `PyTorch`. We will then put together these blocks to create complex networks. Let's start by importing this module with an alias so that we don't have to type `torch` every time we use it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUmrDpbhV4Tn"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joGvRWjEbak0"
      },
      "source": [
        "### **Linear Layer**\n",
        "We can use `nn.Linear(H_in, H_out)` to create a a linear layer. This will take a matrix of `(N, *, H_in)` dimensions and output a matrix of `(N, *, H_out)`. The `*` denotes that there could be arbitrary number of dimensions in between. The linear layer performs the operation `Ax+b`, where `A` and `b` are initialized randomly. If we don't want the linear layer to learn the bias parameters, we can initialize our layer with `bias=False`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XfnKI4-a5j9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b70532e0-29c0-4add-f652-c93295870a19"
      },
      "source": [
        "# Create the inputs\n",
        "input = torch.ones(4, 3)\n",
        "# N, -1, H_in -> N, -1, H_out\n",
        "\n",
        "\n",
        "# Make a linear layers transforming N,-1,H_in dimensinal inputs to N,-1,H_out\n",
        "# dimensional outputs\n",
        "linear = nn.Linear(3, 2)\n",
        "linear_output = linear(input)\n",
        "print(\"Output of Linear\", linear_output)\n",
        "print(\"Output shape\", linear_output.shape)\n",
        "print(\"Linear weights and biases:\\n\", linear.weight, linear.bias)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of Linear tensor([[-0.1373,  0.3348],\n",
            "        [-0.1373,  0.3348],\n",
            "        [-0.1373,  0.3348],\n",
            "        [-0.1373,  0.3348]], grad_fn=<AddmmBackward0>)\n",
            "Output shape torch.Size([4, 2])\n",
            "Linear weights and biases:\n",
            " Parameter containing:\n",
            "tensor([[-0.1842, -0.2575,  0.2432],\n",
            "        [-0.3184,  0.4725, -0.1700]], requires_grad=True) Parameter containing:\n",
            "tensor([0.0611, 0.3507], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_9XKtAFYpdI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2a54836-406c-4466-8be6-a1c0670fa8cb"
      },
      "source": [
        "print(\"All parameters :\\n\", list(linear.parameters())) # Ax + b\n",
        "\n",
        "print(\"Linear weights:\\n\", linear.weight)\n",
        "print(\"Linear biases:\\n\", linear.bias)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All parameters :\n",
            " [Parameter containing:\n",
            "tensor([[-0.3111,  0.0479, -0.4732],\n",
            "        [-0.0173,  0.3803,  0.0819]], requires_grad=True), Parameter containing:\n",
            "tensor([-0.4634, -0.0155], requires_grad=True)]\n",
            "Linear weights:\n",
            " Parameter containing:\n",
            "tensor([[-0.3111,  0.0479, -0.4732],\n",
            "        [-0.0173,  0.3803,  0.0819]], requires_grad=True)\n",
            "Linear biases:\n",
            " Parameter containing:\n",
            "tensor([-0.4634, -0.0155], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data of shape [batch_size, feature_dim] # 4\n",
        "# [batch_size, output_dim] # 2\n",
        "\n",
        "# linear layer of shape (feature_dim, output_dim)"
      ],
      "metadata": {
        "id": "FsyBaqzZppS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAXCCu9keUlW"
      },
      "source": [
        "### **Other Module Layers**\n",
        "There are several other preconfigured layers in the `nn` module. Some commonly used examples are `nn.Conv2d`, `nn.ConvTranspose2d`, `nn.BatchNorm1d`, `nn.BatchNorm2d`, `nn.Upsample` and `nn.MaxPool2d` among many others. We will learn more about these as we progress in the course. For now, the only important thing to remember is that we can treat each of these layers as plug and play components: we will be providing the required dimensions and `PyTorch` will take care of setting them up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yslDOK66fYWn"
      },
      "source": [
        "### **Activation Function Layer**\n",
        "We can also use the `nn` module to apply activations functions to our tensors. Activation functions are used to add non-linearity to our network. Some examples of activations functions are `nn.ReLU()`, `nn.Sigmoid()` and `nn.LeakyReLU()`. Activation functions operate on each element seperately, so the shape of the tensors we get as an output are the same as the ones we pass in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrJP5CveeOON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54501082-efa6-40f1-e91f-2840dfb0dfa3"
      },
      "source": [
        "linear_output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1373,  0.3348],\n",
              "        [-0.1373,  0.3348],\n",
              "        [-0.1373,  0.3348],\n",
              "        [-0.1373,  0.3348]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9v5FjQtd4Ck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "921cbcca-96b5-44d0-cf8c-74f23ed03ef8"
      },
      "source": [
        "sigmoid = nn.Sigmoid()\n",
        "output = sigmoid(linear_output)\n",
        "output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4657, 0.5829],\n",
              "        [0.4657, 0.5829],\n",
              "        [0.4657, 0.5829],\n",
              "        [0.4657, 0.5829]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiYTthJwhEYT"
      },
      "source": [
        "### **Putting the Layers Together**\n",
        "So far we have seen that we can create layers and pass the output of one as the input of the next. Instead of creating intermediate tensors and passing them around, we can use `nn.Sequentual`, which does exactly that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtJeOqLxhBLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc08f2c9-99d7-44bb-85d6-7978eae09351"
      },
      "source": [
        "block = nn.Sequential(\n",
        "    nn.Linear(4, 2),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "input = torch.ones(2,3,4)\n",
        "output = block(input)\n",
        "output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.3491, 0.1892],\n",
              "         [0.3491, 0.1892],\n",
              "         [0.3491, 0.1892]],\n",
              "\n",
              "        [[0.3491, 0.1892],\n",
              "         [0.3491, 0.1892],\n",
              "         [0.3491, 0.1892]]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkJ81p3GUVPM"
      },
      "source": [
        "### Custom Modules\n",
        "\n",
        "Instead of using the predefined modules, we can also build our own by extending the `nn.Module` class. For example, we can build a the `nn.Linear` (which also extends `nn.Module`) on our own using the tensor introduced earlier! We can also build new, more complex modules, such as a custom neural network. You will be practicing these in the later assignment.\n",
        "\n",
        "To create a custom module, the first thing we have to do is to extend the `nn.Module`. We can then initialize our parameters in the `__init__` function, starting with a call to the `__init__` function of the super class. All the class attributes we define which are `nn` module objects are treated as parameters, which can be learned during the training. Tensors are not parameters, but they can be turned into parameters if they are wrapped in `nn.Parameter` class.\n",
        "\n",
        "All classes extending `nn.Module` are also expected to implement a `forward(x)` function, where `x` is a tensor. This is the function that is called when a parameter is passed to our module, such as in `model(x)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2P7eZiMj32_"
      },
      "source": [
        "class MultilayerPerceptronBinary(nn.Module):\n",
        "  \"MLP for logistic regression.\"\n",
        "\n",
        "  def __init__(self, x_size, h_size):\n",
        "    # Call to the __init__ function of the super class\n",
        "    super(MultilayerPerceptronBinary, self).__init__()\n",
        "\n",
        "    # Bookkeeping: Saving the initialization parameters\n",
        "    self.input_size = x_size\n",
        "    self.hidden_size = h_size\n",
        "\n",
        "    # Defining of our model\n",
        "    # There isn't anything specific about the naming of `self.model`. It could\n",
        "    # be something arbitrary.\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(self.input_size, self.hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.hidden_size, self.input_size),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    y = self.model(x)\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2DrfLiBVjNT"
      },
      "source": [
        "Here is an alternative way to define the same class. You can see that we can replace `nn.Sequential` by defining the individual layers in the `__init__` method and connecting the in the `forward` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-lqhsqwViIk"
      },
      "source": [
        "class MultilayerPerceptron(nn.Module):\n",
        "  \"MLP for multiclass logistic regression.\"\n",
        "\n",
        "  def __init__(self, x_size, h_size, y_size):\n",
        "    # Call to the __init__ function of the super class\n",
        "    super(MultilayerPerceptron, self).__init__()\n",
        "\n",
        "    # Bookkeeping: Saving the initialization parameters\n",
        "    self.input_size = x_size\n",
        "    self.hidden_size = h_size\n",
        "    self.output_size = y_size\n",
        "\n",
        "    # Defining of our layers\n",
        "    self.hidden_linear = nn.Linear(self.input_size, self.hidden_size)\n",
        "    self.hidden_act = nn.ReLU()\n",
        "    self.output_linear = nn.Linear(self.hidden_size, self.output_size)\n",
        "    self.output_act = nn.Softmax()\n",
        "\n",
        "  def forward(self, x):\n",
        "    h_in = self.hidden_linear(x)\n",
        "    h = self.hidden_act(h_in)\n",
        "    # Alternatively, you can define more complex behaviour, e.g. a \"residual connection\" between the input tensor and the hidden layer output\n",
        "    # h = self.hidden_act(self.hidden_linear(x)) + x\n",
        "\n",
        "    y_in = self.output_linear(h)\n",
        "    y = self.output_act(y_in)\n",
        "    return y  # returns a probability distribution over the K=`y_size` labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQelcFo5bXgU"
      },
      "source": [
        "Now that we have defined our class, we can instantiate it and see what it does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXi0T0FZbV0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f395637c-ca1e-4c6b-cf1f-20cf62498391"
      },
      "source": [
        "# Make a sample input\n",
        "input = torch.randn(2, 5)\n",
        "\n",
        "# Create our model\n",
        "model = MultilayerPerceptronBinary(5, 3)\n",
        "\n",
        "# Pass our input through our model\n",
        "model(input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4572, 0.4271, 0.4568, 0.3642, 0.4728],\n",
              "        [0.4582, 0.4279, 0.4583, 0.3604, 0.4683]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCCbjc-Fb2-B"
      },
      "source": [
        "We can inspect the parameters of our model with `named_parameters()` and `parameters()` methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d23soYIb2WZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff3b3c21-6094-4ab2-ba9d-85452c024a4b"
      },
      "source": [
        "list(model.named_parameters())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('model.0.weight',\n",
              "  Parameter containing:\n",
              "  tensor([[-0.3012, -0.0858,  0.0569, -0.0735, -0.1743],\n",
              "          [ 0.1012, -0.0810, -0.3994, -0.2952, -0.3351],\n",
              "          [ 0.3343, -0.1778,  0.0729, -0.3328, -0.4405]], requires_grad=True)),\n",
              " ('model.0.bias',\n",
              "  Parameter containing:\n",
              "  tensor([-0.3644,  0.0364, -0.3522], requires_grad=True)),\n",
              " ('model.2.weight',\n",
              "  Parameter containing:\n",
              "  tensor([[ 0.1213,  0.1365,  0.2605],\n",
              "          [ 0.2877,  0.5309,  0.5119],\n",
              "          [ 0.5018, -0.2756,  0.0433],\n",
              "          [ 0.3594,  0.1354, -0.2512],\n",
              "          [-0.5266, -0.4991, -0.3710]], requires_grad=True)),\n",
              " ('model.2.bias',\n",
              "  Parameter containing:\n",
              "  tensor([ 0.4163, -0.4924, -0.0918,  0.1731,  0.5422], requires_grad=True))]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5JegycOdMFy"
      },
      "source": [
        "## Optimization\n",
        "We have showed how gradients are calculated with the `backward()` function. Having the gradients isn't enought for our models to learn. We also need to know how to update the parameters of our models. This is where the optimozers comes in. `torch.optim` module contains several optimizers that we can use. Some popular examples are `optim.SGD` and `optim.Adam`. When initializing optimizers, we pass our model parameters, which can be accessed with `model.parameters()`, telling the optimizers which values it will be optimizing. Optimizers also has a learning rate (`lr`) parameter, which determines how big of an update will be made in every step. Different optimizers have different hyperparameters as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0F-TvV0kk-I"
      },
      "source": [
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgak6o5dlQWF"
      },
      "source": [
        "After we have our optimization function, we can define a `loss` that we want to optimize for. We can either define the loss ourselves, or use one of the predefined loss function in `PyTorch`, such as `nn.BCELoss()`. Let's put everything together now! We will start by creating some dummy data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGYFiaT_vXBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc5c603b-2e6e-4b2d-8cfb-779abff53d07"
      },
      "source": [
        "# Create the y data\n",
        "y = torch.ones(10, 5)\n",
        "\n",
        "# Add some noise to our goal y to generate our x\n",
        "# We want out model to predict our original data labels, albeit the noise\n",
        "x = y + torch.randn_like(y)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0439,  1.5338, -0.2522,  2.1946,  0.7624],\n",
              "        [ 1.9122,  2.1143,  2.7625,  2.0433,  1.7148],\n",
              "        [ 3.8834,  0.4632,  1.7572,  1.1206, -0.4500],\n",
              "        [ 0.4880, -0.1021,  0.4833,  1.1883,  0.5024],\n",
              "        [ 1.8483,  0.1161,  1.2318,  3.5066,  0.3577],\n",
              "        [-0.5430,  0.2910, -0.8017,  2.5302,  0.5991],\n",
              "        [ 1.1935,  1.4306,  2.5219,  1.1166,  0.1193],\n",
              "        [ 1.8744,  0.6625,  1.0545,  1.2671,  1.4071],\n",
              "        [ 1.3618, -0.6269,  1.5119,  0.7388,  1.1972],\n",
              "        [ 0.7703,  1.2040,  1.5003,  2.0767,  1.7160]])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEsiOdpWvfLj"
      },
      "source": [
        "Now, we can define our model, optimizer and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oA2XsdsbN8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea7a81d5-5e4e-4ac8-f9b0-7e38c79e4677"
      },
      "source": [
        "# Instantiate the model\n",
        "model = MultilayerPerceptronBinary(5, 3)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=1)\n",
        "#optimizer = optim.Adam(model.parameters(), lr=1e-1)\n",
        "\n",
        "# Define loss using a predefined loss function\n",
        "loss_function = nn.BCELoss()\n",
        "\n",
        "# Calculate how our model is doing now\n",
        "y_pred = model(x)\n",
        "loss = loss_function(y_pred, y)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6457, grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtxU7Y8ZufSR"
      },
      "source": [
        "Let's see if we can have our model achieve a smaller loss. Now that we have everything we need, we can setup our training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogl6-Ctmuek6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80252f84-6511-49b3-b570-455c20afbec4"
      },
      "source": [
        "# Set the number of epoch, which determines the number of training iterations\n",
        "n_epoch = 10\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "  # Set the gradients to 0\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # Get the model predictions\n",
        "  y_pred = model(x)\n",
        "\n",
        "  # Get the loss\n",
        "  loss = loss_function(y_pred, y)\n",
        "\n",
        "  # Print stats\n",
        "  print(f\"Epoch {epoch}: traing loss: {loss}\")\n",
        "\n",
        "  # Compute the gradients\n",
        "  loss.backward()\n",
        "\n",
        "  # Take a step to optimize the weights\n",
        "  optimizer.step()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: traing loss: 0.6456535458564758\n",
            "Epoch 1: traing loss: 0.5407567024230957\n",
            "Epoch 2: traing loss: 0.4014311730861664\n",
            "Epoch 3: traing loss: 0.24919284880161285\n",
            "Epoch 4: traing loss: 0.14228886365890503\n",
            "Epoch 5: traing loss: 0.0883583053946495\n",
            "Epoch 6: traing loss: 0.061105288565158844\n",
            "Epoch 7: traing loss: 0.04563061147928238\n",
            "Epoch 8: traing loss: 0.03591383248567581\n",
            "Epoch 9: traing loss: 0.029342083260416985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrMJ8AmqeCY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3af7931b-4a0f-4722-a861-fd8e88d52f74"
      },
      "source": [
        "list(model.parameters())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[ 0.2264, -0.4170, -0.0122, -0.2925,  0.1127],\n",
              "         [-0.0281,  0.4767,  1.0541,  0.2641,  0.8284],\n",
              "         [ 0.2989, -0.2280,  0.0177,  0.0099, -0.3006]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.1330,  0.7851, -0.0184], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[-0.4401,  0.8581,  0.2906],\n",
              "         [-0.4191,  0.6303, -0.0774],\n",
              "         [ 0.0524,  0.5051, -0.5452],\n",
              "         [ 0.0107,  0.8030, -0.3300],\n",
              "         [-0.0989,  0.8633,  0.4599]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([0.4768, 0.5623, 0.9843, 0.5688, 0.2585], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nXApd82wlsF"
      },
      "source": [
        "You can see that our loss is decreasing. Let's check the predictions of our model now and see if they are close to our original `y`, which was all `1s`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRqE7P9EtvuS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f77c44ee-0c14-41c2-ce5b-000606794f40"
      },
      "source": [
        "# See how our model performs on the training data\n",
        "y_pred = model(x)\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9887, 0.9706, 0.9656, 0.9867, 0.9863],\n",
              "        [0.9892, 0.9715, 0.9665, 0.9872, 0.9869],\n",
              "        [0.9653, 0.9343, 0.9347, 0.9622, 0.9579],\n",
              "        [0.9978, 0.9911, 0.9868, 0.9972, 0.9974],\n",
              "        [0.9391, 0.9008, 0.9140, 0.9399, 0.9300],\n",
              "        [0.9984, 0.9930, 0.9890, 0.9979, 0.9981],\n",
              "        [0.9744, 0.9471, 0.9451, 0.9715, 0.9689],\n",
              "        [0.9568, 0.9232, 0.9260, 0.9535, 0.9476],\n",
              "        [0.8857, 0.8476, 0.8709, 0.8848, 0.8628],\n",
              "        [0.9874, 0.9683, 0.9635, 0.9853, 0.9848]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJng31_Pi2R6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f97b893d-1cbc-4657-a8c4-5092e8a71a13"
      },
      "source": [
        "# Create test data and check how our model performs on it\n",
        "x2 = y + torch.randn_like(y)\n",
        "y2_pred = model(x2)\n",
        "y2_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9906, 0.9974, 0.9942, 0.9888, 0.9918],\n",
              "        [0.9925, 0.9977, 0.9947, 0.9918, 0.9914],\n",
              "        [0.9806, 0.9941, 0.9886, 0.9778, 0.9839],\n",
              "        [0.9945, 0.9992, 0.9979, 0.9890, 0.9981],\n",
              "        [0.9807, 0.9923, 0.9855, 0.9824, 0.9744],\n",
              "        [0.9188, 0.9486, 0.9304, 0.9441, 0.8622],\n",
              "        [0.8803, 0.9372, 0.9202, 0.8994, 0.8725],\n",
              "        [0.8044, 0.8908, 0.8759, 0.8319, 0.8155],\n",
              "        [0.9342, 0.9660, 0.9509, 0.9475, 0.9125],\n",
              "        [0.9086, 0.9599, 0.9452, 0.9136, 0.9192]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo: mlp_classification_sgd in PyTorch\n",
        "\n",
        "This is a simple example solution of the mlp_classification_sgd task using PyTorch."
      ],
      "metadata": {
        "id": "WXrObpGRv0Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "\n",
        "# Default argparse arguments\n",
        "seed =42\n",
        "classes = 10\n",
        "hidden_size = 50\n",
        "epochs = 100\n",
        "test_size = 797\n",
        "batch_size = 100\n",
        "\n",
        "# Set random seed\n",
        "generator = np.random.RandomState(seed)\n",
        "\n",
        "# Load the digits dataset.\n",
        "data, target = sklearn.datasets.load_digits(n_class=classes, return_X_y=True)\n",
        "\n",
        "# Split the dataset into a train set and a test set.\n",
        "# Use `sklearn.model_selection.train_test_split` method call, passing\n",
        "# arguments `test_size=args.test_size, random_state=args.seed`.\n",
        "train_data, test_data, train_target, test_target = sklearn.model_selection.train_test_split(\n",
        "    data, target, test_size=test_size, random_state=seed)\n",
        "\n",
        "# Convert the data to torch.tensor\n",
        "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
        "test_data = torch.tensor(test_data, dtype=torch.float32)\n",
        "train_target = torch.tensor(train_target, dtype=torch.int64)\n",
        "test_target = torch.tensor(test_target, dtype=torch.int64)\n",
        "\n",
        "# Create the model, loss_fn and optimizer\n",
        "model = MultilayerPerceptron(train_data.shape[1], hidden_size, classes)  # We do not explicitly specify the weigh initialization interval\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05)"
      ],
      "metadata": {
        "id": "ENL4fIZzJi3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    permutation = generator.permutation(train_data.shape[0])\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    train_accuracy = 0\n",
        "    test_accuracy = 0\n",
        "    n_batches = torch.tensor(permutation.shape[0] / batch_size, dtype=torch.float32)\n",
        "    for i in range(0, int(n_batches.item())):\n",
        "        # Prepare batched training data\n",
        "        batch_indices = permutation[i * batch_size : (i+1) * batch_size]\n",
        "        batch_train = train_data[batch_indices]\n",
        "        batch_target = train_target[batch_indices]\n",
        "        batch_target_probs = torch.nn.functional.one_hot(batch_target, num_classes=classes).type(torch.float32)\n",
        "\n",
        "        # Compute label probability distribution\n",
        "        y_probs = model(batch_train)\n",
        "\n",
        "        # Compute loss\n",
        "        loss += loss_function(batch_target_probs, y_probs).mean()\n",
        "\n",
        "        # Get predictions and compute accuracy\n",
        "        y_pred = y_probs.argmax(-1)\n",
        "        train_accuracy += (y_pred == batch_target).type(torch.float32).mean()\n",
        "\n",
        "    # Average over all batches\n",
        "    loss = loss / n_batches  # average the loss by the size of training data\n",
        "    train_accuracy = train_accuracy / n_batches\n",
        "\n",
        "    # Compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # Evaluate on test dataset\n",
        "    y_pred_test = model(torch.tensor(test_data, dtype=torch.float32)).argmax(-1)\n",
        "    test_accuracy = (y_pred_test == torch.tensor(test_target, dtype=torch.float32)).type(torch.float32).mean()\n",
        "\n",
        "    print(\"After epoch {}: train acc {:.1f}%, test acc {:.1f}%\".format(\n",
        "        epoch + 1, 100 * train_accuracy, 100 * test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qez6zNGYIyqZ",
        "outputId": "308c51ef-5d6b-4a32-93cf-a38ca588ebec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/tmp/ipython-input-4051714027.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_pred_test = model(torch.tensor(test_data, dtype=torch.float32)).argmax(-1)\n",
            "/tmp/ipython-input-4051714027.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_accuracy = (y_pred_test == torch.tensor(test_target, dtype=torch.float32)).type(torch.float32).mean()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After epoch 1: train acc 5.4%, test acc 9.4%\n",
            "After epoch 2: train acc 10.1%, test acc 12.2%\n",
            "After epoch 3: train acc 12.8%, test acc 11.9%\n",
            "After epoch 4: train acc 12.2%, test acc 12.2%\n",
            "After epoch 5: train acc 12.0%, test acc 12.3%\n",
            "After epoch 6: train acc 12.3%, test acc 13.3%\n",
            "After epoch 7: train acc 12.9%, test acc 14.3%\n",
            "After epoch 8: train acc 14.7%, test acc 16.8%\n",
            "After epoch 9: train acc 16.2%, test acc 18.8%\n",
            "After epoch 10: train acc 18.8%, test acc 21.5%\n",
            "After epoch 11: train acc 22.7%, test acc 24.8%\n",
            "After epoch 12: train acc 24.7%, test acc 26.6%\n",
            "After epoch 13: train acc 27.0%, test acc 28.9%\n",
            "After epoch 14: train acc 29.0%, test acc 30.4%\n",
            "After epoch 15: train acc 30.9%, test acc 32.0%\n",
            "After epoch 16: train acc 32.1%, test acc 33.4%\n",
            "After epoch 17: train acc 33.2%, test acc 34.3%\n",
            "After epoch 18: train acc 34.1%, test acc 35.1%\n",
            "After epoch 19: train acc 35.3%, test acc 35.6%\n",
            "After epoch 20: train acc 35.9%, test acc 36.3%\n",
            "After epoch 21: train acc 37.0%, test acc 37.1%\n",
            "After epoch 22: train acc 37.8%, test acc 37.9%\n",
            "After epoch 23: train acc 38.4%, test acc 38.9%\n",
            "After epoch 24: train acc 39.2%, test acc 39.9%\n",
            "After epoch 25: train acc 40.6%, test acc 41.2%\n",
            "After epoch 26: train acc 42.1%, test acc 42.0%\n",
            "After epoch 27: train acc 43.7%, test acc 43.2%\n",
            "After epoch 28: train acc 44.5%, test acc 44.5%\n",
            "After epoch 29: train acc 45.2%, test acc 44.9%\n",
            "After epoch 30: train acc 45.8%, test acc 45.5%\n",
            "After epoch 31: train acc 46.0%, test acc 46.0%\n",
            "After epoch 32: train acc 46.5%, test acc 46.3%\n",
            "After epoch 33: train acc 46.7%, test acc 46.8%\n",
            "After epoch 34: train acc 47.4%, test acc 47.3%\n",
            "After epoch 35: train acc 47.9%, test acc 47.6%\n",
            "After epoch 36: train acc 47.9%, test acc 48.1%\n",
            "After epoch 37: train acc 48.1%, test acc 48.3%\n",
            "After epoch 38: train acc 48.1%, test acc 48.7%\n",
            "After epoch 39: train acc 48.1%, test acc 48.9%\n",
            "After epoch 40: train acc 48.2%, test acc 49.2%\n",
            "After epoch 41: train acc 48.4%, test acc 49.3%\n",
            "After epoch 42: train acc 48.4%, test acc 49.3%\n",
            "After epoch 43: train acc 48.5%, test acc 49.6%\n",
            "After epoch 44: train acc 48.5%, test acc 49.8%\n",
            "After epoch 45: train acc 48.5%, test acc 49.9%\n",
            "After epoch 46: train acc 48.5%, test acc 50.2%\n",
            "After epoch 47: train acc 48.6%, test acc 50.4%\n",
            "After epoch 48: train acc 48.8%, test acc 50.4%\n",
            "After epoch 49: train acc 49.1%, test acc 50.6%\n",
            "After epoch 50: train acc 49.1%, test acc 50.9%\n",
            "After epoch 51: train acc 49.7%, test acc 51.4%\n",
            "After epoch 52: train acc 50.0%, test acc 51.8%\n",
            "After epoch 53: train acc 50.9%, test acc 52.7%\n",
            "After epoch 54: train acc 51.7%, test acc 53.2%\n",
            "After epoch 55: train acc 52.2%, test acc 54.1%\n",
            "After epoch 56: train acc 53.3%, test acc 55.7%\n",
            "After epoch 57: train acc 55.3%, test acc 56.3%\n",
            "After epoch 58: train acc 57.0%, test acc 57.5%\n",
            "After epoch 59: train acc 57.4%, test acc 57.3%\n",
            "After epoch 60: train acc 57.6%, test acc 57.7%\n",
            "After epoch 61: train acc 57.7%, test acc 58.0%\n",
            "After epoch 62: train acc 57.8%, test acc 58.1%\n",
            "After epoch 63: train acc 57.9%, test acc 58.3%\n",
            "After epoch 64: train acc 57.9%, test acc 58.3%\n",
            "After epoch 65: train acc 57.9%, test acc 58.5%\n",
            "After epoch 66: train acc 58.0%, test acc 58.5%\n",
            "After epoch 67: train acc 58.3%, test acc 58.6%\n",
            "After epoch 68: train acc 58.6%, test acc 58.7%\n",
            "After epoch 69: train acc 58.6%, test acc 59.2%\n",
            "After epoch 70: train acc 58.7%, test acc 59.5%\n",
            "After epoch 71: train acc 58.8%, test acc 60.0%\n",
            "After epoch 72: train acc 59.5%, test acc 60.6%\n",
            "After epoch 73: train acc 60.4%, test acc 61.4%\n",
            "After epoch 74: train acc 61.4%, test acc 62.7%\n",
            "After epoch 75: train acc 62.1%, test acc 64.0%\n",
            "After epoch 76: train acc 63.2%, test acc 65.2%\n",
            "After epoch 77: train acc 63.8%, test acc 65.4%\n",
            "After epoch 78: train acc 64.3%, test acc 65.4%\n",
            "After epoch 79: train acc 64.6%, test acc 65.7%\n",
            "After epoch 80: train acc 64.7%, test acc 66.2%\n",
            "After epoch 81: train acc 65.1%, test acc 66.6%\n",
            "After epoch 82: train acc 65.6%, test acc 66.9%\n",
            "After epoch 83: train acc 65.8%, test acc 66.9%\n",
            "After epoch 84: train acc 65.9%, test acc 67.0%\n",
            "After epoch 85: train acc 66.0%, test acc 67.0%\n",
            "After epoch 86: train acc 66.0%, test acc 67.0%\n",
            "After epoch 87: train acc 66.1%, test acc 67.4%\n",
            "After epoch 88: train acc 66.1%, test acc 67.4%\n",
            "After epoch 89: train acc 66.3%, test acc 67.4%\n",
            "After epoch 90: train acc 66.4%, test acc 67.5%\n",
            "After epoch 91: train acc 66.4%, test acc 67.8%\n",
            "After epoch 92: train acc 66.4%, test acc 67.8%\n",
            "After epoch 93: train acc 66.5%, test acc 67.8%\n",
            "After epoch 94: train acc 66.6%, test acc 67.8%\n",
            "After epoch 95: train acc 66.6%, test acc 68.0%\n",
            "After epoch 96: train acc 66.7%, test acc 68.0%\n",
            "After epoch 97: train acc 66.7%, test acc 68.0%\n",
            "After epoch 98: train acc 66.7%, test acc 68.0%\n",
            "After epoch 99: train acc 66.9%, test acc 68.0%\n",
            "After epoch 100: train acc 66.9%, test acc 68.0%\n"
          ]
        }
      ]
    }
  ]
}