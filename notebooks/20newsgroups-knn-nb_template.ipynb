{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GbmbKkrURuwS"
      },
      "outputs": [],
      "source": [
        "!pip install fasttext scikit-learn\n",
        "\n",
        "import numpy as np\n",
        "import fasttext\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document classification using k-NN and Naive Bayes\n",
        "Today, we will work with the [20 newsgroups](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) dataset from 1997. It contains relatively short articles (as well as some longer ones) from 20 different newsgroups (see below).\n",
        "\n",
        "Our goal is to decide which group a given document comes from. To make the task more challenging, we will remove any superfluous metadata, and we'll judge each document purely by its content. Don't expect perfect accuracy.\n",
        "\n",
        "In last week's lab, you were introduced to TF-IDF and sentence embedding using Word2Vec. We'll use these techniques as well but, this time we'll employ k-Nearest Neighbors and Naive Bayes classifiers.\n",
        "\n"
      ],
      "metadata": {
        "id": "80By9xa-GY1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch dataset\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "ax.bar(range(len(train.target_names)), np.bincount(train.target), align='center')\n",
        "plt.xticks(range(len(train.target_names)), train.target_names, rotation=45, ha='right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5-T_KCsvBlWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Vectorizing the documents\n",
        "The sklearn implementations of both k-Nearest Neighbors and Naive Bayes classifiers take vectors of fixed length. So, we must first vectorize the documents.\n",
        "\n",
        "Sklearn already implements [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), which simply counts the occurrences of each word from a vocabulary, and [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), which you should already be familiar with.\n",
        "\n",
        "We'll also implement our own vectorizers.\n",
        "First, implement `EmbeddingVectorizer` below, which will vectorize documents using a given Word2Vec embedding model.\n",
        "\n",
        "Last week, you saw that ignoring certain \"stopwords\" often helps. Instead of ignoring them, we could lower their influence by weighting each word embedding by its inverse document frequency (IDF). Try implementing this in `IdfWeightedEmbeddingVectorizer`."
      ],
      "metadata": {
        "id": "V6vSUI-0Pd7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# download a pre-trained Word2Vec model\n",
        "if os.path.exists('cc.en.300.bin'):\n",
        "    print('FastText models already downloaded.')\n",
        "else:\n",
        "    ! wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "    ! gunzip cc.en.300.bin.gz\n",
        "\n",
        "embeddings = fasttext.load_model('cc.en.300.bin')"
      ],
      "metadata": {
        "id": "bZKyhW-PUARi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "class EmbeddingVectorizer(TransformerMixin, BaseEstimator):\n",
        "  def __init__(self, embedding):\n",
        "    self.embedding: fasttext.FastText._FastText = embedding\n",
        "    self.dim = embedding.get_dimension()\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "  def transform(self, X: list[str]) -> np.ndarray:\n",
        "    # TODO: implement this method. It should:\n",
        "    # 1. take a list of documents X\n",
        "    # 2. use self.embedding to convert each document to a vector of fixed size\n",
        "    # (for example by computing the mean of the word embeddings)\n",
        "    # 3. return a numpy array\n",
        "    #\n",
        "    # hint: self.embedding should already have a method to get vector from a whole sentence\n",
        "    # 2nd hint: that method does not like newlines, you can remove them with document.replace('\\n', ' ')\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "nTQ7IROKEnyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "class IdfWeightedEmbeddingVectorizer(TransformerMixin, BaseEstimator):\n",
        "  def __init__(self, embedding):\n",
        "    self.embedding: fasttext.FastText._FastText = embedding\n",
        "    self.idf_weights = None\n",
        "    self.dim = embedding.get_dimension()\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
        "    tfidf.fit(X)\n",
        "    # if a word was never seen - it must be at least as infrequent\n",
        "    # as any of the known words - so the default idf is the max of\n",
        "    # known idf's\n",
        "    max_idf = max(tfidf.idf_)\n",
        "    self.idf_weights = defaultdict(\n",
        "        lambda: max_idf,\n",
        "        [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
        "    return self\n",
        "\n",
        "  def transform(self, X: list[str]) -> np.ndarray:\n",
        "    # TODO: implement\n",
        "    # Same as before but this time compute a weighted mean of the word embeddings\n",
        "    # using the computed IDF weights.\n",
        "    # Hint: some documents have no words, make sure to return a vector of zeroes of the correct dimension for those\n",
        "    # Hint 2: you can vectorize a word with self.embedding.get_word_vector(w) and get its weight with self.idf_weights[w]\n",
        "    raise NotImplementedError()\n"
      ],
      "metadata": {
        "id": "G03mSMRjuPTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.5) Simple document lookup engine\n",
        "Lets use our vectorizers in a very simplistic document lookup engine.\n",
        "The lookup engine works by comparing the vectors of stored documents with a vector computed from a given query. It then shows the documents whose vectors are closest to the query. Try it out!\n",
        "\n",
        "What metric makes sense for finding the closest document vectors?\n",
        "\n",
        "Why doesn't the euclidean metric work well with the TF-IDF vectors?"
      ],
      "metadata": {
        "id": "ui6RopPTS-Tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "class RelevantDocumentFinder:\n",
        "  def __init__(self, documents: list[str], labels: list[str], vectorizer):\n",
        "    self.documents = documents\n",
        "    self.labels = labels\n",
        "    self.vectorizer = vectorizer\n",
        "    self.vectors = self.vectorizer.fit_transform(documents)\n",
        "    pass\n",
        "  def query(self, query: str, num_nearest: int = 5, metric: str = 'euclidean'):\n",
        "    query_vec = self.vectorizer.transform([query])\n",
        "    distances = pairwise_distances(self.vectors, query_vec.reshape(1, -1), metric=metric)\n",
        "    nearest = np.argsort(distances, axis=0)[:num_nearest].flatten()\n",
        "    for idx in nearest:\n",
        "      print(f\"Distance: {distances[idx][0]:.2f}\")\n",
        "      print(f\"Group: {train.target_names[self.labels[idx]]}\")\n",
        "      print(f\"Document:\\n{self.documents[idx]}\")\n",
        "      print(\"-----------------------------------------------\")\n",
        "\n",
        "finder = RelevantDocumentFinder(train.data, train.target, EmbeddingVectorizer(embedding=embeddings))\n",
        "#finder = RelevantDocumentFinder(train.data, train.target, IdfWeightedEmbeddingVectorizer(embedding=embeddings))\n",
        "#finder = RelevantDocumentFinder(train.data, train.target, TfidfVectorizer())\n",
        "\n",
        "query = \"The pope and the church.\"\n",
        "finder.query(query, metric='cosine')"
      ],
      "metadata": {
        "id": "bXYNDv48jLpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Document classification with k-Nearest Neighbors\n",
        "In `RelevantDocumentFinder`, we looked at which document vectors are close to the vector of our query. We could also use the labels of those nearby documents to decide which newsgroup our query corresponds to.\n",
        "\n",
        "Treating a previously unseen document as a query, finding its `k` nearest neighboring documents, and taking the most frequent label is precisely how the [`KNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) works.\n",
        "\n",
        "Try it together with different document vectorizers. Which one works best for this dataset? Which metric works best? Also try neighbor weighting."
      ],
      "metadata": {
        "id": "Xjyr8f-J3yCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Build a k-nearest neighbors classifier\n",
        "# First vectorize the documents. Try different vectorizers:\n",
        "#   CountVectorizer, TfidfVectorizer, EmbeddingVectorizer, IdfWeightedEmbeddingVectorizer\n",
        "# Try different parameters for KNeighborsClassifier, which metric works best?\n",
        "#\n",
        "# Which groups are easy to confuse?\n",
        "\n",
        "# TODO: build a pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', ...),\n",
        "    ('classifier', KNeighborsClassifier(...))\n",
        "])\n",
        "\n",
        "# train, evaluate\n",
        "pipeline.fit(train.data, train.target)\n",
        "test_predictions = pipeline.predict(test.data)\n",
        "\n",
        "# report results\n",
        "print(sklearn.metrics.classification_report(test.target, test_predictions, target_names=test.target_names))\n",
        "disp = ConfusionMatrixDisplay.from_predictions(test_predictions, test.target, display_labels=train.target_names, xticks_rotation='vertical')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FW86z-ibRNUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the above code, try different vectorizers and parameters"
      ],
      "metadata": {
        "id": "Iv49PHwklsUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Document classification with Naive Bayes\n",
        "\n",
        "Recall that Naive Bayes uses the Bayes theorem to model the conditional distribution of labels given the features of a sample:\n",
        "$$P(y | x_1, ..., x_d) = \\frac{P(y)P(x_1, ..., x_d | y)}{P(x_1, ..., x_d)}$$\n",
        "\n",
        "The denominator $P(x_1, ..., x_d)$ is just a constant given by our training data.\n",
        "\n",
        "The \"Naive\" in Naive Bayes means that we naively assume the individual features to be (conditionally) pair-wise independent:\n",
        "$$P(x_1, ..., x_d | y) \\approx P(x_1|y)P(x_2|y)\\cdots P(x_d|y)$$\n",
        "\n",
        "Training an NB classifier involves estimating the probabilities $P(y)$ and $P(x_i|y)$ from the training data.\n",
        "\n",
        "Classification is then done by simply choosing a label $\\hat{y}$ that maximizes:\n",
        "$$\\hat{y} = \\text{argmax}_y P(y)P(x_1|y)P(x_2|y)\\cdots P(x_d|y)$$\n",
        "for a given sample $x = (x_1, x_2, ..., x_d)$.\n",
        "\n",
        "# Choosing features and modelling their distributions\n",
        "$P(y)$ is simple to estimate from the proportion of each label in our training data. However, what features $x_i$ do we choose from our documents? How do we model their distributions $P(x_i|y)$? We have several options, try some of them:\n",
        "\n",
        "### 1. Simplest: Bernoulli Naive Bayes\n",
        "The simplest approach is to select a set of words (or the entire vocabulary) and for each word ask: \"Is this word present in the document?\" Representing the answer as 0/1, we obtain [Bernoulli distributed](https://en.wikipedia.org/wiki/Bernoulli_distribution) features.\n",
        "\n",
        "With these features, we can train a [`BernoulliNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html).\n",
        "Hint: The `CountVectorizer` from `sklearn.feature_extraction.text` can be used to get such features. (BernoulliNB simply checks if a feature is nonzero and treats it as binary)\n",
        "\n",
        "### 2. Better: Multinomial Naive Bayes\n",
        "Instead of a binary flag, we can model the word counts from `CountVectorizer` using the [multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution) with [`MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html). While the distribution is defined for discrete values, the TF-IDF vectors can also be used with `MultinomialNB`. Which feature vectors perform better?\n",
        "\n",
        "### 3. Another possible option: Gaussian Naive Bayes\n",
        "The [`GaussianNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) assumes the features to be distributed by the [Gaussian distribution](https://en.wikipedia.org/wiki/Normal_distribution). This means that it works with any real-valued features. We can therefore use it with word counts, TF-IDF vectors, as well as word embeddings.\n",
        "(However, the implementation does not like sparse representations.)"
      ],
      "metadata": {
        "id": "LsZpD8it6gnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Build a Naive Bayes classifier\n",
        "# First vectorize the documents. Try different Naive Bayes implementations and vectorizers:\n",
        "#\n",
        "# Compatible combinations:\n",
        "#  - BernoulliNB + CountVectorizer\n",
        "#  - MultinomialNB + CountVectorizer / TfidfVectorizer\n",
        "#  - GaussianNB + EmbeddingVectorizer / IdfWeightedEmbeddingVectorizer\n",
        "#\n",
        "# Try different parameters for the classifier\n",
        "\n",
        "# TODO: build a pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', ...),\n",
        "    ('classifier', ...),\n",
        "])\n",
        "\n",
        "# train, evaluate\n",
        "pipeline.fit(train.data, train.target)\n",
        "test_predictions = pipeline.predict(test.data)\n",
        "\n",
        "# report results\n",
        "print(sklearn.metrics.classification_report(test.target, test_predictions, target_names=test.target_names))\n",
        "disp = ConfusionMatrixDisplay.from_predictions(test_predictions, test.target, display_labels=train.target_names, xticks_rotation='vertical')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CEaInD41afjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the above code, try different vectorizers, classifiers, and parameters"
      ],
      "metadata": {
        "id": "cWOi3QPMl3R4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}