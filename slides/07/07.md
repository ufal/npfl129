title: NPFL129, Lecture 7
class: title, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Soft-margin SVM, SMO

## Milan Straka

### November 14, 2022

---
section: Refresh
# Support Vector Machines

In order to solve the constrained problem of
$$\argmin_{â†’w,b} \frac{1}{2} \|â†’w\|^2 \textrm{~~given that~~}t_i y(â†’x_i) â‰¥ 1,$$
we write the Lagrangian with multipliers $â†’a=(a_1, â€¦, a_N)$ as
$$ğ“› = \frac{1}{2} \|â†’w\|^2 - âˆ‘_i a_i \big(t_i y(â†’x_i) - 1\big).$$

Setting the derivatives with respect to $â†’w$ and $b$ to zero, we get
$$\begin{aligned}
â†’w =& âˆ‘_i a_i t_iÏ†(â†’x_i), \\
 0 =& âˆ‘_i a_i t_i. \\
\end{aligned}$$

---
# Support Vector Machines

Substituting these to the Lagrangian, we want to maximize
$$ğ“› = âˆ‘\nolimits_i a_i -  \frac{1}{2} âˆ‘\nolimits_i âˆ‘\nolimits_j a_i a_j t_i t_j K(â†’x_i, â†’x_j)$$
with respect to $a_i$ subject to the constraints $a_i â‰¥ 0$ and $âˆ‘_i a_i t_i = 0$,
using the kernel $K(â†’x, â†’z) = Ï†(â†’x)^T Ï†(â†’z).$

The solution will fulfill the KKT conditions, meaning that
$$\textcolor{gray}{a_i â‰¥ 0},\quad\quad
  \textcolor{gray}{t_i y(â†’x_i) - 1 â‰¥ 0},\quad\quad
  \textcolor{darkgreen}{a_i \big(t_i y(â†’x_i) - 1\big) = 0}.$$

~~~
Therefore, either a point $â†’x_i$ is on a boundary, or $a_i=0$. Given that the
prediction for $â†’x$ is $y(â†’x) = âˆ‘_i a_i t_i K(â†’x, â†’x_i) + b$,
we only need to keep the training points $â†’x_i$ that are on the boundary, the
so-called **support vectors**. Therefore, even though SVM is a nonparametric
model, it needs to store only a subset of the training data.

---
# Support Vector Machines

The dual formulation allows us to use nonlinear kernels.

![w=100%](../06/svm_gaussian.svgz)

---
section: Soft-margin SVM
# Support Vector Machines for Non-linearly Separable Data

![w=28%,f=right](svm_softmargin.svgz)

Until now, we assumed the data to be linearly separable â€“ the
**hard-margin SVM** variant. We now relax this condition to arrive at
**soft-margin SVM**.
~~~
The idea is to allow points to be in the margin or even on the _wrong side_ of
the decision boundary. We introduce **slack variables** $Î¾_i â‰¥ 0$, one for each
training instance, defined as
$$Î¾_i = \begin{cases} 0 &\textrm{~for points fulfilling~}t_i y(â†’x_i) â‰¥ 1,\\
                      |t_i - y(â†’x_i)|&\textrm{~otherwise}.\end{cases}$$

~~~
Therefore, $Î¾_i=0$ signifies a point outside of margin, $0 < Î¾_i < 1$ denotes
a point inside the margin, $Î¾_i=1$ is a point on the decision boundary, and
$Î¾_i>1$ indicates the point is on the opposite side of the separating
hyperplane.

~~~
Therefore, we want to optimize
$$\argmin_{â†’w,b} C âˆ‘\nolimits_i Î¾_i + \frac{1}{2} \|â†’w\|^2 \textrm{~given that~~}t_i y(â†’x_i) â‰¥ 1 - Î¾_i\textrm{~and~}Î¾_i â‰¥ 0.$$

---
# Support Vector Machines for Non-linearly Separable Data

To solve the soft-margin variant, we again create a Lagrangian, this time with
two sets of multipliers $â†’a=(a_1, â€¦, a_N)$ and $â†’Î¼=(Î¼_1, â€¦, Î¼_N)$:
$$ğ“› = \frac{1}{2} \|â†’w\|^2 + C âˆ‘_i Î¾_i - âˆ‘_i a_i \big(t_i y(â†’x_i) - 1 + Î¾_i\big) - âˆ‘_i Î¼_i Î¾_i.$$

~~~
Solving for the critical points and substituting for $â†’w$, $b$, and $â†’Î¾$ (obtaining an additional
constraint $Î¼_i = C - a_i$ compared to the previous case), we obtain the Lagrangian in the form
$$ğ“› = âˆ‘_i a_i -  \frac{1}{2} âˆ‘_i âˆ‘_j a_i a_j t_i t_j K(â†’x_i, â†’x_j),$$
~~~
which is identical to the previous case, but the constraints are a bit
different:
$$âˆ€\,i: C â‰¥ a_i â‰¥ 0\textrm{~~and~~}âˆ‘_i a_i t_i = 0.$$

---
# Support Vector Machines for Non-linearly Separable Data

Using the KKT conditions, we can see that the support vectors (examples with
$a_i>0$) are the ones with $t_i y(â†’x_i) = 1 - Î¾_i$, i.e., the examples
on the margin boundary, inside the margin, and on the opposite side
of the decision boundary.

![w=50%,h=center](svm_softmargin_supports.svgz)

---
section: Hinge
# SGD-like Formulation of Soft-Margin SVM

Note that the slack variables can be written as
$$Î¾_i = \max\big(0, 1 - t_i y(â†’x_i)\big),$$

~~~
so we can reformulate the soft-margin SVM objective using the **hinge loss**
$$ğ“›_\textrm{hinge}(t, y) â‰ \max(0, 1 - t y)$$
to
$$\argmin_{â†’w,b} C âˆ‘_i ğ“›_\textrm{hinge}\big(t_i, y(â†’x_i)\big) + \frac{1}{2} \|â†’w\|^2 .$$

~~~
Such formulation is analogous to a regularized loss, where $C$ is an _inverse_
regularization strength, so $C=âˆ$ implies no regularization, and $C=0$ ignores
the data entirely.

---
class: tablewide
# Comparison of Linear and Logistic Regression and SVM

For $y(â†’x; â†’w, b) â‰ â†’Ï†(â†’x)^T â†’w + b$, we have seen the following losses:

| Model | Objective | Per-Instance Loss |
|:------|:----------|:------------------|
| Linear<br>Regression | $\small \argmin_{â†’w,b} âˆ‘_i ğ“›_\textrm{MSE}\big(t_i, y(â†’x_i)\big) + \frac{1}{2} Î» \|â†’w\|^2$ | $\small ğ“›_\textrm{MSE}(t, y) = \frac{1}{2}(t - y)^2$ |
| Logistic<br>regression | $\small \argmin_{â†’w,b} âˆ‘_i ğ“›_\textrm{Ïƒ-NLL}\big(t_i, y(â†’x_i)\big) + \frac{1}{2} Î» \|â†’w\|^2$ | $\small ğ“›_\textrm{Ïƒ-NLL}(t, y) = - \log \begin{pmatrix}Ïƒ(y)^t â‹…\\ â‹…\big(1-Ïƒ(y)\big)^{1-t}\end{pmatrix}$ |
| Softmax<br>regression | $\small \argmin_{â‡‰W,â†’b} âˆ‘_i ğ“›_\textrm{s-NLL}\big(t_i, â†’y(â†’x_i)\big) + \frac{1}{2} Î» \|â†’w\|^2$ | $\small ğ“›_\textrm{s-NLL}(t, â†’y) = - \log \softmax(â†’y)_t$ |
| SVM | $\small \argmin_{â†’w,b} C âˆ‘_i ğ“›_\textrm{hinge}\big(t_i, y(â†’x_i)\big) + \frac{1}{2} \|â†’w\|^2$ | $\small ğ“›_\textrm{hinge}(t, y) = \max(0, 1 - ty)$ |

~~~
Note that $\small ğ“›_\textrm{MSE}(t, y) âˆ -\log\big(ğ“(t; Î¼=y, Ïƒ^2=\mathrm{const})\big)$ and
$\small ğ“›_\textrm{Ïƒ-NLL}(t, y) = ğ“›_\textrm{s-NLL}(t, [y, 0])$.

---
# Binary Classification Loss Functions Comparison

To compare various functions for binary classification, we need to formulate
them all in the same settings, with $t âˆˆ \{-1, 1\}$.

~~~
- MSE: $\frac{1}{2}(ty - 1)^2$, because it is $(y - 1)^2$ for $t=1$, $(y+1)^2 = (-y - 1)^2$ for $t=-1$,
~~~
- LR: $-\log Ïƒ(ty)$, because it is $Ïƒ(y)$ for $t=1$ and $1-Ïƒ(y)=Ïƒ(-y)$ for $t=-1$,
~~~
- SVM: $\max(0, 1 - ty)$.

![w=42%,h=center](binary_losses.svgz)

---
section: SMO
# Sequential Minimal Optimization Algorithm

To solve the dual formulation of a SVM, usually the Sequential Minimal Optimization
(SMO; John Platt, 1998) algorithm is used.

~~~
Before we introduce it, we start with the **coordinate descent**
optimization algorithm.

~~~
Consider solving an unconstrained optimization problem
$$\argmin_{â†’w} L(w_1, w_2, â€¦, w_D).$$

~~~
Instead of the usual SGD approach, we could optimize the weights one by one,
using the following algorithm:

<div class="algorithm">

- loop until convergence
  - for $i$ in $\{1, 2, â€¦, D\}$:
    - $w_i â† \argmin\nolimits_{w_i} L(w_1, w_2, â€¦, w_D)$
</div>

---
# Sequential Minimal Optimization Algorithm

<div class="algorithm">

- loop until convergence
  - for $i$ in $\{1, 2, â€¦, D\}$:
    - $w_i â† \argmin\nolimits_{w_i} L(w_1, w_2, â€¦, w_D)$
</div>

![w=42%,f=right](coordinate_descent.svgz)

If the inner $\argmin$ can be performed efficiently, the coordinate descent can
be fairly efficient.

~~~
Note that we might want to choose $w_i$ in a different order, for example
by trying to choose $w_i$ providing the largest decrease of $L$.

~~~
The Kernel linear regression dual formulation with single-example batches
was in fact trained by a coordinate descent â€“ updating a single $Î²_i$
corresponds to updating weights for a single example.

---
style: .katex-display { margin: .9em 0 }
# Sequential Minimal Optimization Algorithm

In soft-margin SVM, we try to maximize
$$ğ“› = âˆ‘\nolimits_i a_i -  \frac{1}{2} âˆ‘\nolimits_i âˆ‘\nolimits_j a_i a_j t_i t_j K(â†’x_i, â†’x_j)$$
with respect to $a_i$, such that
$$âˆ€\,i: C â‰¥ a_i â‰¥ 0\textrm{~~and~~}âˆ‘\nolimits_i a_i t_i = 0.$$

~~~
The third KKT conditions of the solution, namely $a_i (t_i y(â†’x_i) + Î¾_i - 1) = 0$ and $Î¼_i Î¾_i = 0$,
can be reformulated as the following equivalent implications (while assuming the
rest of the KKT conditions, so $a_i â‰¥ 0$, $t_i y(â†’x_i) â‰¥ 1 - Î¾_i$, $Î¼_i â‰¥ 0$, $Î¾_i â‰¥ 0$):
$$\begin{aligned}
  a_i > 0 & â‡’ t_i y(â†’x_i) â‰¤ 1,~\textrm{ because }t_i y(â†’x_i) + Î¾_i - 1 = 0 â‡” t_i y(â†’x_i) â‰¤ 1,\\
  a_i < C & â‡’ t_i y(â†’x_i) â‰¥ 1,~\textrm{ because }Î¼_i > 0 â‡” a_i < C~\textrm{ and }~Î¾_i = 0 â‡” t_i y(â†’x_i) â‰¥ 1. \\
\end{aligned}$$

Note that when $0 < a_i < C$, we get $t_i y(â†’x_i) = 1$ by combining both the implications.

---
# Sequential Minimal Optimization Algorithm

At its core, the SMO algorithm is just a coordinate descent.

~~~
It tries to find $a_i$ maximizing $ğ“›$ while fulfilling the KKT conditions â€“ once
found, an optimum has been reached, given that for soft-margin SVM the KKT
conditions are sufficient conditions for optimality (for soft-margin SVM, the
loss is convex and the inequality constraints are not only convex but even
affine).

~~~
However, note that because of the $âˆ‘_i a_i t_i = 0$ constraint, we cannot optimize
just one $a_i$, because a single $a_i$ is determined from the others.
~~~
Therefore, in each step, we pick two $a_i, a_j$ coefficients and try to maximize
the loss while fulfilling the constraints.

~~~
<div class="algorithm">

- loop until convergence (until $âˆ€\,i: a_i < C â‡’ t_i y(â†’x_i) â‰¥ 1$ and $a_i > 0 â‡’  t_i y(â†’x_i) â‰¤ 1$)
  - for $i$ in $\{1, 2, â€¦, N\}$:
    - choose $j â‰  i$ in $\{1, 2, â€¦, N\}$
    - $a_i, a_j â† \argmax\nolimits_{a_i, a_j} ğ“›(a_1, a_2, â€¦, a_N)$, while
      respecting the constraints:
      - $0 â‰¤ a_i â‰¤ C$, $0 â‰¤ a_j â‰¤ C$, $âˆ‘_i a_i t_i = 0$
</div>

---
# Sequential Minimal Optimization Algorithm

The SMO is an efficient algorithm because we can compute the update to
$a_i, a_j$ efficiently, given that there exists a closed-form solution.

~~~
Assume that we are updating $a_i$ and $a_j$. Then using the condition $âˆ‘_k a_k t_k = 0$ we can
write $a_i t_i = -âˆ‘_{kâ‰ i} a_k t_k$. Given that $t_i^2 = 1$ and denoting $Î¶=-âˆ‘_{kâ‰ i, kâ‰ j} a_k t_k$, we get
$$a_i = t_i (Î¶-a_j t_j).$$

~~~
Maximizing $ğ“›(â†’a)$ with respect to $a_i$ and $a_j$ then amounts to maximizing
a quadratic function of $a_j$, which has an analytical solution.

~~~
Note that the real SMO algorithm employs several heuristics for choosing $a_i, a_j$
such that the $ğ“›$ can be maximized the most.

---
# Sequential Minimal Optimization Algorithm Part I

<div class="algorithm">

**Input**: Dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{-1,1\}^N$), kernel $â‡‰K$, regularization parameter $C$, tolerance $\mathit{tol}$,
$\mathit{max\_passes\_without\_as\_changing}$ value

- Initialize $â†’a â† â†’0$, $b â† 0$, $\mathit{passes} â† 0$
- **while** $\mathit{passes} < \mathit{max\_passes\_without\_as\_changing}$ (or
  we run out of patience):
  - $\mathit{changed\_as} â† 0$
  - **for** $i$ in $1, 2, â€¦, N$:
    - $E_i â† y(â†’x_i) - t_i$
    - **if** ($a_i < C-\mathit{tol}$ **and** $t_i E_i < -\mathit{tol}$) **or** ($a_i > \mathit{tol}$ **and** $t_i E_i > \mathit{tol}$):
      - choose $j â‰  i$ randomly
      - try updating $a_i$, $a_j$ to maximize $ğ“›(a_1, a_2, â€¦, a_N)$ such that
        $0 â‰¤ a_k â‰¤ C$ and $âˆ‘_i a_i t_i = 0$; if successful, set $b$ to fulfill
        the KKT conditions and set $\mathit{changed\_as} â† \mathit{changed\_as} + 1$
  - $\mathit{passes} â† 0$ **if** $\mathit{changed\_as}$ **else** $\mathit{passes} + 1$
</div>

---
# Sequential Minimal Optimization Update Rules

We already know that $a_i = t_i (Î¶-a_j t_j).$

~~~
To find $a_j$ maximizing $ğ“›$, we use the formula for locating a vertex of
a parabola
$$a_j^\textrm{new} â† a_j - \frac{âˆ‚ğ“›/âˆ‚a_j}{âˆ‚^2ğ“›/âˆ‚a_j^2},$$
which is in fact one Newton-Raphson iteration step.

~~~
Denoting $E_j â‰ y(â†’x_j) - t_j$, we can compute the first derivative as
$$\frac{âˆ‚ğ“›}{âˆ‚a_j} = t_j (E_i - E_j),$$
~~~
and the second derivative as
$$\frac{âˆ‚^2ğ“›}{âˆ‚a_j^2} = 2K(â†’x_i, â†’x_j) - K(â†’x_i, â†’x_i) - K(â†’x_j, â†’x_j)
  \textcolor{gray}{= -\big\|Ï†(â†’x_i) - Ï†(â†’x_j)\big\|^2 â‰¤ 0.}$$

---
# Sequential Minimal Optimization Update Rules

If the second derivative is strictly negative, we know that the vertex is really
a maximum, in which case we get

$$a_j^\textrm{new} â† a_j - t_j\frac{E_i - E_j}{2K(â†’x_i, â†’x_j) - K(â†’x_i, â†’x_i) - K(â†’x_j, â†’x_j)}.$$

~~~
However, our maximization is constrained â€“ it must hold that $0 â‰¤ a_i â‰¤ C$ and
$0 â‰¤ a_j â‰¤ C$.

~~~
![w=25%,f=right](box_constraints.svgz)

Recalling that $a_i = - t_i t_j a_j + \mathrm{const}$, we can plot the
dependence of $a_i$ and $a_j$. If for example $-t_i t_j=1$ and
$a_j^\textrm{new} > C$, we need to find the â€œright-mostâ€ solution
fulfilling both $a_i â‰¤ C$ and $a_j â‰¤ C$. Such a solution is either:
- when $a_j^\textrm{new}$ is clipped to $C$, as in the green case in the example,
~~~
- when $a_j^\textrm{new}$ is clipped so that $a_i^\textrm{new} = C$ (the purple
  case in the example), in which case $a_j^\textrm{new} = a_j + (C - a_i)$.

---
# Sequential Minimal Optimization Update Rules

If we consider both $t_i t_j = Â±1$ and $a_j^\textrm{new} < 0$,
$a_j^\textrm{new} >C$, we get that the value maximizing the
Lagrangian is $a_j^\textrm{new}$ clipped to range $[L, H]$, where
$$\begin{aligned}
t_i = t_j & â‡’ L = \max(0, a_i + a_j - C), H = \min(C, a_i + a_j) \\
t_i â‰  t_j & â‡’ L = \max(0, a_j - a_i), H = \min(C, C + a_j - a_i).
\end{aligned}$$

~~~
After obtaining $a_j^\textrm{new}$ we can compute $a_i^\textrm{new}$.
Remembering that $a_i = - t_i t_j a_j + \mathrm{const}$, we
can compute it efficiently as
$$a_i^\textrm{new} â† a_i - t_i t_j\big(a_j^\textrm{new} - a_j).$$

---
style: .katex-display { margin: .9em 0 }
# Sequential Minimal Optimization Update Rules

To arrive at the bias update, we consider the KKT condition that
for $0 < a_j^\textrm{new} < C$, it must hold that
$1 = t_j y(â†’x_j) = t_j\big[\big(âˆ‘_l a_l^\textrm{new} t_l K(â†’x_j, â†’x_l)\big) + b^\textrm{new}\big]$.
Combining it with the fact that $\big(âˆ‘_l a_l t_l K(â†’x_j, â†’x_l)\big) + b = E_j + t_j$,
we obtain
$$b_j^\textrm{new} = b - E_j - t_i (a_i^\textrm{new} - a_i)K(â†’x_i, â†’x_j) - t_j (a_j^\textrm{new} - a_j)K(â†’x_j, â†’x_j).$$

~~~
Analogously for $0 < a_i^\textrm{new} < C$ we get
$$b_i^\textrm{new} = b - E_i - t_i (a_i^\textrm{new} - a_i)K(â†’x_i, â†’x_i) - t_j (a_j^\textrm{new} - a_j)K(â†’x_j, â†’x_i).$$

~~~
Finally, if $a_j^\textrm{new}, a_i^\textrm{new} âˆˆ \{0, C\}$ and $L â‰  H$, it
can be shown that all values between $b_i^\textrm{new}$ and $b_j^\textrm{new}$ fulfill the KKT
conditions. We therefore arrive at the following update for the bias:
$$b^\textrm{new} = \begin{cases}
  b_i^\textrm{new} & \textrm{if~~} 0 < a_i^\textrm{new} < C, \\
  b_j^\textrm{new} & \textrm{if~~} 0 < a_j^\textrm{new} < C, \\
  (b_i^\textrm{new} + b_j^\textrm{new})/2 & \textrm{otherwise}.
\end{cases}$$

---
# Sequential Minimal Optimization Algorithm Part II

<div class="algorithm">

**Input**: Dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{-1,1\}^N$), kernel $â‡‰K$, regularization parameter $C$, tolerance $\mathit{tol}$,
$\mathit{max\_passes\_without\_as\_changing}$ value<br>

- Try updating $a_i$, $a_j$ and $b$ to fulfill the KKT conditions:
  - Find $a_j$ maximizing $ğ“›$, in which we express $a_i$ using $a_j$.
    - Such $ğ“›$ is a quadratic function of $a_j$.
    - If the second derivative of $ğ“›$ is not negative, stop.
  - Clip $a_j$ so that $0 â‰¤ a_i â‰¤ C$ and $0 â‰¤ a_j â‰¤ C$.
    - If we did not make enough progress (the new $a_j$ is very similar), revert
      the value of $a_j$ and stop.
  - Compute corresponding $a_i$.
  - Compute $b$ appropriate for the updated $a_i$, $a_j$.
</div>

---
section: Primal vs Dual
class: tablewide
style: td:nth-child(1) {width: 25%}  td:nth-child(2) {width: 35%}
# Primal versus Dual Formulation

Assume we have a dataset with $N$ training examples, each with $D$ features.
Also assume the used feature map $Ï†$ generates $F$ features.

| Property | Primal Formulation | Dual Formulation |
|:---------|:-------------------|:-----------------|
| Parameters | $F$ | $N$ |
~~~
| Model size | $F$ | $sâ‹…D$ for $s$ support vectors |
~~~
| Usual training time | $Î˜(e â‹… N â‹… F)$ for $e$ epochs | between $Î©(ND)$ and $ğ“(N^2D)$ |
~~~
| Inference time | $Î˜(F)$ | $Î˜(sâ‹…D)$ for $s$ support vectors |

---
# SVM With RBF

The SVM algorithm with RBF kernel implements a better variant of the k-NN algorithm,
weighting â€œevidenceâ€ of training data points according to their distance.

~~~
![w=80%,mw=50%,h=center](svm_rbf_data.svgz)![w=80%,mw=50%,h=center](svm_rbf_bins.svgz)

---
section: MultiSVM
# Multiclass SVM

There are two general approaches for building a $K$-class classifier by combining
several binary classifiers:

~~~
- **one-versus-rest** scheme: $K$ binary classifiers are constructed, the
  $i^\mathrm{th}$ separating instances of class $i$ from all others; during
  prediction, the one with the highest probability is chosen

  - the binary classifiers need to return calibrated probabilities (not SVM)

~~~
- **one-versus-one** scheme: $\binom{K}{2}$ binary classifiers are constructed,
  one for each $(i, j)$ pair of class indices; during prediction, the class with
  the majority of votes wins (used by SVM)

~~~
![w=52%,f=right](ovr_ovo_failures.svgz)

However, voting suffers from serious difficulties, because when the binary
classifiers are trained independently, usually large regions in the feature
space receive tied votes (and such regions are then ambiguous).

---
section: MultiSVM
# One-Versus-Rest Compared to Softmax Classification

![w=50%,v=middle](multiclass_ovr.svgz)![w=50%,v=middle](multiclass_softmax.svgz)

---
section: SVR
# SVM For Regression

![w=25%,f=right](svr_loss.svgz)

The idea of SVM for regression is to use an $Îµ$-insensitive error function
$$ğ“›_Îµ\big(t, y(â†’x)\big) = \max\big(0, |y(â†’x) - t| - Îµ\big).$$

~~~
The primary formulation of the loss is then
$$C âˆ‘_i ğ“›_Îµ\big(t_i, y(â†’x_i)\big) + \frac{1}{2} \|â†’w\|^2.$$

~~~
![w=25%,f=right](svr.svgz)

In the dual formulation, we require every training example to be within $Îµ$ of
its target, but introduce two slack variables $â†’Î¾^-$, $â†’Î¾^+$ to allow outliers. We therefore
minimize the loss
$$C âˆ‘_i (Î¾_i^- + Î¾_i^+) + \tfrac{1}{2} \|â†’w\|^2$$
while requiring for every example $t_i - Îµ - Î¾_i^- â‰¤ y(â†’x_i) â‰¤ t_i + Îµ + Î¾_i^+$ for $Î¾_i^- â‰¥ 0, Î¾_i^+ â‰¥ 0$.

---
# SVM For Regression

The Lagrangian after substituting for $â†’w$, $b$, $â†’Î¾^-$ and $â†’Î¾^+$ is
$$ğ“› = âˆ‘_i (a_i^+ - a_i^-) t_i - Îµ âˆ‘_i (a_i^+ + a_i^-)
      - \frac{1}{2} âˆ‘_i âˆ‘_j (a_i^+ - a_i^-) (a_j^+ - a_j^-) K(â†’x_i, â†’x_j)$$

![w=40%,f=right](svr_example.svgz)

subject to
$$\begin{gathered}
  0 â‰¤ a_i^+, a_i^- â‰¤ C,\\
  âˆ‘_i(a_i^+ - a_i^-) = 0.
\end{gathered}$$

~~~
The prediction is then given by
$$y(â†’z) = âˆ‘_i (a_i^+ - a_i^-) K(â†’z, â†’x_i) + b.$$

---
section: Demos
# Demos

## SVM Demos
- https://cs.stanford.edu/~karpathy/svmjs/demo/

## MLP Demos
- https://cs.stanford.edu/~karpathy/svmjs/demo/demonn.html

- https://playground.tensorflow.org

