title: NPFL129, Lecture 4
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Multiclass Logistic Regression, Support Vector Machine

## Milan Straka

### November 11, 2019

---
section: LogisticRegression
# Logistic Regression

An extension of perceptron, which models the conditional probabilities
of $p(C_0|â†’x)$ and of $p(C_1|â†’x)$. Logistic regression can in fact
handle also more than two classes, which we will see shortly.

~~~
Logistic regression employs the following parametrization of the conditional
class probabilities:
$$\begin{aligned}
  P(C_1 | â†’x) &= Ïƒ(â†’x^t â†’w + â†’b) \\
  P(C_0 | â†’x) &= 1 - P(C_1 | â†’x),
\end{aligned}$$
where $Ïƒ$ is a _sigmoid function_
$$Ïƒ(x) = \frac{1}{1+e^{-x}}.$$

~~~
Can be trained using an SGD algorithm.

---
# Sigmoid Function

The sigmoid function has values in range $(0, 1)$, is monotonically
increasing and it has a derivative of $\frac{1}{4}$ at $x=0$.

$$Ïƒ(x) = \frac{1}{1+e^{-x}}$$

~~~
$$Ïƒ'(x) = Ïƒ(x) \big(1 - Ïƒ(x)\big)$$

~~~
![w=100%](../03/sigmoid.pdf)

---
# Logistic Regression

To give some meaning to the sigmoid function, starting with
$$P(C_1 | â†’x) = Ïƒ(f(â†’x; â†’w)) = \frac{1}{1 + e^{-f(â†’x; â†’w)}}$$
~~~
we can arrive at
$$f(â†’x; â†’w) = \log\left(\frac{P(C_1 | â†’x)}{P(C_0 | â†’x)}\right),$$
where the prediction of the model $f(â†’x; â†’w)$ is called a _logit_
and it is a logarithm of odds of the two classes probabilities.

---
# Logistic Regression

To train the logistic regression $y(â†’x; â†’w) = â†’x^T â†’w$, we use MLE (the maximum likelihood
estimation). Note that $P(C_1 | â†’x; â†’w) = Ïƒ(y(â†’x; â†’w))$.

~~~
Therefore, the loss for a batch $ğ•=\{(â†’x_1, t_1), (â†’x_2, t_2), â€¦, (â†’x_N, t_N)\}$
is
$$\begin{aligned}
ğ“›(ğ•) = \frac{1}{N} âˆ‘_i -\log(P(C_{t_i} | â†’x; â†’w)). \\
\end{aligned}$$

<div class="algorithm">

**Input**: Input dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{0, +1\}$), learning rate $Î± âˆˆ â„^+$.<br>

- $â†’w â† 0$
- until convergence (or until patience is over), process batch of $N$ examples:
  - $g â† - \tfrac{1}{N} âˆ‘_i âˆ‡_â†’w \log(P(C_{t_i} | â†’x; â†’w)$
  - $â†’w â† â†’w - Î±â†’g$
</div>

---
section: MulticlassLogisticRegression
# Multiclass Logistic Regression

To extend the binary logistic regression to a multiclass case with $K$ classes, we:
- Generate multiple outputs, notably $K$ outputs, each with its own set of
  weights, so that
  $$y(â†’x; â‡‰W)_i = â†’W_i â†’x.$$

~~~
- Generalize the sigmoid function to a $\softmax$ function, such that
  $$\softmax(â†’z)_i = \frac{e^{z_i}}{âˆ‘_j e^{z_j}}.$$

~~~
  Note that the original sigmoid function can be written as
  $$Ïƒ(x) = \softmax\big([x~~0]\big)_0 = \frac{e^x}{e^x + e^0} = \frac{1}{1 + e^{-x}}.$$

~~~
The resulting classifier is also known as _multinomial logistic regression_,
_maximum entropy classifier_ or _softmax regression_.

---
# Multiclass Logistic Regression

Note that as defined, the multiclass logistic regression is overparametrized.
It is possible to generate only $K-1$ outputs and define $z_K = 0$, analously to
binary logistic regression.

~~~
In this settings, analogously to binary logistic regression, we can recover the
interpretation of $\softmax$ inputs as _logits_:

$$\softmax(â†’z)_i = \log\left(\frac{P(C_i | â†’x; â†’w)}{P(C_K | â†’x; â†’w)}\right).$$

---
# Multiclass Logistic Regression

Using the $\softmax$ function, we naturally define that
$$P(C_i | â†’x; â‡‰W) = \softmax(â‡‰W_i â†’x)i = \frac{e^{â‡‰W_i â†’x}}{âˆ‘_j e^{â‡‰W_j â†’x}}.$$

~~~
We can then use MLE and train the model using stochastic gradient descent.

<div class="algorithm">

**Input**: Input dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{0, 1, â€¦, K-1\}$), learning rate $Î± âˆˆ â„^+$.<br>

- $â†’w â† 0$
- until convergence (or until patience is over), process batch of $N$ examples:
  - $g â† - \tfrac{1}{N} âˆ‘_i âˆ‡_â†’w \log(P(C_{t_i} | â†’x; â†’w)$
  - $â†’w â† â†’w - Î±â†’g$
</div>

---
# Multiclass Logistic Regression

Note that the decision regions of the multiclass logistic regression are singly
connected and convex.

![w=50%,h=center](../03/classification_convex_regions.pdf)

---
section: MSEasMLE
# Mean Square Error as MLE

During regression, we predict a number, not a real probability distribution.
In order to generate a distribution, we might consider a distribution with
the mean of the predicted value and a fixed variance $Ïƒ^2$ â€“ the most general
such a distribution is the normal distribution.

---
section: MSEasMLE
# Mean Square Error as MLE

Therefore, assume our model generates a distribution
$$P(y | â†’x; â†’w) = ğ“(y; f(â†’x; â†’w), Ïƒ^2).$$

~~~
Now we can apply MLE and get
$$\begin{aligned}
\argmax_â†’w P(ğ•; â†’w) =& \argmin_â†’w âˆ‘_{i=1}^m -\log P(y_i | â†’x_i ; â†’w) \\
                         =& -\argmin_â†’w âˆ‘_{i=1}^m \log \sqrt{\frac{1}{2Ï€Ïƒ^2}}
                            e ^ {\normalsize -\frac{(y_i - f(â†’x_i; â†’w))^2}{2Ïƒ^2}} \\
                         =& -\argmin_â†’w {\color{gray} m \log (2Ï€Ïƒ^2)^{-1/2} +}
                            âˆ‘_{i=1}^m -\frac{(y_i - f(â†’x_i; â†’w))^2}{2Ïƒ^2} \\
                         =& \argmin_â†’w âˆ‘_{i=1}^m \frac{(y_i - f(â†’x_i; â†’w))^2}{2Ïƒ^2} = \argmin_â†’w âˆ‘_{i=1}^m (y_i - f(â†’x_i; â†’w))^2.
\end{aligned}$$

---
# Short-term Future Plans

- Derive that normal distribution is really the one with maximum entropy given
  mean and variance.

- Derive that softmax is the one with maximum entropy given a reasonable
  constraint.

- Support Vector Machines (allow efficient handling of user-prescribed
  non-linear transformation of inputs).

- Decision trees, random forests, gradient boosted decision trees.

- K-means, mixture of Gaussians, EM
