title: NPFL129, Lecture 4
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Multiclass Logistic Regression, Multilayer Perceptron

## Milan Straka

### October 24, 2022

---
section: Refresh
# Logistic Regression

An extension of perceptron, which models the conditional probabilities
of $p(C_0|→x)$ and of $p(C_1|→x)$. Logistic regression can in fact
handle also more than two classes, which we will see shortly.

~~~
Logistic regression employs the following parametrization of the conditional
class probabilities:
$$\begin{aligned}
  p(C_1 | →x) &= σ(→x^T →w + b) \\
  p(C_0 | →x) &= 1 - p(C_1 | →x),
\end{aligned}$$

![w=60%,f=right](../03/sigmoid.svgz)

where $σ$ is a **sigmoid function**
$$σ(x) = \frac{1}{1+e^{-x}}.$$

~~~
It can be trained using the SGD algorithm.

---
# Logistic Regression

We denote the output of the “linear part” of the logistic regression as
$ȳ(→x; →w) = →x^T →w$
and the overall prediction as
$y(→x; →w) = σ(ȳ(→x; →w)) = σ(→x^T →w).$

~~~
The logistic regression output $y(→x; →w)$ models the probability of class
$C_1$, $p(C_1 | →x)$.

To give some meaning to the output of the linear part $ȳ(→x; →w)$, starting with
$$p(C_1 | →x) = σ(ȳ(→x; →w)) = \frac{1}{1 + e^{-ȳ(→x; →w)}},$$
~~~
we arrive at
$$ȳ(→x; →w) = \log\left(\frac{p(C_1 | →x)}{1 - p(C_1 | →x)}\right) = \log\left(\frac{p(C_1 | →x)}{p(C_0 | →x)}\right),$$
which is called a **logit** and it is a logarithm of odds of the probabilities
of the two classes.

---
# Logistic Regression

To train the logistic regression, we use MLE (the maximum likelihood
estimation). Its application is straightforward, given that $p(C_1 | →x; →w)$ is
directly the model output $y(→x; →y)$.

~~~
Therefore, the loss for a minibatch $⇉X=\{(→x_1, t_1), (→x_2, t_2), …, (→x_N, t_N)\}$
is
$$\begin{aligned}
E(→w) = \frac{1}{N} ∑_i -\log(p(C_{t_i} | →x_i; →w)). \\
\end{aligned}$$

<div class="algorithm">

**Input**: Input dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ \{0, +1\}^N$), learning rate $α ∈ ℝ^+$.<br>

- $→w ← 0$ or we initialize $→w$ randomly
- until convergence (or patience runs out), process a minibatch of examples with indices $→b$:
  - $→g ← \tfrac{1}{|→b|} ∑_{i∈→b} ∇_{→w} -\log\big(p(C_{t_i} | →x_i; →w)\big)$
  - $→w ← →w - α→g$
</div>

---
# Linearity in Logistic Regression

![w=100%](lr_linearity.svgz)

---
section: GLM
class: tablewide
# Generalized Linear Models

The logistic regression is in fact an extended linear regression. A linear
regression model, which is followed by some **activation function** $a$, is
called **generalized linear model**:
$$p(t | →x; →w, b) = y(→x; →w, b) = a\big(ȳ(→x; →w, b)\big) = a(x^T→w + b).$$

~~~
| Name | Activation | Distribution | Loss | Gradient |
|------|------------|--------------|------|----------|
| linear regression | identity | ? | $\textrm{MSE} ∝ 𝔼 (y(→x) - t)^2$ | $\big(y(→x) - t\big) →x$ |
~~~
| logistic regression | $σ(ȳ)$ | Bernoulli | $\textrm{NLL} ∝ 𝔼 -\log(p(t \vert →x))$ | ? |

---
# Logistic Regression Gradient

We start by computing the gradient of the $σ(x)$.

$\displaystyle \kern4em\mathllap{\frac{∂}{∂x} σ(x)} = \frac{∂}{∂x} \frac{1}{1 + e^{-x}}$

~~~
$\displaystyle \kern4em{}=\mathrlap{\frac{\textcolor{darkgreen}{\frac{∂}{∂x}-(1 + e^{-x})}}{(1 + e^{-x})^2}}
 \kern30em\mathllap{\frac{∂}{∂x} \frac{1}{g(x)} = -\frac{\frac{∂}{∂x} g(x)}{g(x)^2}}$

~~~
$\displaystyle \kern4em{}=\mathrlap{\textcolor{blue}{\frac{1}{1+e^{-x}}} ⋅ \frac{\textcolor{darkgreen}{e^{-x}}}{1 + e^{-x}}}
 \kern30em\mathllap{\tfrac{∂}{∂x} e^{g(x)} = e^{g(x)} ⋅ \tfrac{∂}{∂x} g(x)}$

~~~
$\displaystyle \kern4em{}=\mathrlap{\textcolor{blue}{σ(x)} ⋅ \frac{\textcolor{darkred}{e^{-x}+1}-\textcolor{darkorange}{1}}{1 + e^{-x}}}$

~~~
$\displaystyle \kern4em{}=\mathrlap{σ(x) ⋅ \big(\textcolor{darkred}{1}-\textcolor{darkorange}{σ(x)}\big)}$

---
# Logistic Regression Gradient

Consider the log likelihood of logistic regression $\log p(t|→x; →w)$. For
brevity, we denote $ȳ(→x; →w) = →x^T →w$ just as $ȳ$ in the following
computation.

~~~
Remembering that for $t∼\operatorname{Ber}(φ)$ we have $p(t) = φ^t (1-φ)^{1-t}$,
we can rewrite the log likelihood to:

~~~
$\displaystyle \kern6em\mathllap{\log p(t|→x; →w)} = \log σ(ȳ)^t \big(1-σ(ȳ)\big)^{1-t}$

~~~
$\displaystyle \kern6em{}=t ⋅ \log \big(σ(ȳ)\big) + (1-t) ⋅ \log\big(1-σ(ȳ)\big)$

---
# Logistic Regression Gradient

$\displaystyle ∇_{→w} -\log p(t|→x; →w) =$

$\displaystyle \kern1em{}= ∇_{→w}\Big(-t ⋅ \log \big(σ(ȳ)\big) - (1-t) ⋅ \log\big(1-σ(ȳ)\big)\Big)$

~~~
$\displaystyle \kern34em\mathllap{\frac{∂}{∂x}\log g(x) = \frac{1}{g(x)} ⋅ \frac{∂}{∂x} g(x)}$

~~~
$\displaystyle \kern1em{}=-t⋅\frac{1}{σ(ȳ)}⋅∇_{→w}σ(ȳ) - (1-t)⋅\frac{1}{1-σ(ȳ)}⋅∇_{→w}\big(1\textcolor{magenta}{-}σ(ȳ)\big)$

~~~
$\displaystyle \kern34em\mathllap{\frac{∂}{∂x}f\big(g(x)\big)
  = \frac{∂}{∂g(x)}f\big(g(x)\big) ⋅ \frac{∂}{∂x} g(x) = \frac{∂}{∂z}f(z) ⋅ \frac{∂}{∂x} g(x)}$

~~~
$\displaystyle \kern34em\mathllap{∇_{→w}σ(ȳ) = \tfrac{∂}{∂ȳ}σ(ȳ) ⋅ ∇_{→w}ȳ}$

~~~
$\displaystyle \kern1em{}=\mathrlap{-t⋅\frac{1}{\textcolor{darkgreen}{σ(ȳ)}}⋅\textcolor{darkgreen}{σ(ȳ)}⋅\big(1-σ(ȳ)\big)⋅\textcolor{orange}{∇_{→w}ȳ}
 \textcolor{magenta}{+} (1-t)⋅\frac{1}{\textcolor{blue}{1-σ(ȳ)}}⋅σ(ȳ)⋅\textcolor{blue}{\big(1-σ(ȳ)\big)}⋅\textcolor{orange}{∇_{→w}ȳ}}$

~~~
$\displaystyle \kern1em{}=\big(-t+\textcolor{darkred}{tσ(ȳ)}+σ(ȳ)-\textcolor{darkred}{tσ(ȳ)}\big)\textcolor{orange}{→x}$

~~~
$\displaystyle \kern1em{}=\big(y(→x; →w) - t\big)→x$

---
class: tablewide
# Generalized Linear Models

The logistic regression is in fact an extended linear regression. A linear
regression model, which is followed by some **activation function** $a$, is
called **generalized linear model**:
$$p(t | →x; →w, b) = y(→x; →w, b) = a\big(ȳ(→x; →w, b)\big) = a(x^T→w + b).$$

| Name | Activation | Distribution | Loss | Gradient |
|------|------------|--------------|------|----------|
| linear regression | identity | ? | $\textrm{MSE} ∝ 𝔼 (y(→x) - t)^2$ | $\big(y(→x) - t\big) →x$ |
| logistic regression | $σ(ȳ)$ | Bernoulli | $\textrm{NLL} ∝ 𝔼 -\log(p(t \vert →x))$ | $\color{red}\big(y(→x) - t\big) →x$ |

---
section: MSE as MLE
# Mean Square Error as MLE

During regression, we predict a number, not a real probability distribution.
In order to generate a distribution, we might consider a distribution with
the mean of the predicted value and a fixed variance $σ^2$ – the most general
such a distribution is the normal distribution.

![w=50%,h=center](constant_variance.svgz)

---
# Mean Square Error as MLE

Therefore, assume our model generates a distribution
$p(t | →x; →w) = 𝓝(t; y(→x; →w), σ^2)$.

![w=18%,f=right;position:absolute;right:.5em](normal_paranormal.png)
~~~

Now we can apply the maximum likelihood estimation and get

$\displaystyle \kern8em\mathllap{\argmax_{→w} p(→t|⇉X; →w)} = \argmin_{→w} ∑_{i=1}^N -\log p(t_i | →x_i ; →w)$

~~~
$\displaystyle \kern8em{} = \argmin_{→w} -∑_{i=1}^N \log \sqrt{\frac{1}{2πσ^2}} e ^ {\normalsize -\frac{(t_i - y(→x_i; →w))^2}{2σ^2}}$

~~~
$\displaystyle \kern8em{} = \argmin_{→w} {\color{gray} -N \log (2πσ^2)^{-1/2}} - ∑_{i=1}^N -\frac{\big(t_i - y(→x_i; →w)\big)^2}{2σ^2}$

~~~
$\displaystyle \kern8em{} = \argmin_{→w} ∑_{i=1}^N \frac{\big(t_i - y(→x_i; →w)\big)^2}{2σ^2} = \argmin_{→w} \frac{1}{N} ∑_{i=1}^N \big(y(→x_i; →w) - t_i\big)^2.$

---
class: tablewide
# Generalized Linear Models

We have therefore extended the GLM table to

| Name | Activation | Distribution | Loss | Gradient |
|------|------------|--------------|------|----------|
| linear regression | identity | $\color{red}\textrm{Normal}$ | $\color{red}\textrm{NLL} ∝ \textrm{MSE}$ | $\big(y(→x) - t\big) →x$ |
| logistic regression | $σ(ȳ)$ | Bernoulli | $\textrm{NLL} ∝ 𝔼 -\log(p(t \vert →x))$ | $\big(y(→x) - t\big) →x$ |

---
section: MulticlassLogisticReg
# Multiclass Logistic Regression

To extend the binary logistic regression to a multiclass case with $K$ classes, we:
- generate $K$ outputs, each with its own set of weights, so that for $⇉W ∈ ℝ^{D×K}$,
  $$→ȳ(→x; ⇉W) = →x^T ⇉W,\textrm{~~~or in other words,~~~}→ȳ(→x; ⇉W)_i = →x^T (⇉W_{*, i})$$

~~~
- generalize the sigmoid function to a $\softmax$ function, such that
  $$\softmax(→z)_i = \frac{e^{→z_i}}{∑_j e^{→z_j}}.$$

~~~
  Note that the original sigmoid function can be written as
  $$σ(x) = \softmax\big([x~~0]\big)_0 = \frac{e^x}{e^x + e^0} = \frac{1}{1 + e^{-x}}.$$

~~~
The resulting classifier is also known as **multinomial logistic regression**,
**maximum entropy classifier** or **softmax regression**.

---
# Multiclass Logistic Regression

Using the $\softmax$ function, we naturally define that
$$p(C_i | →x; ⇉W) = →y(→x; ⇉W)_i = \softmax\big(→ȳ(→x; ⇉W)\big)_i = \softmax(→x^T ⇉W)_i = \frac{e^{(→x^T⇉W)_i}}{∑_j e^{(→x^T ⇉W)_j}}.$$

~~~

Considering the definition of the $\softmax$ function,
it is natural to obtain the interpretation of the linear part of the model
$→ȳ(→x; ⇉W)$ as **logits** by computing a logarithm of the above:
$$→ȳ(→x; ⇉W)_i = \log(p(C_i | →x; ⇉W)) + c.$$

~~~
The constant $c$ is present, because the output of the model is
_overparametrized_ (for example, the probability of the last class could be
computed from the remaining ones). This is connected to the fact that softmax
is
 invariant to addition of a constant:
$$\softmax(→z + →c)_i = \frac{e^{→z_i + c}}{∑_j e^{→z_j + c}} = \frac{e^{→z_i}}{∑_j e^{→z_j}}⋅\frac{e^c}{e^c} = \softmax(→z)_i.$$

---
# Multiclass Logistic Regression

The difference between softmax and sigmoid output can be compared on the binary
case, where the binary logistic regression outputs of the linear part of the model are
$$ȳ(→x; →w) = \log\left(\frac{p(C_1 | →x; →w)}{p(C_0 | →x; →w)}\right),$$
while the outputs of the softmax variant with two outputs can be interpreted as<br>
$→ȳ(→x; ⇉W)_0 = \log(p(C_0 | →x; ⇉W)) + c$ and $→ȳ(→x; ⇉W)_1 = \log(p(C_1 | →x; ⇉W)) + c$.

~~~
If we consider $→ȳ(→x; ⇉W)_0$ to be zero, the model can then predict only the
probability $p(C_1 | →x)$, and the constant $c$ is fixed to $-\log(p(C_0 | →x; ⇉W))$, recovering
the original interpretation.

~~~
Generalizing to a $K$-class classification, we could produce only $K-1$ outputs
and define $→ȳ_0=0$, resulting in the interpretation of the linear part outputs
analogous to the binary case:

$$→ȳ(→x; ⇉W)_i = \log\left(\frac{p(C_i | →x; ⇉W)}{p(C_0 | →x; ⇉W)}\right).$$

---
# Multiclass Logistic Regression

To train $K$-class classification, analogously to the binary logistic regression
we can use MLE and train the model using minibatch stochastic gradient descent:

<div class="algorithm">

**Input**: Input dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ \{0, 1, …, K-1\}^N$), learning rate $α ∈ ℝ^+$.<br>
**Model**: Let $→w$ denote all parameters of the model (in our case, the parameters are a weight
matrix $⇉W$ and maybe a bias vector $→b$).

- $→w ← 0$ or we initialize $→w$ randomly
- until convergence (or patience runs out), process a minibatch of examples with indices $→b$:
  - $→g ← \tfrac{1}{|→b|} ∑_{i∈→b} ∇_{→w} -\log\big(p(C_{t_i} | →x_i; →w)\big)$
  - $→w ← →w - α→g$
</div>

---
# Multiclass Logistic Regression

![w=41%,f=right](classification_convex_regions.svgz)

Note that the decision regions of the binary/multiclass logistic regression are
convex (and therefore connected).

~~~
To see this, consider $→x_A$ and $→x_B$ in the same decision region $R_k$.

~~~
Any point $→x$ lying on the line connecting them is their convex combination,
$→x = λ→x_A + (1-λ)→x_B$,
~~~
and from the linearity of $→ȳ(→x) = →x^T ⇉W$ it follows that
$$→ȳ(→x) = λ→ȳ(→x_A) + (1-λ)→ȳ(→x_B).$$

~~~
Given that $→ȳ(→x_A)_k$ was the largest among $→ȳ(→x_A)$ and also
given that $→ȳ(→x_B)_k$ was the largest among $→ȳ(→x_B)$, it must
be the case that $→ȳ(→x)_k$ is the largest among all $→ȳ(→x)$.

---
class: tablewide
# Generalized Linear Models

The multiclass logistic regression can now be added to the GLM table:

| Name | Activation | Distribution | Loss | Gradient |
|------|------------|--------------|------|----------|
| linear regression | identity | Normal | $\textrm{NLL} ∝ \textrm{MSE}$ | $\big(y(→x) - t\big) →x$ |
| logistic regression | $σ(ȳ)$ | Bernoulli | $\textrm{NLL} ∝ 𝔼 -\log(p(t \vert →x))$ | $\big(y(→x) - t\big) →x$ |
| multiclass<br>logistic regression | $\small\operatorname{softmax}(→ȳ)$ | categorical | $\textrm{NLL} ∝ 𝔼 -\log(p(t \vert →x))$ | $\big(→y(→x) - →1_t\big) →x^T$<br>wrt $⇉W^T ∈ ℝ^{K×D}$|

\
Recall that $→1_t = \big([i = t]\big)_{i=0}^{K-1}$ is one-hot representation of target $t ∈ \{0, 1, …, K-1\}$.

---
section: PoissonReg
class: tablewide
# Poisson Regression

Several other GLMs exist, we now describe a final one, this time for
regression and not for classification. Compared to regular linear regression,
where we assume the output distribution is normal, we turn our attention to
**Poisson distribution**.

~~~
## Poisson Distribution

Poisson distribution is a discrete distribution suitable for modeling the
probability of a given number of events occurring in a fixed time interval,
if these events occur at a known rate and independently of each other.

~~~
![w=87%,mw=35%,f=right](poisson_pmf.svgz)

$$P(⁇x = k; λ) = \frac{λ^k e^{-λ}}{k!}$$

~~~
It is easy to show that if $⁇x$ has Poisson distribution,
$$\begin{gathered}
  𝔼[x] = λ\\
  \Var(x) = λ
\end{gathered}$$

---
class: dbend
# Poisson Distribution Derivation

The Poisson distribution can be obtained as a limit of the binomial distribution.

~~~
Assume we are considering $n$ independent events, each with probability $p_n$,
and that $n p_n$ converges to $λ$.
~~~
Then
$$\lim_{n → ∞} {n \choose k} p_n^k (1-p_n)^{n-k} = e^{-λ}\frac{λ^k}{k!}.$$

~~~
$$\begin{aligned}
\lim_{n → ∞}{n \choose k} p_n^k (1-p_n)^{n-k}
  =& \lim_{n → ∞}\frac{n(n-1)(n-2)⋯(n-k+1)}{k!} \left(\frac{λ}{n}\right)^k \left(1- \frac{λ}{n}\right)^{n-k} \\
  =& \lim_{n → ∞}\frac{n^k+𝓞(n^{k-1})}{k!}\frac{λ^k}{n^k} \left(1- \frac{λ}{n}\right)^{n-k} = \lim_{n → ∞}\frac{λ^k}{k!} \left(1-\frac{λ}{n}\right)^{n-k}
\end{aligned}$$
~~~
and the result follows, since $\lim_{n → ∞} \left(1-\frac{λ}{n}\right)^{n} = e^{-λ}$ and 
$\lim_{n → ∞} \left(1- \frac{λ}{n}\right)^{-k}=1$.

---
# Poisson Distribution

An important difference compared to the normal distribution is that the latter
assumes that the variance does not depend on the mean, i.e., that the model
“makes errors of the same magnitude everywhere”.

~~~
On the other hand, the variance of a Poisson distribution increases with the
mean. It is useful if we want to measure error relatively, not as an absolute
difference.

![w=47%,h=center](poisson_vs_normal.png)

---
# Poisson Regression

Poisson regression is a generalized linear model producing a Poisson
distribution (i.e., the mean rate $λ$).

~~~
Again, we use NLL as the loss. To choose a suitable activation, we might be
interested in obtaining the same gradient as for other GLMs – solving for
an activation function while requiring the gradient to be $\big(a(ȳ(→x)) - t\big) ⋅ →x$
yields $a(ȳ) = \exp(ȳ)$, which means the linear part of the model is predicting $\log(λ)$.

| Name | Activation | Distribution | Loss | Gradient |
|------|------------|--------------|------|----------|
| linear regression | identity | Normal | $\textrm{NLL} ∝ \textrm{MSE}$ | $\big(y(→x) - t\big) →x$ |
| logistic regression | $σ(ȳ)$ | Bernoulli | $\textrm{NLL} ∝ 𝔼 -\log(p(t \vert →x))$ | $\big(y(→x) - t\big) →x$ |
| multiclass<br>logistic regression | $\small\operatorname{softmax}(→ȳ)$ | categorical | $\textrm{NLL} ∝ 𝔼 -\log(p(t \vert →x))$ | $\big(→y(→x) - →1_t\big) →x^T$<br>wrt $⇉W^T ∈ ℝ^{K×D}$ |
| Poisson regression | $\small\exp(ȳ)$ | Poisson | $\textrm{NLL} ∝ 𝔼 -\log(p(t \vert →x))$ | $\big(y(→x) - t\big) →x$ |


---
section: MLP
# Multilayer Perceptron

We can reformulate the generalized linear models in the following framework.

![w=90%,h=center,mw=35%,f=right](mlp_no_hidden.svgz)

~~~
- Assume we have an input node for every input feature.

~~~
- Additionally, we have an output node for every model output (one for linear
  regression or binary classification, $K$ for classification in $K$ classes).

~~~
- Every input node and output node are connected with a directed edge, and
  every edge has an associated weight.

~~~
- Value of every (output) node is computed by summing the values of predecessors
  multiplied by the corresponding weights, added to a bias of this node, and
  finally passed through an activation function $a$:
  $$y_i = a\left(∑\nolimits_j x_j w_{j,i} + b_i\right)$$
  or in matrix form $→y = a(→x^T⇉W+→b)$, or for a batch of examples $⇉X$,
  $⇉Y = a(⇉X ⇉W + →b)$.

---
# Multilayer Perceptron

We now extend the model by adding a **hidden layer** with activation $f$.

![w=45%,f=right](mlp.svgz)

~~~
- The computation is performed analogically:
  $$\begin{aligned}
    h_i &= f\left(∑\nolimits_j x_j w^{(h)}_{j,i} + b^{(h)}_i\right), \\
    y_i &= a\left(∑\nolimits_j h_j w^{(y)}_{j,i} + b^{(y)}_i\right),
  \end{aligned}$$
  or in matrix form
  $$\begin{aligned}
    →h &= f\Big(→x^T ⇉W^{(h)} + →b^{(h)}\Big), \\
    →y &= a\Big(→h^T ⇉W^{(y)} + →b^{(y)}\Big),
  \end{aligned}$$
  and for batch of inputs $⇉H = f\Big(⇉X ⇉W^{(h)} + →b^{(h)}\Big)$ and $⇉Y = a\Big(⇉H ⇉W^{(y)} + →b^{(y)}\Big)$.

---
# Multilayer Perceptron

Note that:
- the structure of the _input_ layer depends on the input features;

~~~
- the structure and the _activation_ function of the _output_ layer depends
  on the target data;
~~~
- however, the _hidden_ layer has no pre-image in the data and is completely arbitrary
  – which is the reason why it is called a _hidden_ layer.

~~~
![w=76%,h=center,mw=50%,f=right](mlp_bias.svgz)

Also note that we can absorb biases into weights analogously to the generalized
linear models.

---
# Output Layer Activation Functions
## Output Layer Activation Functions
- regression:

  - identity activation: we model normal distribution on output (linear
    regression)
~~~
  - $\exp(x)$: we model Poisson distribution on output (Poisson regression)

~~~
- binary classification:
  - $σ(x)$: we model the Bernoulli distribution (the model predicts
    a probability)
    $$σ(x) ≝ \frac{1}{1 + e^{-x}}$$

~~~
- $K$-class classification:
  - $\softmax(→x)$: we model the (usually overparametrized) categorical distribution
    $$\softmax(→x) ∝ e^{→x},~~~\softmax(→x)_i ≝ \frac{e^{→x_i}}{∑_j e^{→x_j}}$$

---
# Hidden Layer Activation Functions
## Hidden Layer Activation Functions
- no activation (identity): does not help, composition of linear mapping is a linear mapping

~~~
- $σ$ (but works suboptimally – nonsymmetrical, $\frac{dσ}{dx}(0) = 1/4$)

~~~
- ![w=95%,h=right,mw=61%,f=right](sigmoid_to_tanh.svgz)

  $\tanh$

  - result of making $σ$ symmetrical and making derivation in zero 1
  - $\tanh(x) = 2σ(2x) - 1$

~~~
- ReLU
  - $\max(0, x)$
  - the most common non-linear activation used nowadays

---
# Training MLP

The multilayer perceptron can be trained using again a minibatch SGD algorithm:

<div class="algorithm">

**Input**: Input dataset ($⇉X ∈ ℝ^{N×D}$, $→t$ targets), learning rate $α ∈ ℝ^+$.<br>
**Model**: Let $→w$ denote all parameters of the model (all weight matrices and bias vectors).

- initialize $→w$
  - set weights randomly, for example in $U\big[-1/\sqrt M, 1/\sqrt M\big]$ range,
    where $M$ is the size of the layer computed by the corresponding edge
  - set biases to 0
- until convergence (or patience runs out), process a minibatch of examples with indices $→b$:
  - $→g ← \tfrac{1}{|→b|} ∑_{i∈→b} ∇_{→w} -\log\big(p(t_i | →x_i; →w)\big)$
  - $→w ← →w - α→g$
</div>

---
# Training MLP – Computing the Derivatives

![w=20%,f=right](mlp.svgz)

Assume we have an MLP with input of size $D$, weights $⇉W^{(h)} ∈ ℝ^{D × H}$,
$→b^{(h)} ∈ ℝ^H$, hidden layer of size $H$ and activation $f$ with weights
$⇉W^{(y)} ∈ ℝ^{H × K}$, $→b^{(y)} ∈ ℝ^K$, and finally an output layer of size $K$ with
activation $a$.

~~~
In order to compute the gradient of the loss $L$ with respect to all weights, you
should proceed gradually:
- first compute $\frac{∂L}{∂→y}$,

~~~
- then compute $\frac{∂→y}{∂→y^\mathit{(in)}}$, where $→y^\mathit{(in)}$ are the
  inputs to the output layer (i.e., before applying activation function $a$;
  in other words, $→y = a(→y^\mathit{(in)})$),
~~~
- then compute $\frac{∂→y^\mathit{(in)}}{∂⇉W^{(y)}}$ and $\frac{∂→y^\mathit{(in)}}{∂→b^{(y)}}$,
  which allows us to obtain $\frac{∂L}{∂⇉W^{(y)}} = \frac{∂L}{∂→y}
  ⋅ \frac{∂→y}{∂→y^\mathit{(in)}} ⋅ \frac{∂→y^\mathit{(in)}}{∂⇉W^{(y)}}$ and analogously
  $\frac{∂L}{∂→b^{(y)}}$,
~~~
- followed by $\frac{∂→y^\mathit{(in)}}{∂→h}$ and $\frac{∂→h}{∂→h^\mathit{(in)}}$,
~~~
- and finally using $\frac{∂→h^\mathit{(in)}}{∂⇉W^{(h)}}$ and
  $\frac{∂→h^\mathit{(in)}}{∂→b^{(h)}}$ to compute $\frac{∂L}{∂⇉W^{(h)}}$ and
  $\frac{∂L}{∂→b^{(h)}}$.

---
# Hidden Layer Interpretation and Initialization

![w=90%,h=center,mw=45%,f=right](lr_linearity.svgz)

One way how to interpret the hidden layer is:
- the part from the hidden layer to the output layer is the previously
  used generalized linear model (linear regression, logistic regression, …);

~~~
- the part from the inputs to the hidden layer can be considered automatically
  constructed features. The features are a linear mapping of the input values
  followed by a non-linearity, and the theorem on the next slide proves they can
  always be constructed to achieve as good a fit of the training data as is required.

~~~
Note that the weights in an MLP must be initialized randomly. If we used just
zeros, all the constructed features (hidden layer nodes) would behave
identically and we would never distinguish them.

Using random weights corresponds to starting with random features, which allows the SGD
to make progress (improve the individual features).

---
section: UniversalApproximation
# Universal Approximation Theorem '89

Let $φ(x):ℝ → ℝ$ be a nonconstant, bounded and nondecreasing continuous function. \
(Later a proof was given also for $φ = \ReLU$ and even for any nonpolynomial
function.)

~~~
For any $ε > 0$ and any continuous function $f: [0, 1]^D → ℝ$, there exists
$H ∈ ℕ$, $→v ∈ ℝ^H$, $→b ∈ ℝ^H$ and $⇉W ∈ ℝ^{D×H}$, such that if we denote
$$F(→x) = →v^T φ(→x^T ⇉W + →b) = ∑_{i=1}^H v_i φ(→x^T →{W_{*, i}} + b_i),$$
where $φ$ is applied elementwise,
~~~
then for all $→x ∈ [0, 1]^D$:
$$|F(→x) - f(→x)| < ε.$$

---
class: dbend
# Universal Approximation Theorem for ReLUs

Sketch of the proof:

~~~
- If a function is continuous on a closed interval, it can be approximated by
  a sequence of lines to arbitrary precision.

![w=50%,h=center](universal_approximation_example.png)

~~~
- However, we can create a sequence of $k$ linear segments as a sum of $k$ ReLU
  units – on every endpoint a new ReLU starts (i.e., the input ReLU value is
  zero at the endpoint), with a tangent which is the difference between the
  target tangent and the tangent of the approximation until this point.

---
class: dbend
# Universal Approximation Theorem for Squashes

Sketch of the proof for a squashing function $φ(x)$ (i.e., nonconstant, bounded and
nondecreasing continuous function like sigmoid):

~~~
- We can prove $φ$ can be arbitrarily close to a hard threshold by compressing
  it horizontally.

![w=38%,h=center](universal_approximation_squash.png)

~~~
- Then we approximate the original function using a series of straight line
  segments

![w=38%,h=center](universal_approximation_rectangles.png)
