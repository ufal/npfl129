title: NPFL129, Lecture 4
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Multiclass Logistic Regression, Multilayer Perceptron

## Milan Straka

### October 26, 2020

---
section: Refresh
# Logistic Regression

An extension of perceptron, which models the conditional probabilities
of $p(C_0|â†’x)$ and of $p(C_1|â†’x)$. Logistic regression can in fact
handle also more than two classes, which we will see shortly.

~~~
Logistic regression employs the following parametrization of the conditional
class probabilities:
$$\begin{aligned}
  p(C_1 | â†’x) &= Ïƒ(â†’x^t â†’w + b) \\
  p(C_0 | â†’x) &= 1 - p(C_1 | â†’x),
\end{aligned}$$

![w=60%,f=right](../03/sigmoid.svgz)

where $Ïƒ$ is a **sigmoid function**
$$Ïƒ(x) = \frac{1}{1+e^{-x}}.$$

~~~
It can be trained using the SGD algorithm.

---
# Logistic Regression

To give some meaning to the sigmoid function, starting with
$$p(C_1 | â†’x) = Ïƒ(y(â†’x; â†’w)) = \frac{1}{1 + e^{-y(â†’x; â†’w)}}$$
~~~
we can arrive at
$$y(â†’x; â†’w) = \log\left(\frac{p(C_1 | â†’x)}{p(C_0 | â†’x)}\right),$$
where the prediction of the model $y(â†’x; â†’w)$ is called a **logit**
and it is a logarithm of odds of the two classes probabilities.

---
# Logistic Regression

To train the logistic regression $y(â†’x; â†’w) = â†’x^T â†’w$, we use MLE (the maximum likelihood
estimation). Note that $p(C_1 | â†’x; â†’w) = Ïƒ(y(â†’x; â†’w))$.

~~~
Therefore, the loss for a batch $ğ•=\{(â†’x_1, t_1), (â†’x_2, t_2), â€¦, (â†’x_N, t_N)\}$
is
$$\begin{aligned}
ğ“›(ğ•) = \frac{1}{N} âˆ‘_i -\log(p(C_{t_i} | â†’x_i; â†’w)). \\
\end{aligned}$$

<div class="algorithm">

**Input**: Input dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{0, +1\}^N$), learning rate $Î± âˆˆ â„^+$.<br>

- $â†’w â† 0$
- until convergence (or until patience is over), process batch of $N$ examples:
  - $g â† - \tfrac{1}{N} âˆ‘_i âˆ‡_â†’w \log(p(C_{t_i} | â†’x_i; â†’w)$
  - $â†’w â† â†’w - Î±â†’g$
</div>

---
# Linearity in Logistic Regression

![w=100%](lr_linearity.svgz)

---
section: GLM
class: tablewide
# Generalized Linear Models

The logistic regression is in fact an extended linear regression. A linear
regression model, which is followed by some **activation function** $a$, is
called **generalized linear model**:
$$p(t | â†’x; â†’w, b) = a\big(y(â†’x; â†’w, b)\big) = a(x^Tâ†’w + b).$$

~~~
| Name | Activation | Distribution | Loss | Gradient |
|------|------------|--------------|------|----------|
| linear regression | identity | ? | $\textrm{MSE} âˆ âˆ‘ (t - y(â†’x))^2$ | $\big(t - y(â†’x)\big) â‹… â†’x$ |
~~~
| logistic regression | $Ïƒ(x)$ | Bernoulli | $\textrm{NLL} âˆ âˆ‘ -\log(p(t \vert â†’x))$ | ? |
~~~ ~~
| logistic regression | $Ïƒ(x)$ | Bernoulli | $\textrm{NLL} âˆ âˆ‘ -\log(p(t \vert â†’x))$ | $\color{red}\big(t - a(y(â†’x))\big) â‹… â†’x$ |

---
section: MSE as MLE
# Mean Square Error as MLE

During regression, we predict a number, not a real probability distribution.
In order to generate a distribution, we might consider a distribution with
the mean of the predicted value and a fixed variance $Ïƒ^2$ â€“ the most general
such a distribution is the normal distribution.

---
# Mean Square Error as MLE

Therefore, assume our model generates a distribution
$p(t | â†’x; â†’w) = ğ“(t; y(â†’x; â†’w), Ïƒ^2)$.

~~~
Now we can apply MLE and get
$$\begin{aligned}
\argmax_â†’w p(ğ•; â†’w) =& \argmin_â†’w âˆ‘_{i=1}^N -\log p(t_i | â†’x_i ; â†’w) \\
                         =& -\argmin_â†’w âˆ‘_{i=1}^N \log \sqrt{\frac{1}{2Ï€Ïƒ^2}}
                            e ^ {\normalsize -\frac{(t_i - y(â†’x_i; â†’w))^2}{2Ïƒ^2}} \\
                         =& -\argmin_â†’w {\color{gray} N \log (2Ï€Ïƒ^2)^{-1/2} +}
                            âˆ‘_{i=1}^N -\frac{(t_i - y(â†’x_i; â†’w))^2}{2Ïƒ^2} \\
                         =& \argmin_â†’w âˆ‘_{i=1}^N \frac{(t_i - y(â†’x_i; â†’w))^2}{2Ïƒ^2} = \argmin_â†’w \tfrac{1}{N} âˆ‘_{i=1}^N (t_i - y(â†’x_i; â†’w))^2.
\end{aligned}$$

---
class: tablewide
# Generalized Linear Models

We have therefore extended the GLM table to

| Name | Activation | Distribution | Loss | Gradient |
|------|------------|--------------|------|----------|
| linear regression | identity | $\color{red}\textrm{Normal}$ | $\color{red}\textrm{NLL} âˆ \textrm{MSE}$ | $\big(t - y(â†’x)\big) â‹… â†’x$ |
| logistic regression | $Ïƒ(x)$ | Bernoulli | $\textrm{NLL} âˆ âˆ‘_i -\log(p(t \vert â†’x))$ | $\big(t - a(y(â†’x))\big) â‹… â†’x$ |

---
section: MulticlassLogisticReg
# Multiclass Logistic Regression

To extend the binary logistic regression to a multiclass case with $K$ classes, we:
- generate multiple outputs, notably $K$ outputs, each with its own set of
  weights, so that
  $$â†’y(â†’x; â‡‰W) = â‡‰W â†’x,\textrm{~~~or in other words~~~}â†’y(â†’x; â‡‰W)_i = â†’W_i^T â†’x,$$

~~~
- generalize the sigmoid function to a $\softmax$ function, such that
  $$\softmax(â†’y)_i = \frac{e^{y_i}}{âˆ‘_j e^{y_j}}.$$

~~~
  Note that the original sigmoid function can be written as
  $$Ïƒ(x) = \softmax\big([x~~0]\big)_0 = \frac{e^x}{e^x + e^0} = \frac{1}{1 + e^{-x}}.$$

~~~
The resulting classifier is also known as **multinomial logistic regression**,
**maximum entropy classifier** or **softmax regression**.

---
# Multiclass Logistic Regression

From the definition of the $\softmax$ function
$$\softmax(â†’y)_i = \frac{e^{y_i}}{âˆ‘_j e^{y_j}},$$
it is natural to obtain the interpretation of the model outputs
$â†’y(â†’x; â‡‰W)$ as **logits**:
$$â†’y(â†’x; â‡‰W)_i = \log(p(C_i | â†’x; â‡‰W)) + c.$$

~~~
The constant $c$ is present, because the output of the model is
_overparametrized_ (the probability of for example the last class could be
computed from the remaining ones). This is connected to the fact that softmax
is invariant to addition of a constant:
$$\softmax(â†’y + c)_i = \frac{e^{y_i + c}}{âˆ‘_j e^{y_j + c}} = \frac{e^{y_i}}{âˆ‘_j e^{y_j}}â‹…\frac{e^c}{e^c} = \softmax(â†’y)_i.$$

---
# Multiclass Logistic Regression

The difference between softmax and sigmoid output can be compared on the binary
case, where the binary logistic regression model outputs are
$$y(â†’x; â†’w) = \log\left(\frac{p(C_1 | â†’x; â†’w)}{p(C_0 | â†’x; â†’w)}\right),$$
while the outputs of the softmax variant with two outputs can be interpreted as<br>
$â†’y(â†’x; â‡‰W)_0 = \log(p(C_0 | â†’x; â‡‰W)) + c$ and $â†’y(â†’x; â‡‰W)_1 = \log(p(C_1 | â†’x; â‡‰W)) + c$.

~~~
If we consider $â†’y(â†’x; â‡‰W)_0$ to be zero, the model can then predict only the
probability $p(C_1 | â†’x)$, and the constant $c$ is fixed to $-\log(p(C_0 | â†’x; â‡‰W))$, recovering
the original interpretation.

~~~
Therefore, we could produce only $K-1$ outputs for $K$-class classification and
define $y_K=0$, resulting in the interpretation of the model outputs analogous to the
binary case:

$$â†’y(â†’x; â‡‰W)_i = \log\left(\frac{p(C_i | â†’x; â‡‰W)}{p(C_K | â†’x; â‡‰W)}\right).$$

---
# Multiclass Logistic Regression

Using the $\softmax$ function, we naturally define that
$$p(C_i | â†’x; â‡‰W) = \softmax(â‡‰W_i^T â†’x)_i = \frac{e^{â‡‰W_i^T â†’x}}{âˆ‘_j e^{â‡‰W_j^T â†’x}}.$$

~~~
We can then use MLE and train the model using stochastic gradient descent.

<div class="algorithm">

**Input**: Input dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{0, 1, â€¦, K-1\}^N$), learning rate $Î± âˆˆ â„^+$.<br>

- $â†’w â† 0$
- until convergence (or until patience is over), process batch of $N$ examples:
  - $g â† - \tfrac{1}{N} âˆ‘_i âˆ‡_â†’w \log(p(C_{t_i} | â†’x_i; â†’w)$
  - $â†’w â† â†’w - Î±â†’g$
</div>

---
# Multiclass Logistic Regression

![w=41%,f=right](classification_convex_regions.svgz)

Note that the decision regions of the binary/multiclass logistic regression are
convex (and therefore connected).

~~~
To see this, consider $â†’x_A$ and $â†’x_B$ in the same decision region $R_k$.

~~~
Any point $â†’x$ lying on the line connecting them is their linear combination,
$â†’x = Î»â†’x_A + (1-Î»)â†’x_B$,
~~~
and from the linearity of $â†’y(â†’x) = â‡‰Wâ†’x$ it follows that
$$â†’y(â†’x) = Î»â†’y(â†’x_A) + (1-Î»)â†’y(â†’x_B).$$

~~~
Given that $f_k(â†’x_A)$ was the largest among $â†’y(â†’x_A)$ and also
given that $f_k(â†’x_B)$ was the largest among $â†’y(â†’x_B)$, it must
be the case that $f_k(â†’x)$ is the largest among all $â†’y(â†’x)$.

---
class: tablewide
# Generalized Linear Models

The multiclass logistic regression can now be added to the GLM table:

| Name | Activation | Distribution | Loss | Gradient |
|------|------------|--------------|------|----------|
| linear regression | identity | Normal | $\textrm{NLL} âˆ \textrm{MSE}$ | $\big(t - y(â†’x)\big) â‹… â†’x$ |
| logistic regression | $Ïƒ(x)$ | Bernoulli | $\textrm{NLL} âˆ âˆ‘ -\log(p(t \vert â†’x))$ | $\big(t - a(y(â†’x))\big) â‹… â†’x$ |
| multiclass<br>logistic regression | $\small\operatorname{softmax}(x)$ | categorical | $\textrm{NLL} âˆ âˆ‘ -\log(p(t \vert â†’x))$ | $\big(t - a(y(â†’x))\big) â‹… â†’x$ |

---
section: PoissonReg
class: tablewide
# Poisson Regression

| Name | Activation | Distribution | Loss | Gradient |
|------|------------|--------------|------|----------|
| linear regression | identity | Normal | $\textrm{NLL} âˆ \textrm{MSE}$ | $\big(t - y(â†’x)\big) â‹… â†’x$ |
| logistic regression | $Ïƒ(x)$ | Bernoulli | $\textrm{NLL} âˆ âˆ‘ -\log(p(t \vert â†’x))$ | $\big(t - a(y(â†’x))\big) â‹… â†’x$ |
| multiclass<br>logistic regression | $\small\operatorname{softmax}(x)$ | categorical | $\textrm{NLL} âˆ âˆ‘ -\log(p(t \vert â†’x))$ | $\big(t - a(y(â†’x))\big) â‹… â†’x$ |
| Poisson regression | $\small\exp(x)$ | Poisson | $\textrm{NLL} âˆ âˆ‘ -\log(p(t \vert â†’x))$ | $\big(t - a(y(â†’x))\big) â‹… â†’x$ |


---
section: MLP
# Multilayer Perceptron

![w=45%,h=center](mlp.svgz)

---
# Multilayer Perceptron

There is a weight on each edge, and an activation function $f$ is performed on the
hidden layers, and optionally also on the output layer.
$$h_i = f\left(âˆ‘_j w_{i,j} x_j + b_i\right)$$

If the network is composed of layers, we can use matrix notation and write:
$$â†’h = f\left(â‡‰W â†’x + â†’b\right)$$

---
# Multilayer Perceptron and Biases

![w=55%,h=center](mlp_bias.svgz)

---
# Neural Network Activation Functions
## Output Layers
- none (linear regression if there are no hidden layers)

~~~
- sigmoid (logistic regression model if there are no hidden layers)
  $$Ïƒ(x) â‰ \frac{1}{1 + e^{-x}}$$

~~~
- $\softmax$ (maximum entropy model if there are no hidden layers)
  $$\softmax(â†’x) âˆ e^â†’x$$
  $$\softmax(â†’x)_i â‰ \frac{e^{x_i}}{âˆ‘_j e^{x_j}}$$

---
# Neural Network Activation Functions
## Hidden Layers
- none (does not help, composition of linear mapping is a linear mapping)

~~~
- $Ïƒ$ (but works badly â€“ nonsymmetrical, $\frac{dÏƒ}{dx}(0) = 1/4$)

~~~
- $\tanh$
    - result of making $Ïƒ$ symmetrical and making derivation in zero 1
    - $\tanh(x) = 2Ïƒ(2x) - 1$

~~~
- ReLU
    - $\max(0, x)$

---
# Training MLP

The multilayer perceptron can be trained using an SGD algorithm:

<div class="algorithm">

**Input**: Input dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t$ targets), learning rate $Î± âˆˆ â„^+$.<br>

- $â†’w â† 0$
- until convergence (or until patience is over), process batch of $N$ examples:
  - $g â† âˆ‡_â†’w \frac{1}{N} âˆ‘_j - \log p(t_j | â†’x_j; â†’w)$
  - $â†’w â† â†’w - Î±â†’g$
</div>

---
# Training MLP â€“ Computing the Derivatives

![w=20%,f=right](mlp.svgz)

Assume a network with an input of size $N_1$, then weights
$â†’U âˆˆ â„^{N_1 Ã— N_2}$, hidden layer with size $N_2$ and activation $h$,
weights $â†’V âˆˆ â„^{N_2 Ã— N_3}$, and finally an output layer of size $N_3$
with activation $a$.

~~~
In order to compute the gradient of the loss $L$ with respect to all weights, you
should proceed gradually:
- first compute $\frac{âˆ‚L}{âˆ‚â†’o}$,

~~~
- then compute $\frac{âˆ‚â†’o}{âˆ‚â†’o_\mathit{in}}$, where $o_\mathit{in}$ are the
  inputs to the output layer (i.e., before applying activation function $f$),
~~~
- then compute $\frac{âˆ‚â†’o_\mathit{in}}{âˆ‚â‡‰V}$ and $\frac{âˆ‚L}{âˆ‚â‡‰V}$,
~~~
- followed by $\frac{âˆ‚â†’o_\mathit{in}}{âˆ‚â†’h}$ and $\frac{âˆ‚â†’h}{âˆ‚â†’h_\mathit{in}}$,
~~~
- and finally using $\frac{âˆ‚â†’h_\mathit{in}}{âˆ‚â‡‰U}$ to compute $\frac{âˆ‚L}{âˆ‚â‡‰U}$.

---
section: UniversalApproximation
# Universal Approximation Theorem '89

Let $Ï†(x)$ be a nonconstant, bounded and nondecreasing continuous function.
<br>(Later a proof was given also for $Ï† = \ReLU$.)

Then for any $Îµ > 0$ and any continuous function $f$ on $[0, 1]^m$ there exists
an $N âˆˆ â„•, v_i âˆˆ â„, b_i âˆˆ â„$ and $â†’{w_i} âˆˆ â„^m$, such that if we denote
$$F(â†’x) = âˆ‘_{i=1}^N v_i Ï†(â†’{w_i} \cdot â†’x + b_i),$$
then for all $x âˆˆ [0, 1]^m$:
$$|F(â†’x) - f(â†’x)| < Îµ.$$

---
class: dbend
# Universal Approximation Theorem for ReLUs

Sketch of the proof:

~~~
- If a function is continuous on a closed interval, it can be approximated by
  a sequence of lines to arbitrary precision.

![w=50%,h=center](universal_approximation_example.png)

~~~
- However, we can create a sequence of $k$ linear segments as a sum of $k$ ReLU
  units â€“ on every endpoint a new ReLU starts (i.e., the input ReLU value is
  zero at the endpoint), with a tangent which is the difference between the
  target tanget and the tangent of the approximation until this point.

---
class: dbend
# Universal Approximation Theorem for Squashes

Sketch of the proof for a squashing function $Ï†(x)$ (i.e., nonconstant, bounded and
nondecreasing continuous function like sigmoid):

~~~
- We can prove $Ï†$ can be arbitrarily close to a hard threshold by compressing
  it horizontally.

![w=38%,h=center](universal_approximation_squash.png)

~~~
- Then we approximate the original function using a series of straight line
  segments

![w=38%,h=center](universal_approximation_rectangles.png)
