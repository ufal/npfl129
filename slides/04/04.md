title: NPFL129, Lecture 4
class: title, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Multiclass Logistic Regression, Multilayer Perceptron

## JindÅ™ich LibovickÃ½ <small>(reusing materials by Milan Straka)</small>

### October 21, 2024

---
class: middle
# Today's Lecture Objectives


After this lecture you should be able to

- Implement **multiclass classification** with softmax.

- Reason about linear regression, logistic regression and softmax
  classification in a **single probabilistic framework**: with different target
  distributions, activation functions and training using maximum likelihood
  estimate.

- Explain **multi-layer perceptron** as a further generalization of linear models.


---
section: Refresh
class: section
# Refresh from the Last Week

---
# Logistic Regression

An extension of perceptron, which models the conditional probabilities
of $p(C_0|â†’x)$ and of $p(C_1|â†’x)$. <small>(It can, in fact,
handle more than two classes as well, which we will see shortly.)</small>

~~~
Logistic regression employs the following parametrization of the conditional
class probabilities:
$$\begin{aligned}
  p(C_1 | â†’x) &= Ïƒ(â†’x^T â†’w + b) \\
  p(C_0 | â†’x) &= 1 - p(C_1 | â†’x),
\end{aligned}$$

![w=60%,f=right](../03/sigmoid.svgz)

where $Ïƒ$ is the **sigmoid function**
$$Ïƒ(x) = \frac{1}{1+e^{-x}}.$$

~~~
It can be trained using the SGD algorithm.

---
# Logistic Regression

We denote the output of the â€œlinear partâ€ of logistic regression as
$yÌ„(â†’x; â†’w) = â†’x^T â†’w$
and the overall prediction as
$y(â†’x; â†’w) = Ïƒ(yÌ„(â†’x; â†’w)) = Ïƒ(â†’x^T â†’w).$

~~~
The logistic regression output $y(â†’x; â†’w)$ models the probability of class
$C_1$, $p(C_1 | â†’x)$.

To give some meaning to the output of the linear part $yÌ„(â†’x; â†’w)$, starting with
$$p(C_1 | â†’x) = Ïƒ(yÌ„(â†’x; â†’w)) = \frac{1}{1 + e^{-yÌ„(â†’x; â†’w)}},$$
~~~
we arrive at
$$yÌ„(â†’x; â†’w) = \log\left(\frac{p(C_1 | â†’x)}{1 - p(C_1 | â†’x)}\right) = \log\left(\frac{p(C_1 | â†’x)}{p(C_0 | â†’x)}\right),$$
which is called a **logit**, and it is the logarithm of the odds of the probabilities
of the two classes.

---
# Logistic Regression

To train logistic regression, we use MLE (the maximum likelihood
estimation). Its application is straightforward, given that $p(C_1 | â†’x; â†’w)$ is
directly the model output $y(â†’x; â†’w)$.

~~~
Therefore, the loss for a minibatch $ğ•=\{(â†’x_1, t_1), (â†’x_2, t_2), â€¦, (â†’x_N, t_N)\}$
is
$$\begin{aligned}
E(â†’w) = \frac{1}{N} âˆ‘_i -\log(p(C_{t_i} | â†’x_i; â†’w)). \\
\end{aligned}$$

<div class="algorithm">

**Input**: Input dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{0, +1\}^N$), learning rate $Î± âˆˆ â„^+$.<br>

- $â†’w â† â†’0$ or we initialize $â†’w$ randomly
- until convergence (or patience runs out), process a minibatch of examples $ğ”¹$:
  - $â†’g â† \tfrac{1}{|ğ”¹|} âˆ‘_{iâˆˆğ”¹} âˆ‡_{â†’w} \Big(-\log\big(p(C_{t_i} | â†’x_i; â†’w)\big)\Big)$
  - $â†’w â† â†’w - Î±â†’g$
</div>

---
section: GLM
class: section
# Generalized Linear Models

---
class: tablewide
# Generalized Linear Models

Logistic regression is in fact extended linear regression. A linear
regression model, which is followed by an **activation function** $a$, is
called a **generalized linear model**:
$$p(t | â†’x; â†’w, b) = y(â†’x; â†’w, b) = a\big(yÌ„(â†’x; â†’w, b)\big) = a(â†’x^Tâ†’w + b).$$

~~~
| Name | Activation | Distribution | Loss | Gradient |
|------|------------|--------------|------|----------|
| linear regression | identity | ? | $\textrm{MSE} âˆ ğ”¼ (y(â†’x) - t)^2$ | $\big(y(â†’x) - t\big) â†’x$ |
~~~
| logistic regression | $Ïƒ(yÌ„)$ | Bernoulli | $\textrm{NLL} âˆ ğ”¼ -\log(p(t \vert â†’x))$ | ? |

---
# Logistic Regression Gradient

We start by computing the gradient of $Ïƒ(x)$.

$\displaystyle \kern4em\mathllap{\frac{âˆ‚}{âˆ‚x} Ïƒ(x)} = \frac{âˆ‚}{âˆ‚x} \frac{1}{1 + e^{-x}}$

~~~
$\displaystyle \kern4em{}=\mathrlap{\frac{\textcolor{darkgreen}{\frac{âˆ‚}{âˆ‚x}-(1 + e^{-x})}}{(1 + e^{-x})^2}}
 \kern30em\mathllap{\color{gray}\frac{âˆ‚}{âˆ‚x} \frac{1}{g(x)} = -\frac{\frac{âˆ‚}{âˆ‚x} g(x)}{g(x)^2}}$

~~~
$\displaystyle \kern4em{}=\mathrlap{\textcolor{blue}{\frac{1}{1+e^{-x}}} â‹… \frac{\textcolor{darkgreen}{e^{-x}}}{1 + e^{-x}}}
 \kern30em\mathllap{\color{gray}\tfrac{âˆ‚}{âˆ‚x} e^{g(x)} = e^{g(x)} â‹… \tfrac{âˆ‚}{âˆ‚x} g(x)}$

~~~
$\displaystyle \kern4em{}=\mathrlap{\textcolor{blue}{Ïƒ(x)} â‹… \frac{\textcolor{darkred}{e^{-x}+1}-\textcolor{darkorange}{1}}{1 + e^{-x}}}$

~~~
$\displaystyle \kern4em{}=\mathrlap{Ïƒ(x) â‹… \big(\textcolor{darkred}{1}-\textcolor{darkorange}{Ïƒ(x)}\big)}$

---
# Logistic Regression Gradient

Consider the log-likelihood of logistic regression $\log p(t|â†’x; â†’w)$. For
brevity, we denote $yÌ„(â†’x; â†’w) = â†’x^T â†’w$ just as $yÌ„$ in the following
computation.

~~~
Remembering that for $tâˆ¼\operatorname{Ber}(Ï†)$ we have $p(t) = Ï†^t (1-Ï†)^{1-t}$,
we can rewrite the log-likelihood to:

~~~
$\displaystyle \kern6em\mathllap{\log p(t|â†’x; â†’w)} = \log Ïƒ(yÌ„)^t \big(1-Ïƒ(yÌ„)\big)^{1-t}$

~~~
$\displaystyle \kern6em{}=t â‹… \log \big(Ïƒ(yÌ„)\big) + (1-t) â‹… \log\big(1-Ïƒ(yÌ„)\big).$

---
# Logistic Regression Gradient

$\displaystyle âˆ‡_{â†’w} -\log p(t|â†’x; â†’w) =$

$\displaystyle \kern1em{}= âˆ‡_{â†’w}\Big(-t â‹… \log \big(Ïƒ(yÌ„)\big) - (1-t) â‹… \log\big(1-Ïƒ(yÌ„)\big)\Big)$

~~~
$\displaystyle \kern34em\mathllap{\color{gray}\frac{âˆ‚}{âˆ‚x}\log g(x) = \frac{1}{g(x)} â‹… \frac{âˆ‚}{âˆ‚x} g(x)}$

~~~
$\displaystyle \kern1em{}=-tâ‹…\frac{1}{Ïƒ(yÌ„)}â‹…âˆ‡_{â†’w}Ïƒ(yÌ„) - (1-t)â‹…\frac{1}{1-Ïƒ(yÌ„)}â‹…âˆ‡_{â†’w}\big(1\textcolor{magenta}{-}Ïƒ(yÌ„)\big)$

~~~
$\displaystyle \kern34em\mathllap{\color{gray}\frac{âˆ‚}{âˆ‚x}f\big(g(x)\big)
  = \frac{âˆ‚}{âˆ‚g(x)}f\big(g(x)\big) â‹… \frac{âˆ‚}{âˆ‚x} g(x) = \frac{âˆ‚}{âˆ‚z}f(z) â‹… \frac{âˆ‚}{âˆ‚x} g(x)}$

~~~
$\displaystyle \kern34em\mathllap{\color{gray}âˆ‡_{â†’w}Ïƒ(yÌ„) = \tfrac{âˆ‚}{âˆ‚yÌ„}Ïƒ(yÌ„) â‹… âˆ‡_{â†’w}yÌ„}$

~~~
$\displaystyle \kern1em{}=\mathrlap{-tâ‹…\frac{1}{\textcolor{darkgreen}{Ïƒ(yÌ„)}}â‹…\textcolor{darkgreen}{Ïƒ(yÌ„)}â‹…\big(1-Ïƒ(yÌ„)\big)â‹…\textcolor{orange}{âˆ‡_{â†’w}yÌ„}
 \textcolor{magenta}{+} (1-t)â‹…\frac{1}{\textcolor{blue}{1-Ïƒ(yÌ„)}}â‹…Ïƒ(yÌ„)â‹…\textcolor{blue}{\big(1-Ïƒ(yÌ„)\big)}â‹…\textcolor{orange}{âˆ‡_{â†’w}yÌ„}}$

~~~
$\displaystyle \kern1em{}=\big(-t+\textcolor{darkred}{tÏƒ(yÌ„)}+Ïƒ(yÌ„)-\textcolor{darkred}{tÏƒ(yÌ„)}\big)\textcolor{orange}{â†’x}$

~~~
$\displaystyle \kern1em{}=\big(y(â†’x; â†’w) - t\big)â†’x$

---
class: tablewide
# Generalized Linear Models

Logistic regression is in fact extended linear regression. A linear
regression model, which is followed by some **activation function** $a$, is
called a **generalized linear model**:
$$p(t | â†’x; â†’w, b) = y(â†’x; â†’w, b) = a\big(yÌ„(â†’x; â†’w, b)\big) = a(â†’x^Tâ†’w + b).$$

| Name | Activation | Distribution | Loss | Gradient |
|------|------------|--------------|------|----------|
| linear regression | identity | ? | $\textrm{MSE} âˆ ğ”¼ (y(â†’x) - t)^2$ | $\big(y(â†’x) - t\big) â†’x$ |
| logistic regression | $Ïƒ(yÌ„)$ | Bernoulli | $\textrm{NLL} âˆ ğ”¼ -\log(p(t \vert â†’x))$ | $\color{red}\big(y(â†’x) - t\big) â†’x$ |

---
section: MSE as MLE
class: section
# Mean Square Error as Maximum Likelihood Estimation

---
# Mean Square Error as MLE

![w=50%,h=center](constant_variance.svgz)
~~~

During regression, we predict a number, not a probability distribution.
To generate a distribution, we might consider a distribution with
the mean of the predicted value and a fixed variance $Ïƒ^2$ â€“ the most general
such distribution is the normal distribution.

---
# Mean Square Error as MLE

Therefore, assume our model generates a distribution
$p(t | â†’x; â†’w) = ğ“(t; y(â†’x; â†’w), Ïƒ^2)$.

![w=18%,f=right;position:absolute;right:.5em](normal_paranormal.png)
~~~

Now we can apply the maximum likelihood estimation and get

$\displaystyle \kern8em\mathllap{\argmax_{â†’w} p(â†’t|â‡‰X; â†’w)} = \argmin_{â†’w} âˆ‘_{i=1}^N -\log p(t_i | â†’x_i ; â†’w)$

~~~
$\displaystyle \kern8em{} = \argmin_{â†’w} -âˆ‘_{i=1}^N \log \sqrt{\frac{1}{2Ï€Ïƒ^2}} e ^ {\normalsize -\frac{(t_i - y(â†’x_i; â†’w))^2}{2Ïƒ^2}}$

~~~
$\displaystyle \kern8em{} = \argmin_{â†’w} {\color{gray} -N \log (2Ï€Ïƒ^2)^{-1/2}} - âˆ‘_{i=1}^N -\frac{\big(t_i - y(â†’x_i; â†’w)\big)^2}{2Ïƒ^2}$

~~~
$\displaystyle \kern8em{} = \argmin_{â†’w} âˆ‘_{i=1}^N \frac{\big(t_i - y(â†’x_i; â†’w)\big)^2}{2Ïƒ^2} = \argmin_{â†’w} \frac{1}{N} âˆ‘_{i=1}^N \big(y(â†’x_i; â†’w) - t_i\big)^2.$

---
class: tablewide
# Generalized Linear Models

We have therefore extended the GLM table to

| Name | Activation | Distribution | Loss | Gradient |
|------|------------|--------------|------|----------|
| linear regression | identity | $\color{red}\textrm{Normal}$ | $\color{red}\textrm{NLL} âˆ \textrm{MSE}$ | $\big(y(â†’x) - t\big) â†’x$ |
| logistic regression | $Ïƒ(yÌ„)$ | Bernoulli | $\textrm{NLL} âˆ ğ”¼ -\log(p(t \vert â†’x))$ | $\big(y(â†’x) - t\big) â†’x$ |

---
section: MulticlassLogisticReg
class: section
# Multiclass Logistic Regression
---

# Multiclass Logistic Regression

To extend the binary logistic regression to a multiclass case with $K$ classes, we:
- generate $K$ outputs, each with its own set of weights, so that for $â‡‰W âˆˆ â„^{DÃ—K}$,
  $$â†’yÌ„(â†’x; â‡‰W) = â†’x^T â‡‰W,\textrm{~~~or in other words,~~~}â†’yÌ„(â†’x; â‡‰W)_i = â†’x^T (â‡‰W_{*, i})$$

~~~
- generalize the sigmoid function to a $\softmax$ function, such that
  $$\softmax(â†’z)_i = \frac{e^{z_i}}{âˆ‘_j e^{z_j}}.$$

~~~
  Note that the original sigmoid function can be written as
  $$Ïƒ(x) = \softmax\big([x, 0]\big)_0 = \frac{e^x}{e^x + e^0} = \frac{1}{1 + e^{-x}}.$$

~~~
The resulting classifier is also known as **multinomial logistic regression**,
**maximum entropy classifier** or **softmax regression**.

---
# Multiclass Logistic Regression

Using the $\softmax$ function, we naturally define that
$$p(C_i | â†’x; â‡‰W) = â†’y(â†’x; â‡‰W)_i = \softmax\big(â†’yÌ„(â†’x; â‡‰W)\big)_i = \softmax(â†’x^T â‡‰W)_i = \frac{e^{(â†’x^Tâ‡‰W)_i}}{âˆ‘_j e^{(â†’x^T â‡‰W)_j}}.$$

~~~

Considering the definition of the $\softmax$ function,
it is natural to obtain the interpretation of the linear part of the model,
$â†’yÌ„(â†’x; â‡‰W)$, as **logits** by computing the logarithm of the above:
$$â†’yÌ„(â†’x; â‡‰W)_i = \log(p(C_i | â†’x; â‡‰W)) + c.$$

~~~
The constant $c$ is present, because the output of the model is
_overparametrized_ (for example, the probability of the last class could be
computed from the remaining ones). This is connected to the fact that softmax
is
 invariant to addition of a constant:
$$\softmax(â†’z + c)_i = \frac{e^{z_i + c}}{âˆ‘_j e^{z_j + c}} = \frac{e^{z_i}}{âˆ‘_j e^{z_j}}â‹…\frac{e^c}{e^c} = \softmax(â†’z)_i.$$

---
# Multiclass Logistic Regression

To train $K$-class classification, analogously to binary logistic regression,
we can use MLE and train the model using minibatch stochastic gradient descent:

<div class="algorithm">

**Input**: Input dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{0, 1, â€¦, K-1\}^N$), learning rate $Î± âˆˆ â„^+$.<br>
**Model**: Let $â†’w$ denote all parameters of the model (in our case, the parameters are a weight
matrix $â‡‰W$ and maybe a bias vector $â†’b$).

- $â†’w â† â†’0$ or we initialize $â†’w$ randomly
- until convergence (or patience runs out), process a minibatch of examples $ğ”¹$:
  - $â†’g â† \tfrac{1}{|ğ”¹|} âˆ‘_{iâˆˆğ”¹} âˆ‡_{â†’w} \Big(-\log\big(p(C_{t_i} | â†’x_i; â†’w)\big)\Big)$
  - $â†’w â† â†’w - Î±â†’g$
</div>

---
# Multiclass Logistic Regression

![w=41%,f=right](classification_convex_regions.svgz)

Note that the decision regions of binary/multiclass logistic regression are
convex (and therefore connected).

~~~
To see this, consider $â†’x_A$ and $â†’x_B$ in the same decision region $R_k$.

~~~
Any point $â†’x$ lying on the line connecting them is their convex combination,
$â†’x = Î»â†’x_A + (1-Î»)â†’x_B$,
~~~
and from the linearity of $â†’yÌ„(â†’x) = â†’x^T â‡‰W$ it follows that
$$â†’yÌ„(â†’x) = Î»â†’yÌ„(â†’x_A) + (1-Î»)â†’yÌ„(â†’x_B).$$

~~~
Given that $â†’yÌ„(â†’x_A)_k$ was the largest among $â†’yÌ„(â†’x_A)$ and also
given that $â†’yÌ„(â†’x_B)_k$ was the largest among $â†’yÌ„(â†’x_B)$, it must
be the case that $â†’yÌ„(â†’x)_k$ is the largest among all $â†’yÌ„(â†’x)$.

---
# What went wrong?

![w=80%,h=center](majority.svgz)
~~~
<center>The model only predicts the majority class.<br />
~~~
Insufficient features, learning rate is too high.
</center>


---
class: tablewide
# Generalized Linear Models

Multiclass logistic regression can now be added to the GLM table:

| Name | Activation | Distribution | Loss | Gradient |
|------|------------|--------------|------|----------|
| linear regression | identity | Normal | $\textrm{NLL} âˆ \textrm{MSE}$ | $\big(y(â†’x) - t\big) â†’x$ |
| logistic regression | $Ïƒ(yÌ„)$ | Bernoulli | $\textrm{NLL} âˆ ğ”¼ -\log(p(t \vert â†’x))$ | $\big(y(â†’x) - t\big) â†’x$ |
| multiclass<br>logistic regression | $\small\operatorname{softmax}(â†’yÌ„)$ | categorical | $\textrm{NLL} âˆ ğ”¼ -\log(p(t \vert â†’x))$ | $\big((â†’y(â†’x) - â†’1_t) â†’x^T\big)^T$ |

\
Recall that $â†’1_t = \big([i = t]\big)_{i=0}^{K-1}$ is one-hot representation of target $t âˆˆ \{0, 1, â€¦, K-1\}$.

The gradient $\big((â†’y(â†’x) - â†’1_t) â†’x^T\big)^T$ can be of course also computed
as $â†’x\big(â†’y(â†’x) - â†’1_t\big)^T$.

---
section: MLP
class: section
# Multilayer Perceptron

---
# Multilayer Perceptron

We can reformulate generalized linear models in the following framework.

![w=90%,h=center,mw=35%,f=right](mlp_no_hidden.svgz)

~~~
- Assume we have an input node for every input feature.

~~~
- Additionally, we have an output node for every model output (one for linear
  regression or binary classification, $K$ for classification in $K$ classes).

~~~
- Every input node and output node is connected with a directed edge, and
  every edge has an associated weight.

~~~
- Value of every (output) node is computed by summing the values of its predecessors
  multiplied by the corresponding weights, added to the bias of this node, and
  finally passed through an activation function $a$:
  $$y_i = a\left(âˆ‘\nolimits_j x_j w_{j,i} + b_i\right)$$
  or in matrix form $â†’y = a(â†’x^Tâ‡‰W + â†’b)$, or for a batch of examples $â‡‰X$,
  $â‡‰Y = a(â‡‰X â‡‰W + â†’b)$.

---
# Multilayer Perceptron

We now extend the model by adding a **hidden layer** with activation $f$.

![w=45%,f=right](mlp.svgz)

~~~
- The computation is performed analogously:
  $$\begin{aligned}
    h_i &= f\left(âˆ‘\nolimits_j x_j w^{(h)}_{j,i} + b^{(h)}_i\right), \\
    y_i &= a\left(âˆ‘\nolimits_j h_j w^{(y)}_{j,i} + b^{(y)}_i\right),
  \end{aligned}$$
  or in matrix form
  $$\begin{aligned}
    â†’h &= f\Big(â†’x^T â‡‰W^{(h)} + â†’b^{(h)}\Big), \\
    â†’y &= a\Big(â†’h^T â‡‰W^{(y)} + â†’b^{(y)}\Big),
  \end{aligned}$$
  and for a batch of inputs $â‡‰H = f\Big(â‡‰X â‡‰W^{(h)} + â†’b^{(h)}\Big)$ and $â‡‰Y = a\Big(â‡‰H â‡‰W^{(y)} + â†’b^{(y)}\Big)$.

---
# Multilayer Perceptron

Note that:
- the structure of the _input_ layer depends on the input features;

~~~
- the structure and the _activation_ function of the _output_ layer depend
  on the target data;
~~~
- however, the _hidden_ layer has no pre-image in the data and is completely arbitrary
  â€“ which is the reason why it is called a _hidden_ layer.

~~~
![w=76%,h=center,mw=50%,f=right](mlp_bias.svgz)

Also note that we can absorb biases into weights analogously to generalized
linear models.

---
# Output Layer Activation Functions
## Output Layer Activation Functions
- regression:

  - identity activation: we model normal distribution on output (linear
    regression)

~~~
- binary classification:
  - $Ïƒ(x)$: we model the Bernoulli distribution (the model predicts
    a probability)
    $$Ïƒ(x) â‰ \frac{1}{1 + e^{-x}}$$

~~~
- $K$-class classification:
  - $\softmax(â†’x)$: we model the (usually overparametrized) categorical distribution
    $$\softmax(â†’x) âˆ e^{â†’x},~~~\softmax(â†’x)_i â‰ \frac{e^{â†’x_i}}{âˆ‘_j e^{â†’x_j}}$$

---
# Hidden Layer Activation Functions
## Hidden Layer Activation Functions
- no activation (identity): does not help, composition of linear mappings is a linear mapping

~~~
- $Ïƒ$ (but works suboptimally â€“ nonsymmetrical, $\frac{dÏƒ}{dx}(0) = 1/4$)

~~~
- ![w=95%,h=right,mw=61%,f=right](sigmoid_to_tanh.svgz)

  $\tanh$

  - result of making $Ïƒ$ symmetrical and making derivation in zero 1
  - $\tanh(x) = 2Ïƒ(2x) - 1$

~~~
- ReLU
  - $\max(0, x)$
  - the most common nonlinear activation used nowadays

---
# Training MLP

The multilayer perceptron can again be trained using a minibatch SGD algorithm:

<div class="algorithm">

**Input**: Input dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t$ targets), learning rate $Î± âˆˆ â„^+$.<br>
**Model**: Let $â†’w$ denote all parameters of the model (all weight matrices and bias vectors).

- initialize $â†’w$
  - set weights randomly
    - for a weight matrix processing a layer of size $M$ to a layer of size $O$,
      we can sample its elements uniformly for example from the
      $\left[-\frac{1}{\sqrt M}, \frac{1}{\sqrt M}\right]$ range
    - the exact range becomes more important for networks with many hidden
      layers
  - set biases to 0
- until convergence (or patience runs out), process a minibatch of examples $ğ”¹$:
  - $â†’g â† \tfrac{1}{|ğ”¹|} âˆ‘_{iâˆˆğ”¹} âˆ‡_{â†’w} \Big(-\log\big(p(t_i | â†’x_i; â†’w)\big)\Big)$
  - $â†’w â† â†’w - Î±â†’g$
</div>

---
class: summary
# Today's Lecture Objectives

After this lecture you should be able to

- Implement **multiclass classification** with softmax.

- Reason about linear regression, logistic regression and softmax
  classification in a **single probabilistic framework**: with different target
  distributions, activation functions and training using maximum likelihood
  estimate.

- Explain **multi-layer perceptron** as a further generalization of linear models.
