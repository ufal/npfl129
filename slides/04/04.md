title: NPFL129, Lecture 4
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Multiclass Logistic Regression, Multiplayer Perceptron

## Milan Straka

### November 11, 2019

---
section: LR
# Logistic Regression

An extension of perceptron, which models the conditional probabilities
of $p(C_0|â†’x)$ and of $p(C_1|â†’x)$. Logistic regression can in fact
handle also more than two classes, which we will see shortly.

~~~
Logistic regression employs the following parametrization of the conditional
class probabilities:
$$\begin{aligned}
  P(C_1 | â†’x) &= Ïƒ(â†’x^t â†’w + â†’b) \\
  P(C_0 | â†’x) &= 1 - P(C_1 | â†’x),
\end{aligned}$$
where $Ïƒ$ is a _sigmoid function_
$$Ïƒ(x) = \frac{1}{1+e^{-x}}.$$

~~~
Can be trained using an SGD algorithm.

---
# Sigmoid Function

The sigmoid function has values in range $(0, 1)$, is monotonically
increasing and it has a derivative of $\frac{1}{4}$ at $x=0$.

$$Ïƒ(x) = \frac{1}{1+e^{-x}}$$

~~~
$$Ïƒ'(x) = Ïƒ(x) \big(1 - Ïƒ(x)\big)$$

~~~
![w=100%](../03/sigmoid.pdf)

---
# Logistic Regression

To give some meaning to the sigmoid function, starting with
$$P(C_1 | â†’x) = Ïƒ(f(â†’x; â†’w)) = \frac{1}{1 + e^{-f(â†’x; â†’w)}}$$
~~~
we can arrive at
$$f(â†’x; â†’w) = \log\left(\frac{P(C_1 | â†’x)}{P(C_0 | â†’x)}\right),$$
where the prediction of the model $f(â†’x; â†’w)$ is called a _logit_
and it is a logarithm of odds of the two classes probabilities.

---
# Logistic Regression

To train the logistic regression $y(â†’x; â†’w) = â†’x^T â†’w$, we use MLE (the maximum likelihood
estimation). Note that $P(C_1 | â†’x; â†’w) = Ïƒ(y(â†’x; â†’w))$.

~~~
Therefore, the loss for a batch $ğ•=\{(â†’x_1, t_1), (â†’x_2, t_2), â€¦, (â†’x_N, t_N)\}$
is
$$\begin{aligned}
ğ“›(ğ•) = \frac{1}{N} âˆ‘_i -\log(P(C_{t_i} | â†’x_i; â†’w)). \\
\end{aligned}$$

<div class="algorithm">

**Input**: Input dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{0, +1\}$), learning rate $Î± âˆˆ â„^+$.<br>

- $â†’w â† 0$
- until convergence (or until patience is over), process batch of $N$ examples:
  - $g â† - \tfrac{1}{N} âˆ‘_i âˆ‡_â†’w \log(P(C_{t_i} | â†’x_i; â†’w)$
  - $â†’w â† â†’w - Î±â†’g$
</div>

---
# Linearity in Logistic Regression

![w=100%](lr_linearity.pdf)

---
section: MulticlassLR
# Multiclass Logistic Regression

To extend the binary logistic regression to a multiclass case with $K$ classes, we:
- Generate multiple outputs, notably $K$ outputs, each with its own set of
  weights, so that
  $$y(â†’x; â‡‰W)_i = â†’W_i â†’x.$$

~~~
- Generalize the sigmoid function to a $\softmax$ function, such that
  $$\softmax(â†’z)_i = \frac{e^{z_i}}{âˆ‘_j e^{z_j}}.$$

~~~
  Note that the original sigmoid function can be written as
  $$Ïƒ(x) = \softmax\big([x~~0]\big)_0 = \frac{e^x}{e^x + e^0} = \frac{1}{1 + e^{-x}}.$$

~~~
The resulting classifier is also known as _multinomial logistic regression_,
_maximum entropy classifier_ or _softmax regression_.

---
# Multiclass Logistic Regression

Note that as defined, the multiclass logistic regression is overparametrized.
It is possible to generate only $K-1$ outputs and define $z_K = 0$, which is
the approach used in binary logistic regression.

~~~
In this settings, analogously to binary logistic regression, we can recover the
interpretation of the model outputs $â†’y(â†’x; â‡‰W)$ (i.e., the $\softmax$ inputs)
as _logits_:

$$y(â†’x; â‡‰W)_i = \log\left(\frac{P(C_i | â†’x; â†’w)}{P(C_K | â†’x; â†’w)}\right).$$

~~~
However, in all our implementations, we will use weights for all $K$ outputs.

---
# Multiclass Logistic Regression

Using the $\softmax$ function, we naturally define that
$$P(C_i | â†’x; â‡‰W) = \softmax(â‡‰W_i â†’x)i = \frac{e^{â‡‰W_i â†’x}}{âˆ‘_j e^{â‡‰W_j â†’x}}.$$

~~~
We can then use MLE and train the model using stochastic gradient descent.

<div class="algorithm">

**Input**: Input dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{0, 1, â€¦, K-1\}$), learning rate $Î± âˆˆ â„^+$.<br>

- $â†’w â† 0$
- until convergence (or until patience is over), process batch of $N$ examples:
  - $g â† - \tfrac{1}{N} âˆ‘_i âˆ‡_â†’w \log(P(C_{t_i} | â†’x_i; â†’w)$
  - $â†’w â† â†’w - Î±â†’g$
</div>

---
# Multiclass Logistic Regression

Note that the decision regions of the binary/multiclass logistic regression are
singly connected and convex.

~~~
![w=40%,f=right](../03/classification_convex_regions.pdf)

To see this, consider $â†’x_A$ and $â†’x_B$ in the same decision region $R_k$.

~~~
Any point $â†’x$ lying on the line connecting them is their linear combination,
$â†’x = Î»â†’x_A + (1-Î»)â†’x_B$,
~~~
and from the linearity of $â†’y(â†’x) = â‡‰Wâ†’x$ it follows that
$$â†’y(â†’x) = Î»â†’y(â†’x_A) + (1-Î»)â†’y(â†’x_B).$$

~~~
Given that $y_k(â†’x_A)$ was the largest among $â†’y(â†’x_A)$ and also
given that $y_k(â†’x_B)$ was the largest among $â†’y(â†’x_B)$, it must
be the case that $y_k(â†’x)$ is the largest among all $â†’y(â†’x)$.

---
section: MSE as MLE
# Mean Square Error as MLE

During regression, we predict a number, not a real probability distribution.
In order to generate a distribution, we might consider a distribution with
the mean of the predicted value and a fixed variance $Ïƒ^2$ â€“ the most general
such a distribution is the normal distribution.

---
section: MSE as MLE
# Mean Square Error as MLE

Therefore, assume our model generates a distribution
$$P(y | â†’x; â†’w) = ğ“(y; f(â†’x; â†’w), Ïƒ^2).$$

~~~
Now we can apply MLE and get
$$\begin{aligned}
\argmax_â†’w P(ğ•; â†’w) =& \argmin_â†’w âˆ‘_{i=1}^m -\log P(y_i | â†’x_i ; â†’w) \\
                         =& -\argmin_â†’w âˆ‘_{i=1}^m \log \sqrt{\frac{1}{2Ï€Ïƒ^2}}
                            e ^ {\normalsize -\frac{(y_i - f(â†’x_i; â†’w))^2}{2Ïƒ^2}} \\
                         =& -\argmin_â†’w {\color{gray} m \log (2Ï€Ïƒ^2)^{-1/2} +}
                            âˆ‘_{i=1}^m -\frac{(y_i - f(â†’x_i; â†’w))^2}{2Ïƒ^2} \\
                         =& \argmin_â†’w âˆ‘_{i=1}^m \frac{(y_i - f(â†’x_i; â†’w))^2}{2Ïƒ^2} = \argmin_â†’w âˆ‘_{i=1}^m (y_i - f(â†’x_i; â†’w))^2.
\end{aligned}$$

---
section: MLP
# Multilayer Perceptron

![w=45%,h=center](mlp.svg)

---
# Multilayer Perceptron

There is a weight on each edge, and an activation function $f$ is performed on the
hidden layers, and optionally also on the output layer.
$$h_i = f\left(âˆ‘_j w_{i,j} x_j + b_i\right)$$

If the network is composed of layers, we can use matrix notation and write:
$$â†’h = f\left(â‡‰W â†’x + â†’b\right)$$

---
# Multilayer Perceptron and Biases

![w=55%,h=center](mlp_bias.pdf)

---
# Neural Network Activation Functions
## Output Layers
- none (linear regression if there are no hidden layers)

~~~
- sigmoid (logistic regression model if there are no hidden layers)
  $$Ïƒ(x) â‰ \frac{1}{1 + e^{-x}}$$

~~~
- $\softmax$ (maximum entropy model if there are no hidden layers)
  $$\softmax(â†’x) âˆ e^â†’x$$
  $$\softmax(â†’x)_i â‰ \frac{e^{x_i}}{âˆ‘_j e^{x_j}}$$

---
# Neural Network Activation Functions
## Hidden Layers
- none (does not help, composition of linear mapping is a linear mapping)

~~~
- $Ïƒ$ (but works badly â€“ nonsymmetrical, $\frac{dÏƒ}{dx}(0) = 1/4$)

~~~
- $\tanh$
    - result of making $Ïƒ$ symmetrical and making derivation in zero 1
    - $\tanh(x) = 2Ïƒ(2x) - 1$

~~~
- ReLU
    - $\max(0, x)$

---
# Training MLP

The multilayer perceptron can be trained using an SGD algorithm:

<div class="algorithm">

**Input**: Input dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{0, +1\}$), learning rate $Î± âˆˆ â„^+$.<br>

- $â†’w â† 0$
- until convergence (or until patience is over), process batch of $N$ examples:
  - $g â† âˆ‡_â†’w \frac{1}{N} âˆ‘_j - \log p(y_j | â†’x_j; â†’w)$
  - $â†’w â† â†’w - Î±â†’g$
</div>

---
# Training MLP â€“ Computing the Derivatives

Assume a network with an input of size $N_1$, then weights
$â†’U âˆˆ â„^{N_1 Ã— N_2}$, hidden layer with size $N_2$ and activation $h$,
weights $â†’V âˆˆ â„^{N_2 Ã— N_3}$, and finally an output layer of size $N_3$
with activation $o$.

![w=35%,h=center](mlp.svg)

---
# Training MLP â€“ Computing the Derivatives

(to be finished later)

---
# Universal Approximation Theorem '89

Let $Ï†(x)$ be a nonconstant, bounded and monotonically-increasing continuous function.
<br>(Later a proof was given also for $Ï† = \ReLU$.)

Then for any $Îµ > 0$ and any continuous function $f$ on $[0, 1]^m$ there exists
an $N âˆˆ â„•, v_i âˆˆ â„, b_i âˆˆ â„$ and $â†’{w_i} âˆˆ â„^m$, such that if we denote
$$F(â†’x) = âˆ‘_{i=1}^N v_i Ï†(â†’{w_i} \cdot â†’x + b_i)$$
then for all $x âˆˆ [0, 1]^m$
$$|F(â†’x) - f(â†’x)| < Îµ.$$

---
class: dbend
# Universal Approximation Theorem for ReLUs

Sketch of the proof:

~~~
- If a function is continuous on a closed interval, it can be approximated by
  a sequence of lines to arbitrary precision.

![w=50%,h=center](universal_approximation_example.png)

~~~
- However, we can create a sequence of $k$ linear segments as a sum of $k$ ReLU
  units â€“ on every endpoint a new ReLU starts (i.e., the input ReLU value is
  zero at the endpoint), with a tangent which is the difference between the
  target tanget and the tangent of the approximation until this point.

---
class: dbend
# Universal Approximation Theorem for Squashes

Sketch of the proof for a squashing function $Ï†(x)$ (i.e., nonconstant, bounded and
monotonically-increasing continuous function like sigmoid):

~~~
- We can prove $Ï†$ can be arbitrarily close to a hard threshold by compressing
  it horizontally.

![w=38%,h=center](universal_approximation_squash.png)

~~~
- Then we approximate the original function using a series of straight line
  segments

![w=38%,h=center](universal_approximation_rectangles.png)

---
section: LagrangeMult
# Lagrange Multipliers â€“ Equality Constraints

![w=28%,f=right](lagrange_equalities.pdf)

Given a funtion $J(â†’x)$, we can find a maximum with respect to a vector
$â†’x âˆˆ â„^d$, by investigating the critical points $âˆ‡_â†’x J(â†’x) = 0$.

~~~
Consider now finding maximum subject to a constraint $g(â†’x) = 0$.

~~~
- Note that $âˆ‡_â†’x g(â†’x)$ is orthogonal to the surface of the constraing, because
  if $â†’x$ and a nearby point $â†’x+â†’Îµ$ lie on the surface, from the Taylor
  expansion $g(â†’x+â†’Îµ) â‰ˆ g(â†’x) + â†’Îµ^T âˆ‡_â†’x g(â†’x)$ we get $â†’Îµ^T âˆ‡_â†’x g(â†’x) â‰ˆ 0$.

~~~
- In the seeked maximum, $âˆ‡_â†’x f(â†’x)$ must also be orthogonal to the constraing
  surface (or else moving in the direction of the derivative would increase the
  value).

~~~
- Therefore, there must exist $Î»$ such that $âˆ‡_â†’x f + Î»âˆ‡_â†’x g = 0$.

---
# Lagrange Multipliers â€“ Equality Constraints

![w=28%,f=right](lagrange_equalities.pdf)

We therefore introduce the _Lagrangian function_
$$L(â†’x, Î») â‰ f(â†’x) + Î»g(â†’x).$$

~~~
We can then find the maximum under the constraing by inspecting critical points
of $L(â†’x, Î»)$ with respect to both $â†’x$ and $Î»$:
- $\frac{âˆ‚L}{âˆ‚Î»} = 0$ leads to $g(â†’x)=0$;
- $\frac{âˆ‚L}{âˆ‚â†’x} = 0$ is the previously derived $âˆ‡_â†’x f + Î»âˆ‡_â†’x g = 0$.



---
section: VariationalCalc
class: dbend
# Calculus of Variations

Many optimization techniques depend on minimizing a function $J(â†’w)$ with
respect to a vector $â†’w âˆˆ â„^d$, by investigating the critical points $âˆ‡_â†’w J(â†’w) = 0$.

~~~
A function of a function, $J[f]$, is known as a **functional**, for example
entropy $H[â‹…]$.

~~~
Similarly to partial derivatives, we can take **functional derivatives** of
a functional $J[f]$ with respect to individual values $f(â†’x)$ for all points
$â†’x$. The functional derivative of $J$ with respect to a function $f$ in a point
$â†’x$ is denoted as
$$\frac{âˆ‚}{âˆ‚f(â†’x)} J.$$

~~~
For this class, we will use only the following theorem, which states that for
all differentiable functions $f$ and differentiable functions $g(y=f(â†’x), â†’x)$ with
continuous derivatives, it holds that
$$\frac{âˆ‚}{âˆ‚f(â†’x)} âˆ«g(f(â†’x), â†’x) \dâ†’x = \frac{âˆ‚}{âˆ‚y} g(y, â†’x).$$

---
section: VariationalCalc
class: dbend
# Calculus of Variations

An intuitive view is to think about $f(â†’x)$ as a vector of uncountably many
elements (for every value $â†’x)$. In this interpretation the result is analogous
to computing partial derivatives of a vector $â†’w âˆˆ â„^d$:
$$\frac{âˆ‚}{âˆ‚w_i} âˆ‘_j g(w_j, â†’x) = \frac{âˆ‚}{âˆ‚w_i} g(w_i, â†’x).$$
$$\frac{âˆ‚}{âˆ‚f(â†’x)} âˆ«g(f(â†’x), â†’x) \dâ†’x = \frac{âˆ‚}{âˆ‚y} g(y, â†’x).$$

---
section: NormalAsMaxEntropy
class: dbend
# Function with Maximum Entropy

What distribution over $â„$ maximizes entropy $H[p] = -ğ”¼_x \log p(x)$?

~~~
For continuous values, the entropy is an integral $H[p] = -âˆ«p(x) \log p(x) \d x$.

~~~
We cannot just maximize $H$ with respect to a function $p$, because:
- the result might not be a probability distribution â€“ we need to add
  a constraint that $âˆ«p(x) \d x=1$;
~~~
- the problem is unspecified because a distribution can be shifted without
  changing entropy â€“ we add a constraing $ğ”¼[x] = Î¼$;
~~~
- because entropy increases as variance increases, we ask which distribution
  with a _fixed_ variance $Ïƒ^2$ has maximum entropy â€“ adding a constraing
  $\Var(x) = Ïƒ^2$.

---
class: dbend
# Function with Maximum Entropy

Lagrangian of all the constraings and the entropy function is
$$L(p; Î¼, Ïƒ^2) = Î»_1 \Big(âˆ«p(x) \d x - 1\Big) + Î»_2 \big(ğ”¼[x] - Î¼\big) + Î»_3\big(\Var(x) - Ïƒ^2\big) + H[p].$$

~~~
By expanding all definitions to integrals, we get
$$\begin{aligned}
L(p; Î¼, Ïƒ^2) =& âˆ«\Big(Î»_1 p(x) + Î»_2 p(x) x Î»_3 p(x) (x - Î¼)^2 - p(x)\log p(x) \Big) \d x - \\
              & -Î»_1 - Î¼ Î»_2 - Ïƒ^2Î»_3.
\end{aligned}$$

~~~
The functional derivative of $L$ is:
$$\frac{âˆ‚}{âˆ‚p(x)} L(p; Î¼, Ïƒ^2) = Î»_1 + Î»_2 x + Î»_3 (x - Î¼)^2 - 1 - \log p(x) = 0.$$

---
class: dbend
# Function with Maximum Entropy
Rearrangint the functional derivative of $L$:
$$\frac{âˆ‚}{âˆ‚p(x)} L(p; Î¼, Ïƒ^2) = Î»_1 + Î»_2 x + Î»_3 (x - Î¼)^2 - 1 - \log p(x) = 0.$$

we obtain
$$p(x) = \exp\Big(Î»_1 + Î»_2 x + Î»_3 (x-Î¼)^2 - 1\Big).$$

~~~
We can verify that setting $Î»_1 = 1 - \log Ïƒ \sqrt{2Ï€}$, $Î»_2=0$ and $Î»_3=-1/(2Ïƒ^2)$
fulfils all the constraints, arriving at
$$p(x) = ğ“(x; Î¼, Ïƒ^2).$$
