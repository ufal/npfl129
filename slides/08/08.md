title: NPFL129, Lecture 8
class: title, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Correlation, Model Combination

## Jind≈ôich Libovick√Ω <small>(reusing materials by Milan Straka)</small>

### November 25, 2024

---
class: middle
# Today's Lecture Objectives

After this lecture you should be able to

- Explain and implement different ways of measuring correlation: Pearson's
  correlation, Spearman's correlation, Kendall's $\tau$.

- Decide if correlation is a good metric for your model.

- Measure inter-annotator agreement and draw conclusions for data
  cleaning and for limits of your models.

- Use correlation with human judgment to validate evaluation metrics.

- Ensemble models with uncorrelated predictions.

- Distill ensembles into smaller models.

---
section: Covariance
class: section
# Covariance

---
# Covariance

Given a collection of random variables $‚Åáx_1, ‚Ä¶, ‚Åáx_N$, we know that
$$ùîº\left[‚àë\nolimits_i ‚Åáx_i\right] = ‚àë_i ùîº \big[‚Åáx_i\big].$$

~~~
But how about $\Var\big(‚àë_i ‚Åáx_i\big)$?

~~~
$\displaystyle \kern5em\Var\left(‚àë\nolimits_i ‚Åáx_i\right)
 = ùîº\left[\left(‚àë\nolimits_i ‚Åáx_i - ‚àë\nolimits_i ùîº[‚Åáx_i]\right)^2\right]$

~~~
$\displaystyle \phantom{\kern5em\Var\left(‚àë\nolimits_i ‚Åáx_i\right)}
 = ùîº\left[\left(‚àë\nolimits_i \big(‚Åáx_i - ùîº[‚Åáx_i]\big)\right)^2\right]$

~~~
$\displaystyle \phantom{\kern5em\Var\left(‚àë\nolimits_i ‚Åáx_i\right)}
 = ùîº\left[‚àë\nolimits_i ‚àë\nolimits_j \big(‚Åáx_i - ùîº[‚Åáx_i]\big) \big(‚Åáx_j - ùîº[‚Åáx_j]\big)\right]$

~~~
$\displaystyle \phantom{\kern5em\Var\left(‚àë\nolimits_i ‚Åáx_i\right)}
 = ‚àë_i ‚àë_j ùîº\left[\big(‚Åáx_i - ùîº[‚Åáx_i]\big) \big(‚Åáx_j - ùîº[‚Åáx_j]\big)\right].$

---
# Covariance

We define **covariance** of two random variables $‚Åáx, ‚Åáy$ as
$$\Cov(‚Åáx, ‚Åáy) = ùîº\Big[\big(‚Åáx - ùîº[‚Åáx]\big) \big(‚Åáy - ùîº[‚Åáy]\big)\Big].$$

~~~
Then,
$$\Var\left(‚àë\nolimits_i ‚Åáx_i\right) = ‚àë_i ‚àë_j \Cov(‚Åáx_i, ‚Åáx_j).$$

~~~
Note that $\Cov(‚Åáx, ‚Åáx) = \Var(‚Åáx)$ and that we can write covariance as
$$\begin{aligned}
  \Cov(‚Åáx, ‚Åáy)
   &= ùîº\Big[\big(‚Åáx - ùîº[‚Åáx]\big) \big(‚Åáy - ùîº[‚Åáy]\big)\Big] \\
   &= ùîº\big[‚Åáx ‚Åáy - ‚Åáx ùîº[‚Åáy] - ùîº[‚Åáx] ‚Åáy + ùîº[‚Åáx] ùîº[‚Åáy]\big] \\
   &= ùîº\big[‚Åáx ‚Åáy\big] - ùîº\big[‚Åáx\big] ùîº\big[‚Åáy\big].
\end{aligned}$$

---
section: Correlation
class: section
# Correlation

---
# Correlation

Random variables $‚Åáx, ‚Åáy$ are **uncorrelated** if $\Cov(‚Åáx, ‚Åáy) = 0$;
otherwise, they are **correlated**.

~~~
Note that two _independent_ random variables are uncorrelated, because

$\displaystyle \kern10em\mathllap{\Cov(‚Åáx, ‚Åáy)} = ùîº\Big[\big(‚Åáx - ùîº[‚Åáx]\big) \big(‚Åáy - ùîº[‚Åáy]\big)\Big]$

~~~
$\displaystyle \kern10em{} = ‚àë_{x,y} P(x, y) \big(x - ùîº[x]\big) \big(y - ùîº[y]\big)$

~~~
$\displaystyle \kern10em{} = ‚àë_{x,y} P(x) \big(x - ùîº[x]\big) P(y) \big(y - ùîº[y]\big)$

~~~
$\displaystyle \kern10em{} = \left(‚àë_x P(x) \big(x - ùîº[x]\big)\right) \left(‚àë_y P(y) \big(y - ùîº[y]\big)\right)$

~~~
$\displaystyle \kern10em{} = ùîº_‚Åáx \big[‚Åáx - ùîº[‚Åáx]\big] ùîº_‚Åáy \big[‚Åáy - ùîº[‚Åáy]\big] = 0.$

~~~
However, uncorrelated random variables can be dependent: random
uniform $‚Åáx$ on $[-1, 1]$ and $‚Åáy = |‚Åáx|$ are not independent ($‚Åáy$ is
completely determined by $‚Åáx$), but they are uncorrelated.

---
# Pearson correlation coefficient

There are several ways to measure correlation of random variables $‚Åáx, ‚Åáy$.

**Pearson correlation coefficient**, denoted as $œÅ$ or $r$, is defined as
$$\begin{aligned}
  œÅ &‚âù \frac{\Cov(‚Åáx, ‚Åáy)}{\sqrt{\Var(‚Åáx)} \sqrt{\Var(‚Åáy)}}, \\
  r &‚âù \frac{‚àë_i (x_i - xÃÑ) (y_i - yÃÑ)}{\sqrt{‚àë_i (x_i - xÃÑ)^2} \sqrt{‚àë_i (y_i - yÃÑ)^2}},
\end{aligned}$$
where:
~~~
- $œÅ$ is used when the full expectation is computed (population Pearson
  correlation coefficient);
~~~
- $r$ is used when estimating the coefficient from data (sample Pearson
  correlation coefficient);
  - $xÃÑ$ and $yÃÑ$ are sample estimates of the respective means.

---
class: dbend
# Pearson correlation coefficient

The value of Pearson correlation coefficient is in fact normalized covariance,
because its value is always bounded by $-1 ‚â§ œÅ ‚â§ 1$ (and the same holds for $r$).

~~~
The bound can be derived from

$\displaystyle \kern5em\mathllap{0} ‚â§ ùîº\bigg[\bigg(\frac{(‚Åáx - ùîº[‚Åáx])}{\sqrt{\Var(‚Åáx)}} - œÅ\frac{(‚Åáy - ùîº[‚Åáy])}{\sqrt{\Var(‚Åáy)}}\bigg)^2\bigg]$

~~~
$\displaystyle \kern5em{} = ùîº\bigg[\frac{(‚Åáx - ùîº[‚Åáx])^2}{\Var(‚Åáx)}\bigg]
                            - 2œÅùîº\bigg[\frac{(‚Åáx - ùîº[‚Åáx])}{\sqrt{\Var(‚Åáx)}}\frac{(‚Åáy - ùîº[‚Åáy])}{\sqrt{\Var(‚Åáy)}}\bigg]
                            + œÅ^2 ùîº\bigg[\frac{(‚Åáy - ùîº[‚Åáy])^2}{\Var(‚Åáy)}\bigg]$

~~~
$\displaystyle \kern5em{} = \frac{\Var(‚Åáx)}{\Var(‚Åáx)} - 2œÅ‚ãÖœÅ + œÅ^2 \frac{\Var(‚Åáy)}{\Var(‚Åáy)} = 1 - œÅ^2,$

~~~
which yields $œÅ^2 ‚â§ 1$.

---
# Pearson correlation coefficient

Pearson correlation coefficient quantifies **linear dependence** of two
random variables.

![w=84%,h=center](correlation_coefficient.png)

---
# Pearson correlation coefficient

Pearson correlation coefficient quantifies **linear dependence** of two
random variables.

![w=100%,h=center](correlation_examples.svgz)

---
# Pearson correlation coefficient

The four displayed variables have the same mean 7.5, variance 4.12,
Pearson correlation coefficient 0.816 and regression line $3 + \frac{1}{2}x$.

![w=60%,h=center](ancombes_quartet.svgz)

---
# Nonlinear Correlation ‚Äì Spearman's $œÅ$

To measure nonlinear correlation as well, two coefficients are commonly used.

### Spearman's rank correlation coefficient $œÅ$
Spearman's $œÅ$ is the Pearson correlation coefficient measured on **ranks** of the
original data, where a rank of an element is its index in sorted ascending
order.

![w=100%](spearman.svgz)

---
# Nonlinear Correlation ‚Äì Kendall's $œÑ$

### Kendall rank correlation coefficient $œÑ$
Kendall's $œÑ$ measures the number of _concordant pairs_ (pairs where $y$
increases/decreases when $x$ does), minus the _discordant pairs_
(where $y$ increases/decreases when $x$ does the opposite):

$$\begin{aligned}
  œÑ &‚âù \frac{|\{\mathrm{pairs}~i ‚â† j: x_j > x_i, y_j > y_i\}| - |\{\mathrm{pairs}~i ‚â† j: x_j > x_i, y_j < y_i\}|}{\binom{n}{2}} \\
    &= \frac{‚àë_{i < j} \sign(x_j - x_i) \sign(y_j - y_i)}{\binom{n}{2}}.
\end{aligned}$$

~~~
There is no clear consensus on whether to use Spearman's $œÅ$ or Kendall's $œÑ$.
When there are no/few ties in the data, Kendall's $œÑ$ offers two minor
advantages: $\frac{1+œÑ}{2}$ can be interpreted as a probability of
a concordant pair, and Kendall's $œÑ$ converges to a normal distribution faster.

~~~
As defined, the range of Kendall's $œÑ ‚àà [-1, 1]$. However, if there are ties,
its range is smaller ‚Äì therefore, several corrections (not discussed here) exist
to adjust its value in case of ties.

---
class: middle
# Correlation is not causation

![w=90%,mw=40%,h=left](correlation_meme.png)![w=50%](correlation_xkcd.png)

---
section: Correlation in ML
class: section
# Correlation in Machine Learning

---
# Use of Correlation in Machine Learning

In ML, correlation is commonly used as

- Evaluation metric for some tasks;

- Measuring data annotation quality;

- Assessing the quality of automatic metrics by comparing them to human judgment.

---
# Correlation as evaluation metric

- Learning to rank (e.g., document retrieval): we do not care about the actual values.

   - Kendall's $\tau$, Spearman's correlation coefficient.

   - When we want the correct items to rank before incorrect ones: precision
     (assuming fixed top-$k$, typically at 5, 10), recall (often ill-defined),
     mean reciprocal rank

   $$\operatorname{MRR} = \frac{1}{N}\sum_{i=1}^{N}\frac{1}{\text{rank of the first relevant item}}.$$
~~~
- Evaluating pair similarity: word embeddings, sentence embeddings.

   - Similarity estimates from psycholinguistic experiments: scores for word/sentence pairs.

   - Measure Pearson/Spearman correlation coefficient between embedding distances and similarity scores.

---
# Inter-annotator agreement (1)

![w=35%,f=right](iaa_meme.png)

- Inter-annotator agreement can tell us

   - how well defined the task is;

   - how reliable annotators/user ratings are;

   - which data items are suspicious / difficult.

<br>
~~~

- For continuous target values: Pearson's/Spearman's correlation coefficient.

~~~

- For classification tasks: Cohen's $\kappa$.

$$\kappa = \frac{p_O - p_E}{1 - p_E},$$

    where $p_O$ is the observed agreement, and $p_E$ is the expected agreement by chance.

---
# Inter-annotator agreement (2)

- Can be used to filter out confusing data points and unreliable annotators.

- Not all outliers are noise! Low IAA can reveal cultural differences.

<br>
<br>

~~~
IAA sets a natural upper boundary for ML performance. Performance over IAA is
suspicious!

<br>

![w=60%,h=center](baseline_to_agreement.svgz)

~~~
* Trivial baseline for classification: majority class; for regression average,
  or something based on simple rules.

* Performance over IAA is more likely overfitting for the way the data is
  curated than super-human performance.

---
# Correlation with human judgment

For some tasks, it might not be clear how to measure the model performance:

**Grammar checking**: the $\beta$ parameter
![w=24%,h=center](correlation_metrics.svgz)

~~~

**Machine translation**: evaluation is subjective by definition, we design
metrics to correlate with human judgment.

- SoTA machine translation metrics are typically machine-learned.

- Different metrics might be suitable for different tiers of translation quality.

- There is an annual competition in MT quality and MT metric quality.

---
section: Model Combination
class: section
# Model Combination

---
# Model Combination aka Ensembling

The goal of **ensembling** is to combine several models in order to reach
higher performance.

~~~
The simplest approach is to train several independent models and then combine
their predictions by averaging or voting.

~~~
The terminology varies, but for classification:
- voting (or hard voting) usually means predicting the class predicted most
  often by the individual models,
~~~
- averaging (or soft voting) denotes averaging the returned model distributions
  and predicting the class with the highest probability.

~~~
The main idea behind ensembling is that if models have uncorrelated
errors, then by averaging model predictions the errors will cancel out.

---
# Visualization of Ensembling Performance

Consider ensembling predictions generated uniformly on a planar disc:
![w=100%](ensemble_visualization-r1.svgz)
~~~
![w=100%](ensemble_visualization-r1b.svgz)
~~~
![w=100%](ensemble_visualization-r42.svgz)

---
# Model Combination aka Ensembling

If we denote the prediction of a model $y_i$ on a training example $(‚Üíx, t)$ as
$y_i(‚Üíx) = t + Œµ_i(‚Üíx)$, so that $Œµ_i(‚Üíx)$ is the model error on example $‚Üíx$,
the mean squared error of the model is
$$ùîº\big[(y_i(‚Üíx) - t)^2\big] = ùîº\big[Œµ_i^2(‚Üíx)\big].$$

~~~
Considering $M$ models, we analogously get that the mean squared error
of the ensemble is
$$ùîº\bigg[\Big(\frac{1}{M} ‚àë\nolimits_i Œµ_i(‚Üíx)\Big)^2\bigg].$$

~~~
Finally, assuming that the individual errors $Œµ_i$ have zero mean and are _uncorrelated_,
we get that $ùîº\big[Œµ_i(‚Üíx) Œµ_j(‚Üíx)\big] = 0$ for $i ‚â† j$, and therefore,
~~~
$$ùîº\Big[\Big(\frac{1}{M} ‚àë\nolimits_i Œµ_i(‚Üíx)\Big)^2\Big]
= ùîº\Big[\frac{1}{M^2} ‚àë\nolimits_{i,j} Œµ_i(‚Üíx) Œµ_j(‚Üíx)\Big]
= \frac{1}{M} ùîº\Big[\frac{1}{M} ‚àë\nolimits_i Œµ_i^2(‚Üíx)\Big],$$
~~~
so the average error of the ensemble is $\frac{1}{M}$ times the average error
of the individual models.

---
# Bagging ‚Äì Bootstrap Aggregation

For neural network models, training models with independent random initialization is
usually enough, given that the loss has many local minima, so the models tend to
be quite independent just when using different random initialization.

~~~
However, algorithms with convex loss functions usually converge to the same
optimum independent of randomization.

~~~
In these cases, we can use **bagging**, which stands for **bootstrap
aggregation**.

~~~
![w=50%,f=right](bagging.svgz)

In bagging, we construct a different dataset for every model we train.
We construct it using **bootstrapping** ‚Äì we sample as many training instances
as the original dataset has, but **with replacement**.

Such dataset is sampled using the same empirical data distribution and has the
same size, but is not identical.

---
# Knowledge Distillation

- Model ensemble might be too slow or too big to use.

~~~

- Knowledge distillation = training a **student** model that mimics behaviour
  of a **teacher model** (a bigger one or model ensemble).

~~~

**Algorithm:**

1. Process training data (or additional unlabelled data) with the best current
   model and get the output distribution $p_\text{teacher}(\boldsymbol y |
   \boldsymbol x; \boldsymbol w )$ (sometimes called _pseudolikelihood_).

2. Train a model with $H\left(p_\text{teacher}(\boldsymbol y | \boldsymbol x;
   \boldsymbol w), p_\text{student}(\boldsymbol y | \boldsymbol x; \boldsymbol
   w )\right)$ as the training objective.

~~~
**Intuition:** Complete distribution provides stronger supervision than just
a one-hot target, so it is easier for the smaller model to learn from such
synthetic data.

~~~
<br>

<small>Historical note: Term knowledge distillation comes from a 2015 paper by
Geoffrey Hinton et al. Before, a similar approach was called model
compression.</small>


---
class: summary
# Today's Lecture Objectives

After this lecture you should be able to

- Explain and implement different ways of measuring correlation: Pearson's
  correlation, Spearman's correlation, Kendall's $\tau$

- Decide if correlation is a good metric for your model

- Measure inter-annotator agreement and draw conclusions for data cleaning and
  for limits of your models

- Use correlation with human judgment to validate evaluation metrics

- Ensemble models with uncorrelated predictions

- Distill ensembles into smaller models.

