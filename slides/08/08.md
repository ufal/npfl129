title: NPFL129, Lecture 8
class: title, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Correlation, Model Combination

## Jind≈ôich Libovick√Ω <small>(reusing materials by Milan Straka)</small>

### November 20, 2025

---
class: middle
# Today's Lecture Objectives

After this lecture you should be able to

- Explain and implement different ways of measuring correlation: Pearson's
  correlation, Spearman's correlation, Kendall's $\tau$.

- Decide if correlation is a good metric for your model.

- Measure inter-annotator agreement and draw conclusions for data
  cleaning and for limits of your models.

- Use correlation with human judgment to validate evaluation metrics.

---
section: Covariance
class: section
# Covariance

---
# Covariance

Given a collection of random variables $‚Åáx_1, ‚Ä¶, ‚Åáx_N$, we know that
$$ùîº\left[‚àë\nolimits_i ‚Åáx_i\right] = ‚àë_i ùîº \big[‚Åáx_i\big].$$

~~~
But how about $\Var\big(‚àë_i ‚Åáx_i\big)$?

~~~
$\displaystyle \kern5em\Var\left(‚àë\nolimits_i ‚Åáx_i\right)
 = ùîº\left[\left(‚àë\nolimits_i ‚Åáx_i - ‚àë\nolimits_i ùîº[‚Åáx_i]\right)^2\right]$

~~~
$\displaystyle \phantom{\kern5em\Var\left(‚àë\nolimits_i ‚Åáx_i\right)}
 = ùîº\left[\left(‚àë\nolimits_i \big(‚Åáx_i - ùîº[‚Åáx_i]\big)\right)^2\right]$

~~~
$\displaystyle \phantom{\kern5em\Var\left(‚àë\nolimits_i ‚Åáx_i\right)}
 = ùîº\left[‚àë\nolimits_i ‚àë\nolimits_j \big(‚Åáx_i - ùîº[‚Åáx_i]\big) \big(‚Åáx_j - ùîº[‚Åáx_j]\big)\right]$

~~~
$\displaystyle \phantom{\kern5em\Var\left(‚àë\nolimits_i ‚Åáx_i\right)}
 = ‚àë_i ‚àë_j ùîº\left[\big(‚Åáx_i - ùîº[‚Åáx_i]\big) \big(‚Åáx_j - ùîº[‚Åáx_j]\big)\right].$

---
# Covariance

We define **covariance** of two random variables $‚Åáx, ‚Åáy$ as
$$\Cov(‚Åáx, ‚Åáy) = ùîº\Big[\big(‚Åáx - ùîº[‚Åáx]\big) \big(‚Åáy - ùîº[‚Åáy]\big)\Big].$$

~~~
Then,
$$\Var\left(‚àë\nolimits_i ‚Åáx_i\right) = ‚àë_i ‚àë_j \Cov(‚Åáx_i, ‚Åáx_j).$$

~~~
Note that $\Cov(‚Åáx, ‚Åáx) = \Var(‚Åáx)$ and that we can write covariance as
$$\begin{aligned}
  \Cov(‚Åáx, ‚Åáy)
   &= ùîº\Big[\big(‚Åáx - ùîº[‚Åáx]\big) \big(‚Åáy - ùîº[‚Åáy]\big)\Big] \\
   &= ùîº\big[‚Åáx ‚Åáy - ‚Åáx ùîº[‚Åáy] - ùîº[‚Åáx] ‚Åáy + ùîº[‚Åáx] ùîº[‚Åáy]\big] \\
   &= ùîº\big[‚Åáx ‚Åáy\big] - ùîº\big[‚Åáx\big] ùîº\big[‚Åáy\big].
\end{aligned}$$

---
section: Correlation
class: section
# Correlation

---
# Correlation

Random variables $‚Åáx, ‚Åáy$ are **uncorrelated** if $\Cov(‚Åáx, ‚Åáy) = 0$;
otherwise, they are **correlated**.

~~~
Note that two _independent_ random variables are uncorrelated, because

$\displaystyle \kern10em\mathllap{\Cov(‚Åáx, ‚Åáy)} = ùîº\Big[\big(‚Åáx - ùîº[‚Åáx]\big) \big(‚Åáy - ùîº[‚Åáy]\big)\Big]$

~~~
$\displaystyle \kern10em{} = ‚àë_{x,y} P(x, y) \big(x - ùîº[x]\big) \big(y - ùîº[y]\big)$

~~~
$\displaystyle \kern10em{} = ‚àë_{x,y} P(x) \big(x - ùîº[x]\big) P(y) \big(y - ùîº[y]\big)$

~~~
$\displaystyle \kern10em{} = \left(‚àë_x P(x) \big(x - ùîº[x]\big)\right) \left(‚àë_y P(y) \big(y - ùîº[y]\big)\right)$

~~~
$\displaystyle \kern10em{} = ùîº_‚Åáx \big[‚Åáx - ùîº[‚Åáx]\big] ùîº_‚Åáy \big[‚Åáy - ùîº[‚Åáy]\big] = 0.$

~~~
However, dependent random variables can be uncorrelated ‚Äì random
uniform $‚Åáx$ on $[-1, 1]$ and $‚Åáy = |‚Åáx|$ are not independent ($‚Åáy$ is
completely determined by $‚Åáx$), but they are uncorrelated.

---
# Pearson correlation coefficient

There are several ways to measure correlation of random variables $‚Åáx, ‚Åáy$.

**Pearson correlation coefficient**, denoted as $œÅ$ or $r$, is defined as
$$\begin{aligned}
  œÅ &‚âù \frac{\Cov(‚Åáx, ‚Åáy)}{\sqrt{\Var(‚Åáx)} \sqrt{\Var(‚Åáy)}} \\
  r &‚âù \frac{‚àë_i (x_i - xÃÑ) (y_i - yÃÑ)}{\sqrt{‚àë_i (x_i - xÃÑ)^2} \sqrt{‚àë_i (y_i - yÃÑ)^2}},
\end{aligned}$$
where:
~~~
- $œÅ$ is used when the full expectation is computed (population Pearson
  correlation coefficient);
~~~
- $r$ is used when estimating the coefficient from data (sample Pearson
  correlation coefficient);
  - $xÃÑ$ and $yÃÑ$ are sample estimates of the respective means.

---
class: dbend
# Pearson correlation coefficient

The value of Pearson correlation coefficient is in fact normalized covariance,
because its value is always bounded by $-1 ‚â§ œÅ ‚â§ 1$ (and the same holds for $r$).

~~~
The bound can be derived from

$\displaystyle \kern5em\mathllap{0} ‚â§ ùîº\bigg[\bigg(\frac{(‚Åáx - ùîº[‚Åáx])}{\sqrt{\Var(‚Åáx)}} - œÅ\frac{(‚Åáy - ùîº[‚Åáy])}{\sqrt{\Var(‚Åáy)}}\bigg)^2\bigg]$

~~~
$\displaystyle \kern5em{} = ùîº\bigg[\frac{(‚Åáx - ùîº[‚Åáx])^2}{\Var(‚Åáx)}\bigg]
                            - 2œÅùîº\bigg[\frac{(‚Åáx - ùîº[‚Åáx])}{\sqrt{\Var(‚Åáx)}}\frac{(‚Åáy - ùîº[‚Åáy])}{\sqrt{\Var(‚Åáy)}}\bigg]
                            + œÅ^2 ùîº\bigg[\frac{(‚Åáy - ùîº[‚Åáy])^2}{\Var(‚Åáy)}\bigg]$

~~~
$\displaystyle \kern5em{} = \frac{\Var(‚Åáx)}{\Var(‚Åáx)} - 2œÅ‚ãÖœÅ + œÅ^2 \frac{\Var(‚Åáy)}{\Var(‚Åáy)} = 1 - œÅ^2,$

~~~
which yields $œÅ^2 ‚â§ 1$.

---
# Pearson correlation coefficient

Pearson correlation coefficient quantifies **linear dependence** of the two
random variables.

![w=84%,h=center](correlation_coefficient.png)

---
# Pearson correlation coefficient

Pearson correlation coefficient quantifies **linear dependence** of the two
random variables.

![w=100%,h=center](correlation_examples.svgz)

---
# Pearson correlation coefficient

The four displayed variables have the same mean 7.5, variance 4.12,
Pearson correlation coefficient 0.816 and regression line $3 + \frac{1}{2}x$.

![w=60%,h=center](ancombes_quartet.svgz)

---
# Nonlinear Correlation ‚Äì Spearman's $œÅ$

To measure also nonlinear correlation, two coefficients are commonly used.

### Spearman's rank correlation coefficient $œÅ$
Spearman's $œÅ$ is Pearson correlation coefficient measured on **ranks** of the
original data, where a rank of an element is its index in sorted ascending
order.

![w=100%](spearman.svgz)

---
# Nonlinear Correlation ‚Äì Kendall's $œÑ$

### Kendall rank correlation coefficient $œÑ$
Kendall's $œÑ$ measures the amount of _concordant pairs_ (pairs where $y$
increases/decreases when $x$ does), minus the _discordant pairs_
(where $y$ increases/decreases when $x$ does the opposite):

$$\begin{aligned}
  œÑ &‚âù \frac{|\{\mathrm{pairs}~i ‚â† j: x_j > x_i, y_j > y_i\}| - |\{\mathrm{pairs}~i ‚â† j: x_j > x_i, y_j < y_i\}|}{\binom{n}{2}} \\
    &= \frac{‚àë_{i < j} \sign(x_j - x_i) \sign(y_j - y_i)}{\binom{n}{2}}.
\end{aligned}$$

~~~
There is no clear consensus on whether to use Spearman's $œÅ$ or Kendall's $œÑ$.
When there are no/few ties in the data, Kendall's $œÑ$ offers two minor
advantages ‚Äì $\frac{1+œÑ}{2}$ can be interpreted as a probability of
a concordant pair, and Kendall's $œÑ$ converges to a normal distribution faster.

~~~
As defined, the range of Kendall's $œÑ ‚àà [-1, 1]$. However, if there are ties,
its range is smaller ‚Äì therefore, several corrections (not discussed here) exist
to adjust its value in case of ties.

---
class: middle
# Correlation is not causation

![w=90%,mw=40%,h=left](correlation_meme.png)![w=50%](correlation_xkcd.png)

---
section: Correlation in ML
class: section
# Correlation in Machine Learning

---
# Use of Correlation in Machine Learning

In ML, correlation is commonly used as

- Evaluation metric for some tasks;

- Measuring data annotation quality;

- Assessing the quality of automatic metrics by comparing them to human judgment.

---
# Correlation as evaluation metric

- Learning to rank (e.g., document retrieval): we do not care about the actual values

   - Kendall's $\tau$, Spearman's correlation

   - When we want the correct items to rank before incorrect ones: precision
     (assuming fixed top-$k$, typically at 5, 10), recall (often ill-defined),
     mean reciprocal rank

   $$\operatorname{MRR} = \frac{1}{N}\sum_{i=1}^{N}\frac{1}{\text{rank of the first relevant item}}$$
~~~
- Evaluating pair similarity: word embeddings, sentence embeddings

   - Similarity estimates from psycholinguistic experiments: scores for word/sentence pairs

   - Measure Pearson/Spearman correlation between embedding distances and similarity scores

---
# Inter-annotator agreement (1)

![w=35%,f=right](iaa_meme.png)

- Inter-annotator agreement can tell us

   - How well defined the task is

   - How reliable annotators/user ratings are

   - What data items are suspicious / difficult

<br>
~~~

- For continuous target values: Pearson's/Spearman's correlation

~~~

- For classification tasks: Cohen's $\kappa$ \
    $p_O$ is observed agreement, $p_E$ expected agreement by chance

$$\kappa = \frac{p_O - p_E}{1 - p_E}$$

---
# Inter-annotator agreement (2)

- Can be used to filter out confusing data points and unreliable annotators

- Not all outliers are noise! Low IAA can reveal cultural differences.

<br>
<br>

~~~
IAA sets natural upper boundary for ML performance. Performance over IAA is
suspicious!

<br>

![w=60%,h=center](baseline_to_agreement.svgz)

~~~
* Trivial baseline for classification: majority class, for regression average,
  or something based on simple rules

* Performance over IAA is more likely overfitting for the way the data is
  curated than super-human performance.

---
# Correlation with human judgment

For some tasks, it might not be clear how to measure the model performance:

**Grammar checking**: the $\beta$ parameter
![w=24%,h=center](correlation_metrics.svgz)

~~~

**Machine translation**: evaluation is subjective by definition, we design
metrics to correlate with human judgment.

- SoTA machine translation metrics are typically machine-learned.

- Different metrics might be suitable for different tiers of translation quality.

- There is an annual competition in MT quality and MT metric quality.

---
class: summary
# Today's Lecture Objectives

After this lecture you should be able to

- Explain and implement different ways of measuring correlation: Pearson's
  correlation, Spearman's correlation, Kendall's $\tau$

- Decide if correlation is a good metric for your model

- Measure inter-annotator agreement and draw conclusions for data cleaning and
  for limits of your models

- Use correlation with human judgment to validate evaluation metrics
