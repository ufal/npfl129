title: NPFL129, Lecture 8
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# SVR, Kernel Approximation,<br>Naive Bayes

## Milan Straka

### November 23, 2020

---
section: SVR
# SVM For Regression

![w=25%,f=right](svr_loss.svgz)

The idea of SVM for regression is to use an $Îµ$-insensitive error function
$$ğ“›_Îµ\big(t, y(â†’x)\big) = \max\big(0, |y(â†’x) - t| - Îµ\big).$$

~~~
The primary formulation of the loss is then
$$C âˆ‘_i ğ“›_Îµ\big(t, y(â†’x)\big) + \frac{1}{2} ||â†’w||^2.$$

~~~
![w=25%,f=right](svr.svgz)

In the dual formulation, we ideally require every example to be withing $Îµ$ of
its target, but introduce two slack variables $â†’Î¾^-$, $â†’Î¾^+$ to allow outliers. We therefore
minimize the loss
$$C âˆ‘_i (Î¾_i^- + Î¾_i^+) + \tfrac{1}{2} ||â†’w||^2$$
while requiring for every example $t_i - Îµ - Î¾_i^- â‰¤ y(â†’x) â‰¤ t_i + Îµ + Î¾_i^+$ for $Î¾_i^- â‰¥ 0, Î¾_i^+ â‰¥ 0$.

---
# SVM For Regression

The Langrangian after substituting for $â†’w$, $b$, $â†’Î¾^-$ and $â†’Î¾^+$ is
$$L = âˆ‘_i (a_i^+ - a_i^-) t_i - Îµ âˆ‘_i (a_i^+ + a_i^-)
      - \frac{1}{2} âˆ‘_i âˆ‘_j (a_i^+ - a_i^-) (a_j^+ - a_j^-) K(â†’x_i, â†’x_j)$$

![w=40%,f=right](svr_example.svgz)

subject to
$$\begin{gathered}
  0 â‰¤ a_i^+, a_i^- â‰¤ C,\\
  âˆ‘_i(a_i^+ - a_i^-) = 0.
\end{gathered}$$

~~~
The prediction is then given by
$$y(â†’z) = âˆ‘_i (a_i^+ - a_j^-) K(â†’z, â†’x_i) + b.$$

---
section: KernelApproximation
# Using RBF Kernel in Parametric Methods

The RBF kernel empirically works well, but can be used only in the kernel
methods (i.e., in the dual formulation, which is a non-parametric model), which
have time complexity superlinear with the size of the training data.

~~~
Therefore, several methods have been developed to allow using an approximation
of the RBF kernel in parametric models.

---
class: dbend
# Random Fourier Features

One way to approximate RBF kernel is Monte Carlo approximation of its Fourier
transform.

~~~
The Fourier transform of a real-valued integrable function $f$ is
$$fÌ‚(w) â‰ \frac{1}{2Ï€} âˆ«_{-âˆ}^âˆ f(x) e^{-i x w} \d x,$$
where the $fÌ‚(w)$ can be considered its _frequence spectrum_.

~~~
The transformation is invertible, and we can recover the original function as
$$f(x) = âˆ«_{-âˆ}^âˆ fÌ‚(w) e^{i x w} \d w.$$

---
class: dbend
# Random Fourier Features

Now consider a shift-invariant kernel $K(â†’x, â†’y) = k(â†’x - â†’y)$. If we knew its
frequency spectrum $p$, we could write it as
$$k(â†’x - â†’y) = âˆ«_{R^D} p(â†’w) e^{i â†’w^T (â†’x - â†’y)} \d â†’w,$$
which we can rewrite using $Î¾(â†’x; â†’w) = e^{j â†’w^T â†’x}$ as
$$k(â†’x - â†’y) = âˆ«_{R^D} p(â†’w) e^{i â†’w^T (â†’x - â†’y)} \d â†’w = ğ”¼_â†’w \big[Î¾(â†’x; â†’w) Î¾(â†’y; â†’w)^*\big].$$

~~~
Therefore, $Î¾(â†’x; â†’w) Î¾(â†’y; â†’w)^*$ is the unbiased estimate of the kernel.

~~~
Because the kernel is real-valued, it is enough to consider the real part of
$Î¾$, notably $z(â†’x; â†’w, b) = \sqrt 2 \cos(â†’w^T â†’x + b)$ for $â†’w âˆ¼ p$ and $b$
sampled uniformly from $[0, 2Ï€]$, and still get that
$k(â†’x - â†’y) = ğ”¼_{â†’w,b} \big[z(â†’x; â†’w, b) z(â†’y; â†’w, b)\big]$ (by applying expectation
on $b$ using the identity $\cos(â†’w^T â†’x + b) \cos(â†’w^T â†’y + b) = \tfrac{1}{2}\cos(â†’w^T (â†’x-â†’y)) + \frac{1}{2}\cos(â†’w^T (â†’x+â†’y) + 2b)$.

---
class: dbend
# Random Fourier Features

In order to decrease the variance of the estimator, we sample $M$ values
of $â†’w$ and $b$ and approximate
$$k(â†’x - â†’y) â‰ˆ \frac{1}{M} âˆ‘_{i=1}^M z(â†’x; â†’w_i, b_i) z(â†’y; â†’w_i, b_i).$$

~~~
Lastly, we need the frequency spectum of an RBF kernel, which is
$$p(â†’w) = \sqrt{2Î³} ğ“(â†’w; 0, 1).$$

~~~
The disadvantage of this approach is that we sample completely randomly,
not taking any data into consideration.

---
# NystrÃ¶m Approximation

A different approach to approximate an RBF kernel is to use a subset
of data as basis.

~~~
Assume that we have a sample of $M$ data $â†’x_1, â€¦, â†’x_M$, denoting
$â‡‰K_{i,j} = k(â†’x_i, â†’x_j)$.

~~~
Our goal is to represent $k(â†’x, â†’y)$ as
$$k(â†’x, â†’y) = âˆ‘_{l=1}^M Ï†_l(â†’x) Ï†_l(â†’y)$$
by using linear mappings $Ï†_l(â†’y) = â‡‰V_l^T k(â†’y, â†’x_*)$ for a matrix $â‡‰V$.

~~~
If such identity should hold for all data $â†’x_1, â€¦, â†’x_M$, it must hold that
$$â‡‰K_{i,j} = âˆ‘_l (â‡‰V_l^T â‡‰K_i) â‡‰V_l^T â‡‰K_j.$$

~~~
We can rewrite the condition for all indices as $â‡‰K = â‡‰K^T â‡‰V â‡‰V^T â‡‰K$.

---
# NystrÃ¶m Approximation

If e want $â‡‰K = â‡‰K^T â‡‰V â‡‰V^T â‡‰K$ to hold, $â‡‰V$ should be $â‡‰K^{-1/2}$.

~~~
Because the kernel is a real symmetric matrix, is has an **eigenvalue decomposition**
$$â‡‰K = â‡‰U â‡‰D â‡‰U^T,$$
where $â‡‰U$ is an orthogonal matrix and $â‡‰D$ is a diagonal one.

~~~
Therefore, we can take
$$â‡‰V = â‡‰U â‡‰D^{-1/2} â‡‰U^T,$$
which fulfils the required equation. Note that $â‡‰D^{-1/2}$ can be computed
element-wise, because it is a diagonal matrix.

~~~
The overall kernel is then approximated by measuring the distances of a given
point to the chosen data subset and multiplied by $â‡‰V$. Empirically, the
approximation works usually better than random Fourier features, because
it concentrates more on the part of the space populated by the data.

---
section: TF-IDF
# Term Frequency â€“ Inverse Document Frequency

To represent a document, we might consider it a **bag of words**, and create
a feature space with a dimension for every word. We can represent a word
in a document as:

- **binary indicators**: 1/0 depending on whether a word is present in
  a document or not;
~~~
- **term frequency (TF)**: relative frequency of a term in a document;
  $$\mathit{TF}(t) = \frac{\textrm{number of occurrences of $t$ in the document}}{\textrm{number of terms in the document}}$$
~~~
- **inverse document frequency (IDF)**: we could also represent a term using
  self-information of a probability of a random document containing it (therefore,
  terms with lower document probability have higher weights);
  $$\mathit{IDF}(t) = \log \frac{\textrm{number of documents}}{\textrm{number of documents containing $t$ }\big(\textrm{optionally} + 1)}$$
~~~
- **TF-IDF**: empirically, product $\mathit{TF} â‹… \mathit{IDF}$ is a feature
  reflecting quite well how important is a word to a document in a corpus
  (used by 83\% text-based recommender systems in 2015).

---
section: NaiveBayes
# Naive Bayes Classifier

Consider a discriminative classifier modelling probabilities
$$p(C_k|â†’x) = p(C_k | x_1, x_2, â€¦, x_D).$$

~~~
We might use Bayes' theorem and rewrite it to
$$p(C_k|â†’x) = \frac{p(C_k) p(â†’x | C_k)}{p(â†’x)}.$$

~~~
The so-called **Naive Bayes** classifier assumes all $x_i$
are independent given $C_k$, so we can write
$$p(â†’x | C_k) = p(x_1 | C_k) p(x_2 | C_k, x_1) p(x_3 | C_k, x_1, x_2) â‹¯ p(x_D | C_k, x_1, â€¦)$$
as
$$p(C_k | â†’x) âˆ p(C_k) âˆ_i p(x_i | C_k).$$

---
# Naive Bayes Classifier

There are several used naive Bayes classifiers, depending on the distribution
$p(x_i | C_k)$.

### Gaussian NB

The probability $p(x_i | C_k)$ is modelled as a normal distribution
$ğ“(Î¼_{i, k}, Ïƒ_{i, k}^2)$.

~~~
The parameters $Î¼_{i,k}$ and $Ïƒ_{i,k}^2$ are estimated directly from the data.
However, the variances are usually smoothed (increased) by a given constant $Î±$
to avoid too sharp distributions.

~~~
- The default value of $Î±$ in Scikit-learn is $10^9$ times the largest variance
  of all features.

~~~
Gaussian NB is useful if we expect a continuous feature has normal distribution
for a gicen $C_k$.

---
# Naive Bayes Classifier

### Multinomial NB

The probability $p(x_i | C_k)$ is proportional to $p_{i, k}^{x_i}$, so the
$$\log p(C_k, â†’x) = \log p(C_k) + âˆ‘_i\log p_{i, k}^{x_i} = \log p(C_k) + âˆ‘_i x_i \log p_{i, k} = b + â†’x^T â†’w$$
is a linear model in the log space with $b = \log p(C_k)$ and $w_i = \log p_{i, k}$.

~~~
Denoting $n_{i, k}$ as the sum of features $x_i$ for a class $C_k$, the
probabilities $p_{i, k}$ are usually estimated as
$$p_{i, k} = \frac{n_{i, k} + Î±}{âˆ‘_j n_{j, k} + Î±D}$$
where $Î±$ is a _smoothing_ parameter accounting for terms not appearing in any
document of class $C_k$ (we can view it as a _pseudocount_ given to every
term in ever document).

---
# Naive Bayes Classifier

### Bernoulli NB

When the input features are binary, the $p(x_i | C_k)$ might also be a Bernoulli
distribution
$$p(x_i | C_k) = p_{i, k}^{x_i} â‹… (1 - p_{i, k})^{(1-x_i)},$$
and as in the Multinomial NB case, we can write
$$\log p(C_k, â†’x) = \log p(C_k) + âˆ‘\nolimits_i \big(x_i \log \tfrac{p_{i, k}}{1-p_{i,k}} + \log(1-p_{i,k})\big) = b + â†’x^T â†’w.$$
~~~
Similarly to multinomial NB, the probabilities are usually estimated as
$$p_{i, k} = \frac{\textrm{number of documents of class $k$ with nonzero feature $i$} + Î±}{\textrm{number of documents of class $k$} + 2Î±}.$$

~~~
The difference with respect to Multinomial NB is that Bernoulli NB explicitly
models also the _absence of terms_ by $(1-p_{i,k})$, while $p_{i,k}^0=1$ is used
in Multinomial NB. However, the cost is that the input features must be binary
(so for example TF-IDF cannot be used).

---
section: GenerativeAndDiscriminative
# Naive Bayes Classifier as a Generative Model

Given that a Multinomial/Bernoulli NB fits $\log p(C_k, â†’x)$ as a linear model and
a logistic regression also fits $\log p(C_k | â†’x)$ as a linear model, naive Bayes and
logistic regression form a so-called **generative-discriminative** pair, where
the naive Bayes is a **generative** model, while logistic regression is
a **discriminative** model.

---
# Generative and Discriminative Models

So far, most of our models have been **discriminative**, modeling a _conditional
distribution_ $p(â†’t | â†’x)$ (predicting some output distribution). Empirically,
such models usually perform better in classification tasks, but because they do
not estimate the probability of $â†’x$, it might be difficult for them to
recognize outliers (out-of-distribution data).

~~~
On the other hand, the **generative** models estimate a _joint distribution_
$p(â†’t, â†’x)$, often by employing Bayes' theorem and estimating
$p(â†’x | â†’t) â‹… p(â†’t)$. They therefore model the probability of the data being
generated by an outcome, and only transform it to $p(â†’t|â†’x)$ during prediction.

~~~
The term generative comes from a (theoretical) possibility of â€œgeneratingâ€
random instances (either of $(â†’x, â†’t)$ or $â†’x$ given $â†’t$). However, just
being able to evaluate $p(â†’x | â†’t)$ does not necessarily mean there must be an
efficient procedure of actually sampling (generating) $â†’x$.

~~~
In recent years, generative modeling combined with deep neural networks created
a new family of _deep generative models_ like VAE or GAN, which can in fact
efficiently generate samples from $p(â†’x)$.

---
section: MAP
# Maximum A Posteriori Estimation

We already discussed maximum likelihood estimation
$$â†’w_\mathrm{MLE} = \argmax_â†’w p(ğ•; â†’w).$$

~~~
Instead, we may want to maximize _maximum a posteriori (MAP)_ point estimate:
$$â†’w_\mathrm{MAP} = \argmax_â†’w p(â†’w; ğ•)$$

~~~
Using Bayes' theorem
$$p(â†’w; ğ•) = \frac{p(ğ•; â†’w) â‹… p(â†’w)}{p(ğ•)},$$
we get
$$â†’w_\mathrm{MAP} = \argmax_â†’w p(ğ•; â†’w) â‹… p(â†’w).$$

---
# L2 Regularization as MAP

Another way to arrive at L2 regularization is to employ the MAP estimation
and assume that the prior probabilities $p(â†’w)$ of the parameter values (our
_preference_ among the models) is $ğ“(â†’w; 0, Ïƒ^2)$.

~~~
Then
$$\begin{aligned}
â†’w_\mathrm{MAP} &= \argmax_â†’w p(ğ•; â†’w) â‹… p(â†’w) \\
                &= \argmax_â†’w âˆ\nolimits_{i=1}^N p(â†’x_i; â†’w) â‹… p(â†’w) \\
                &= \argmin_â†’w âˆ‘\nolimits_{i=1}^N -\log p(â†’x_i; â†’w) - \log p(â†’w) \\
\end{aligned}$$

~~~
By substituting the probability of the Gaussian prior, we get
$$â†’w_\mathrm{MAP} = \argmin_â†’w âˆ‘_{i=1}^N -\log p(â†’x_i; â†’w) {\color{gray} + \frac{D}{2} \log(2Ï€Ïƒ^2)} + \frac{||â†’w||^2}{2Ïƒ^2}.$$
