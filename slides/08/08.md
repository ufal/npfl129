title: NPFL129, Lecture 8
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# SVR, Kernel Approximation,<br>Naive Bayes

## Milan Straka

### November 23, 2020

---
section: SVR
# SVM For Regression

![w=25%,f=right](svr_loss.svgz)

The idea of SVM for regression is to use an $Îµ$-insensitive error function
$$ğ“›_Îµ\big(t, y(â†’x)\big) = \max\big(0, |y(â†’x) - t| - Îµ\big).$$

~~~
The primary formulation of the loss is then
$$C âˆ‘_i ğ“›_Îµ\big(t, y(â†’x_i)\big) + \frac{1}{2} ||â†’w||^2.$$

~~~
![w=25%,f=right](svr.svgz)

In the dual formulation, we ideally require every example to be withing $Îµ$ of
its target, but introduce two slack variables $â†’Î¾^-$, $â†’Î¾^+$ to allow outliers. We therefore
minimize the loss
$$C âˆ‘_i (Î¾_i^- + Î¾_i^+) + \tfrac{1}{2} ||â†’w||^2$$
while requiring for every example $t_i - Îµ - Î¾_i^- â‰¤ y(â†’x_i) â‰¤ t_i + Îµ + Î¾_i^+$ for $Î¾_i^- â‰¥ 0, Î¾_i^+ â‰¥ 0$.

---
# SVM For Regression

The Langrangian after substituting for $â†’w$, $b$, $â†’Î¾^-$ and $â†’Î¾^+$ is
$$L = âˆ‘_i (a_i^+ - a_i^-) t_i - Îµ âˆ‘_i (a_i^+ + a_i^-)
      - \frac{1}{2} âˆ‘_i âˆ‘_j (a_i^+ - a_i^-) (a_j^+ - a_j^-) K(â†’x_i, â†’x_j)$$

![w=40%,f=right](svr_example.svgz)

subject to
$$\begin{gathered}
  0 â‰¤ a_i^+, a_i^- â‰¤ C,\\
  âˆ‘_i(a_i^+ - a_i^-) = 0.
\end{gathered}$$

~~~
The prediction is then given by
$$y(â†’z) = âˆ‘_i (a_i^+ - a_i^-) K(â†’z, â†’x_i) + b.$$

---
section: KernelApprox
# Using RBF Kernel in Parametric Methods

The RBF kernel empirically works well, but can be used only in the kernel
methods (i.e., in the dual formulation, which is a non-parametric model), which
have time complexity superlinear with the size of the training data.

~~~
Therefore, several methods have been developed to allow using an approximation
of the RBF kernel in parametric models like logistic regression or MLP.

~~~
Generally, these methods define a mapping $â†’Ïˆ:â„^D â†’ â„^M$, generating $M$
features from a given input example, such that
$$K(â†’x, â†’z) â‰ˆ â†’Ïˆ(â†’x)^T â†’Ïˆ(â†’z) = âˆ‘_m â†’Ïˆ_m(â†’x)^T â†’Ïˆ_m(â†’z).$$

~~~
For a given example $â†’x$, the features $â†’Ïˆ(â†’x)$ are then used as input to
a parametric classifier (or appended to other features we construct).

~~~
The hyperparameter $M$ affects the quality of the approximation and is usually
on the order of hundreds.

---
section: RFF
class: dbend
# Random Fourier Features

One way to approximate RBF kernel is Monte Carlo approximation of its Fourier
transform.

~~~
The Fourier transform of a real-valued integrable function $f$ is
$$fÌ‚(w) â‰ \frac{1}{2Ï€} âˆ«_{-âˆ}^âˆ f(x) e^{-i x w} \d x,$$
where the $fÌ‚(w)$ can be considered its _frequency spectrum_.

~~~
The transformation is invertible, and we can recover the original function as
$$f(x) = âˆ«_{-âˆ}^âˆ fÌ‚(w) e^{i x w} \d w.$$

---
class: dbend
# Random Fourier Features

Now consider a shift-invariant kernel $K(â†’x, â†’y) = k(â†’x - â†’y)$. If we knew its
frequency spectrum $p$, we could write it as
$$k(â†’x - â†’y) = âˆ«_{R^D} p(â†’w) e^{i â†’w^T (â†’x - â†’y)} \d â†’w,$$
~~~
which we can rewrite using $Î¾(â†’x; â†’w) = e^{i â†’w^T â†’x}$ as
$$k(â†’x - â†’y) = âˆ«_{R^D} p(â†’w) e^{i â†’w^T (â†’x - â†’y)} \d â†’w = ğ”¼_{â†’w âˆ¼ p} \big[Î¾(â†’x; â†’w) Î¾(â†’y; â†’w)^*\big].$$

~~~
Therefore, $Î¾(â†’x; â†’w) Î¾(â†’y; â†’w)^*$ is an unbiased estimate of the kernel.

---
class: dbend
# Random Fourier Features

However, working with the complex numbers $Î¾(â†’x; â†’w) = e^{i â†’w^T â†’x}$.
Nevertheless, considering that the kernel and the frequency spectrum is
real-valued, it is enough just to consider the real part of $Î¾(â†’x; â†’w) Î¾(â†’y; â†’w)^*$.

~~~
Recalling Euler formula stating that $e^{iÎ¸} = \cos Î¸ + i \sin Î¸$, the real part
of the $Î¾$ product is $\cos(â†’w^T (â†’x-â†’y))$, and we would like to compute it
from $\cos(â†’w^T â†’x)$ and $\cos(â†’w^T â†’y)$, which are the real parts of
$Î¾(â†’x; â†’w)$ and $Î¾(â†’y; â†’w)$, respectively.

~~~
Remembering that $\cos(xÂ±y) = \cos x \cos y âˆ“ \sin x \sin y$, we can rewrite
$\cos x\cos y$ as
$$\cos x \cos y = \tfrac{1}{2}\big(\cos(x - y) + \cos(x + y)\big).$$

~~~
In order to get rid of the last term, we introduce a bias $b$ sampled from
uniform distribution $U[0, 2Ï€]$ and consider mappings
$$Ïˆ(â†’x; â†’w, b) â‰ \sqrt 2 \cos(â†’w^T â†’x + b).$$

---
class: dbend
# Random Fourier Features

Combining the last two equations leads to
$$\begin{aligned}
  & ğ”¼_{bâˆ¼U[0, 2Ï€]} \big[\sqrt 2 \cos(â†’w^T â†’x + b) \sqrt 2 \cos(â†’w^T â†’y + b)\big] \\
 =& ğ”¼_{bâˆ¼U[0, 2Ï€]} \big[\cos(â†’w^T â†’x - â†’w^T â†’y) + \cos(â†’w^T â†’x + â†’w^T â†’y + 2b)\big] \\
 =& \cos(â†’w^T(â†’x - â†’y)),
\end{aligned}$$
where the last equation holds because the $\cos$ integrate to zero with respect
to $b$ (actually, range $[0, Ï€]$ would be sufficient).

~~~
In order to decrease the variance of the estimator, we sample $M$ values
of $â†’w$ and $b$ and define
$$Ïˆ_i(â†’x; â†’w_i, b_i) â‰ \sqrt{2/M} \cos(â†’w_i^T â†’x + b_i).$$

---
class: dbend
# Random Fourier Features

Lastly, we need the frequency spectrum of an RBF kernel.

~~~
It can be shown that for an RBF kernel with $Î³=\tfrac{1}{2}$, the
frequency spectrum is the density function of the standard normal distribution,
$$p(â†’w) = ğ“(â†’w; 0, 1).$$

~~~
To handle different values of $Î³$, it is sufficient to suitably scale the
input features, which we can implement by scaling the sampled $â†’w$. It is
therefore straightforward to verify that using
$$â†’w âˆ¼ \sqrt{2Î³} ğ“(0, 1)$$
results in an approximation of an RBF kernel with scale parameter $Î³$.

~~~
The disadvantage of this approach is that we sample completely randomly,
not taking any data into consideration.

---
section: NystrÃ¶m
# NystrÃ¶m Approximation

A different approach to approximate an RBF kernel is to use a subset
of data as basis.

~~~
Assume that we have a sample of $M$ data $â†’x_1, â€¦, â†’x_M$, denoting
$â‡‰K_{i,j} = K(â†’x_i, â†’x_j)$.

~~~
Our goal is to represent $K(â†’x, â†’y)$ as
$$K(â†’x, â†’y) â‰ˆ âˆ‘_{m=1}^M Ïˆ_m(â†’x; â†’v_m) Ïˆ_m(â†’y; â†’v_m)$$
by using linear mappings $Ïˆ_m(â†’y; â†’v_m) = âˆ‘_i â†’v_{m,i} K(â†’y, â†’x_i) â†’v_{m,i}$. If we denote
a matrix with columns $â†’v_1, â€¦, â†’v_M$ as $â‡‰V$, we can then write
$$â†’Ïˆ(â†’y; â‡‰V) = â‡‰V^T K(â†’y, â†’x_*),$$
where $K(â†’y, â†’x_*)$ is a vector of $K(â†’y, â†’x_1), â€¦, K(â†’y, â†’x_M)$.

---
# NystrÃ¶m Approximation

In order to construct our approximation, we choose $â‡‰V$ such that
our approximation is exact for all data $â†’x_1, â€¦, â†’x_M$. Therefore,
it must hold that
$$â‡‰K_{i,j} = âˆ‘_m (â‡‰V_m^T â‡‰K_i)^T â‡‰V_m^T â‡‰K_j.$$

~~~
We can rewrite the condition for all indices as $â‡‰K = â‡‰K^T â‡‰V â‡‰V^T â‡‰K$.

~~~
Therefore, we would like $â‡‰V$ to be something like $â‡‰K^{-1/2}$.

---
# NystrÃ¶m Approximation

Because the kernel is a real symmetric matrix, is has an **eigenvalue decomposition**
$$â‡‰K = â‡‰U â‡‰D â‡‰U^T,$$
where $â‡‰U$ is an orthogonal matrix and $â‡‰D$ is a diagonal one.

~~~
Therefore, we can take $â‡‰V = â‡‰U â‡‰D^{-1/2} â‡‰U^T$, where
$$â‡‰D^{-1/2}_{i,i} = \begin{cases}
  0 &\mathrm{if}~D_{i,i} = 0,\\
  D_{i,i}^{-1/2}&\mathrm{otherwise}.
\end{cases}$$

~~~
It is then straightforward to show that the required equation holds.

~~~
The overall kernel is therefore approximated by computing the kernel values
a given point and the chosen data subset, multiplied by $â‡‰V$. Empirically, the
approximation works usually better than random Fourier features, because it
concentrates more on the part of the space populated by the data.

---
section: TF-IDF
# Term Frequency â€“ Inverse Document Frequency

To represent a document, we might consider it a **bag of words**, and create
a feature space with a dimension for every word. We can represent a word
in a document as:

- **binary indicators**: 1/0 depending on whether a word is present in
  a document or not;
~~~
- **term frequency (TF)**: relative frequency of a term in a document;
  $$\mathit{TF}(t) = \frac{\textrm{number of occurrences of $t$ in the document}}{\textrm{number of terms in the document}}$$
~~~
- **inverse document frequency (IDF)**: we could also represent a term using
  self-information of a probability of a random document containing it (therefore,
  terms with lower document probability have higher weights);
  $$\mathit{IDF}(t) = \log \frac{\textrm{number of documents}}{\textrm{number of documents containing $t$ }\big(\textrm{optionally} + 1)}$$
~~~
- **TF-IDF**: empirically, product $\mathit{TF} â‹… \mathit{IDF}$ is a feature
  reflecting quite well how important is a word to a document in a corpus
  (used by 83\% text-based recommender systems in 2015).

---
section: NaiveBayes
# Naive Bayes Classifier

Consider a discriminative classifier modelling probabilities
$$p(C_k|â†’x) = p(C_k | x_1, x_2, â€¦, x_D).$$

~~~
We might use Bayes' theorem and rewrite it to
$$p(C_k|â†’x) = \frac{p(C_k) p(â†’x | C_k)}{p(â†’x)}.$$

~~~
The so-called **Naive Bayes** classifier assumes all $x_i$
are independent given $C_k$, so we can write
$$p(â†’x | C_k) = p(x_1 | C_k) p(x_2 | C_k, x_1) p(x_3 | C_k, x_1, x_2) â‹¯ p(x_D | C_k, x_1, â€¦)$$
as
$$p(C_k | â†’x) âˆ p(C_k) âˆ_i p(x_i | C_k).$$

---
# Naive Bayes Classifier

There are several used naive Bayes classifiers, depending on the distribution
$p(x_i | C_k)$.

### Gaussian NB

The probability $p(x_i | C_k)$ is modelled as a normal distribution
$ğ“(Î¼_{i, k}, Ïƒ_{i, k}^2)$.

~~~
The parameters $Î¼_{i,k}$ and $Ïƒ_{i,k}^2$ are estimated directly from the data.
However, the variances are usually smoothed (increased) by a given constant $Î±$
to avoid too sharp distributions.

~~~
- The default value of $Î±$ in Scikit-learn is $10^{-9}$ times the largest variance
  of all features.

~~~
Gaussian NB is useful if we expect a continuous feature has normal distribution
for a given $C_k$.

---
# Naive Bayes Classifier

### Multinomial NB

When the distribution $p(x_i | C_k)$ is multinomial, it is proportional to $p_{i, k}^{x_i}$, so the
$$\log p(C_k, â†’x) = \log p(C_k) + âˆ‘_i\log p_{i, k}^{x_i} = \log p(C_k) + âˆ‘_i x_i \log p_{i, k} = b + â†’x^T â†’w$$
is a linear model in the log space with $b = \log p(C_k)$ and $w_i = \log p_{i, k}$.

~~~
Denoting $n_{i, k}$ as the sum of features $x_i$ for a class $C_k$, the
probabilities $p_{i, k}$ are usually estimated as
$$p_{i, k} = \frac{n_{i, k} + Î±}{âˆ‘_j n_{j, k} + Î±D}$$
where $Î±$ is a _smoothing_ parameter accounting for terms not appearing in any
document of class $C_k$ (we can view it as a _pseudocount_ given to every
term in ever document).

---
# Naive Bayes Classifier

### Bernoulli NB

When the input features are binary, the $p(x_i | C_k)$ might also be a Bernoulli
distribution
$$p(x_i | C_k) = p_{i, k}^{x_i} â‹… (1 - p_{i, k})^{(1-x_i)},$$
and as in the Multinomial NB case, we can write
$$\log p(C_k, â†’x) = \log p(C_k) + âˆ‘\nolimits_i \big(x_i \log \tfrac{p_{i, k}}{1-p_{i,k}} + \log(1-p_{i,k})\big) = b + â†’x^T â†’w.$$
~~~
Similarly to the Multinomial NB, the probabilities are usually estimated as
$$p_{i, k} = \frac{\textrm{number of documents of class $k$ with nonzero feature $i$} + Î±}{\textrm{number of documents of class $k$} + 2Î±}.$$

~~~
The difference with respect to Multinomial NB is that Bernoulli NB explicitly
models also the _absence of terms_ by $(1-p_{i,k})$, while $p_{i,k}^0=1$ is used
in Multinomial NB. However, the cost is that the input features must be binary
(so for example TF-IDF cannot be used).

---
section: GenerativeAndDiscriminative
# Naive Bayes Classifier as a Generative Model

Given that a Multinomial/Bernoulli NB fits $\log p(C_k, â†’x)$ as a linear model and
a logistic regression also fits $\log p(C_k | â†’x)$ as a linear model, naive Bayes and
logistic regression form a so-called **generative-discriminative** pair, where
the naive Bayes is a **generative** model, while logistic regression is
a **discriminative** model.

---
# Generative and Discriminative Models

So far, most of our models have been **discriminative**, modeling a _conditional
distribution_ $p(â†’t | â†’x)$ (predicting some output distribution). Empirically,
such models usually perform better in classification tasks, but because they do
not estimate the probability of $â†’x$, it might be difficult for them to
recognize outliers (out-of-distribution data).

~~~
On the other hand, the **generative** models estimate a _joint distribution_
$p(â†’t, â†’x)$, often by employing Bayes' theorem and estimating
$p(â†’x | â†’t) â‹… p(â†’t)$. They therefore model the probability of the data being
generated by an outcome, and only transform it to $p(â†’t|â†’x)$ during prediction.

~~~
The term generative comes from a (theoretical) possibility of â€œgeneratingâ€
random instances (either of $(â†’x, â†’t)$ or $â†’x$ given $â†’t$). However, just
being able to evaluate $p(â†’x | â†’t)$ does not necessarily mean there must be an
efficient procedure of actually sampling (generating) $â†’x$.

~~~
In recent years, generative modeling combined with deep neural networks created
a new family of _deep generative models_ like VAE or GAN, which can in fact
efficiently generate samples from $p(â†’x)$.

---
section: MAP
# Maximum A Posteriori Estimation

We already discussed maximum likelihood estimation
$$â†’w_\mathrm{MLE} = \argmax_â†’w p(ğ•; â†’w) = \argmax_â†’w p(ğ• | â†’w).$$

~~~
Instead, we may want to maximize _maximum a posteriori (MAP)_ point estimate:
$$â†’w_\mathrm{MAP} = \argmax_â†’w p(â†’w | ğ•)$$

~~~
Using Bayes' theorem
$$p(â†’w | ğ•) = \frac{p(ğ• | â†’w) p(â†’w)}{p(ğ•)},$$
we get
$$â†’w_\mathrm{MAP} = \argmax_â†’w p(ğ• | â†’w) p(â†’w).$$

---
# L2 Regularization as MAP

Another way to arrive at L2 regularization is to employ the MAP estimation
and assume that the prior probabilities $p(â†’w)$ of the parameter values (our
_preference_ among the models) is $ğ“(â†’w; 0, Ïƒ^2)$.

~~~
Then
$$\begin{aligned}
â†’w_\mathrm{MAP} &= \argmax_â†’w p(ğ• | â†’w) p(â†’w) \\
                &= \argmax_â†’w âˆ\nolimits_{i=1}^N p(â†’x_i | â†’w) p(â†’w) \\
                &= \argmin_â†’w âˆ‘\nolimits_{i=1}^N \big(-\log p(â†’x_i | â†’w) - \log p(â†’w)\big). \\
\end{aligned}$$

~~~
By substituting the probability of the Gaussian prior, we get
$$â†’w_\mathrm{MAP} = \argmin_â†’w âˆ‘_{i=1}^N -\log p(â†’x_i | â†’w) {\color{gray} - \frac{1}{2} \log(2Ï€Ïƒ^2)} + \frac{||â†’w||^2}{2Ïƒ^2}.$$
