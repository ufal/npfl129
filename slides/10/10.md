title: NPFL129, Lecture 10
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Gradient Boosting Decision Trees

## Milan Straka

### December 07, 2020

---
section: Gradient Boosing
# Gradient Boosting Decision Trees

The gradient boosting decision trees also train a collection of decision trees,
but unlike random forests, where the trees are trained independently,
in GBDT they are trained sequentially to correct the errors of the previous
trees.

~~~
![w=70%,f=right,mh=80%,v=middle](gbt_example.svgz)

If we denote $y_t$ as the prediction function of the $t^\mathrm{th}$
tree, the prediction of the whole collection is then
$$y(â†’x_i) = âˆ‘_{t=1}^T y_t(â†’x_i; â†’W_t),$$
where $â†’W_t$ is a vector of parameters (leaf values, to be concrete) of the
$t^\mathrm{th}$ tree.

---
# Gradient Boosting for Regression

Considering a regression task first, we define the overall loss as

$$ğ“›(â‡‰W) = âˆ‘_i \ell\big(t_i, y(â†’x_i; â‡‰W)\big) + âˆ‘_{t=1}^T \frac{1}{2} Î» ||â†’W_t||^2,$$
where
~~~
- $â‡‰W = (â†’W_1, â€¦, â†’W_T)$ are the parameters (leaf values) of the trees;

~~~
- $\ell\big(t_i, y(â†’x_i; â‡‰W)\big)$ is an per-example loss, $(t_i - y(â†’x_i;
  â‡‰W))^2$ for regression;
~~~
- the $Î»$ is the usual $L_2$ regularization strength.

---
# Gradient Boosting for Regression

To construct the trees sequentially, we extend the definition to

$$ğ“›^{(t)}(â‡‰W_t; â†’W_{1..(t-1)}) = âˆ‘_i \ell\big(t_i, y^{(t-1)}(â†’x_i; â‡‰W_{1..(t-1)}) + y_t(â†’x_i; â†’W_t)\big) + \frac{1}{2} Î» ||â†’W_t||^2.$$

~~~
In the following text, we drop the parameters of $y^{(t-1)}$ and $y_t$ for
brevity.

~~~
The original idea of gradient boosting was to set $y_t(â†’x_i) âˆ -\frac{âˆ‚\ell\big(t_i, y^{(t-1)}(â†’x_i)\big)}{âˆ‚y^{(t-1)}(â†’x_i)}$
as a direction minimizing the residual loss and then finding a suitable constant
$Î³_t$ which would minimize the loss $âˆ‘_i \ell\big(t_i, y^{(t-1)}(â†’x_i) + Î³_t y_t(â†’x_i)\big)$.

---
# Gradient Boosting

However, a more principled approach was later suggested. Denoting
$$g_i = \frac{âˆ‚\ell\big(t_i, y^{(t-1)}(â†’x_i)\big)}{âˆ‚y^{(t-1)}(â†’x_i)}$$
and
$$h_i = \frac{âˆ‚^2\ell\big(t_i, y^{(t-1)}(â†’x_i)\big)}{âˆ‚y^{(t-1)}(â†’x_i)^2},$$

~~~
we can expand the objective $ğ“›^{(t)}$ using a second-order approximation to
$$ğ“›^{(t)}(â‡‰W_t; â†’W_{1..(t-1)}) â‰ˆ âˆ‘_i \Big[\ell\big(t_i, y^{(t-1)}(â†’x_i)\big) + g_i y_t(â†’x_i) + \frac{1}{2} h_i y_t^2(â†’x_i)\Big] + \frac{1}{2} Î» ||â†’W_t||^2.$$

---
# Gradient Boosting

Recall that we denote the indices of instances belonging to a node $ğ“£$ as $I_ğ“£$,
and let us denote the prediction for the node $ğ“£$ as $w_ğ“£$. Then we can rewrite
$$\begin{aligned}
ğ“›^{(t)}(â‡‰W_t; â†’W_{1..(t-1)})
  &â‰ˆ âˆ‘_i \Big[g_i y_t(â†’x_i) + \frac{1}{2} h_i y_t^2(â†’x_i)\Big] + \frac{1}{2} Î» ||â†’W_t||^2 + \textrm{const} \\
  &â‰ˆ âˆ‘_ğ“£ \bigg[\Big(âˆ‘_{i âˆˆ I_ğ“£} g_i\Big) w_ğ“£ + \frac{1}{2} \Big(Î» + âˆ‘_{i âˆˆ I_ğ“£} h_i\Big) w_ğ“£^2\bigg] + \textrm{const}\\
\end{aligned}$$

~~~
By setting a derivative with respect to $w_ğ“£$ to zero, we get the optimal
weight for a node $ğ“£$:
$$w^*_ğ“£ = -\frac{âˆ‘_{i âˆˆ I_ğ“£} g_i}{Î» + âˆ‘_{i âˆˆ I_ğ“£} h_i}.$$

---
# Gradient Boosting

Substituting the optimum weights to the loss, we get
$$ğ“›^{(t)}(â‡‰W) â‰ˆ -\frac{1}{2} âˆ‘_ğ“£ \frac{\left(âˆ‘_{i âˆˆ I_ğ“£} g_i\right)^2}{Î» + âˆ‘_{i âˆˆ I_ğ“£} h_i} + \textrm{const},$$

which can be used as a splitting criterion.

~~~
![w=60%,h=center](gbt_scores.svgz)

---
# Gradient Boosting

When splitting a node, the criterions of all possible splits can be effectively
computed using the following algorithm:

![w=60%,h=center](gbt_algorithm.svgz)

---
# Gradient Boosting

Furthermore, gradient boosting trees frequently use:
- data subsampling: either bagging, or (even more commonly) utilize only
  a fraction of the original training data for training a single tree (with 0.5
  a common value),

~~~
- feature bagging;
~~~
- shrinkage: multiply each trained tree by a learning rate $Î±$, which reduces
  influence of each individual tree and leaves space for future optimization.

---
section: GB Classification
# Binary Classification with Gradient Boosting Decision Trees

To peform classification, we train the trees to perform the linear part of
a generalized linear model.

~~~
Specifically, for a binary classification, we perform prediction by
$$Ïƒ\big(y(â†’x_i)\big) = Ïƒ\left(âˆ‘_{t=1}^T y_t(â†’x_i; â†’W_t)\right),$$
~~~
and the per-example loss is defined as
$$\ell\big(t_i, y(â†’x_i)\big) = -\log \Big[Ïƒ\big(y(â†’x_i)\big)^{t_i} \big(1- Ïƒ\big(y(â†’x_i)\big)\big)^{1-t_i}\Big].$$

---
# Multiclass Classification with Gradient Boosting Decision Trees

For multiclass classification, we need to model the full categorical output
distribution. Therefore, for each â€œtimestepâ€ $t$, we train $K$ trees $â‡‰W_{t,k}$,
each predicting a single value of the linear part of a generalized linear model.

~~~
Then, we perform prediction by
$$\softmax\big(â†’y(â†’x_i)\big) = \softmax\left(âˆ‘\nolimits_{t=1}^T y_{t,1}(â†’x_i; â†’W_{t,1}), â€¦, âˆ‘\nolimits_{t=1}^T y_{t,K}(â†’x_i; â†’W_{t,K})\right),$$
~~~
and the per-example loss is defined analogously as
$$\ell\big(t_i, â†’y(â†’x_i)\big) = -\log \Big(\softmax\big(â†’y(â†’x_i)\big)_{t_i}\Big).$$

---
# Multiclass Classification with Gradient Boosting Decision Trees

![w=80%,h=center](gradient_boosting_example_1.svgz)
~~~ ~
# Multiclass Classification with Gradient Boosting Decision Trees

![w=80%,h=center](gradient_boosting_example_2.svgz)
~~~ ~
# Multiclass Classification with Gradient Boosting Decision Trees

![w=80%,h=center](gradient_boosting_example_3.svgz)

---
section: GB Demo
# Gradient Boosting Demo and Implementations

## Playground

You can explore the [Gradient Boosting Trees playground](https://ufal.mff.cuni.cz/~straka/courses/npfl129/2021/slides/10/gbt/).

## Implementations

Scikit-learn offers an implementation of gradient boosting decision trees,
`sklearn.ensemble.GradientBoostingClassifier` for classification and
`sklearn.ensemble.GradientBoostingRegressor` for regression.

There are additional efficient implementations, capable of distributed
processing of data larger than available memory:
- XGBoost,
- LightGBM,
both offering scikit-learn interface, among others.

---
section: SupervisedML
# Supervised Machine Learning

This concludes the **supervised machine learning** part of our course.

~~~
We have encountered:
- parametric models

~~~
  - generalized linear models: perceptron algorithm, linear regression, logistic regression,
    multinomial (softmax) logistic regression, Poisson regression
    - linear models, but manual feature engineering allows solving
      non-linear problems
~~~
  - multilayer perceptron: non-linear model according to Universal approximation
    theorem
~~~

- non-parametric models
  - k-nearest neighbors
~~~
  - kernelized linear regression
~~~
  - support vector machines

~~~
- decision trees
  - can be both parametric or non-parametric depending on the constraints

~~~
- generative models
  - naive Bayes
