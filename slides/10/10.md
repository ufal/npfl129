title: NPFL129, Lecture 10
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Gradient Boosting Decision Trees

## Milan Straka

### December 06, 2021

---
section: Gradient Boosting
# Gradient Boosting Decision Trees

The gradient boosting decision trees also train a collection of decision trees,
but unlike random forests, where the trees are trained independently,
in GBDT they are trained sequentially to correct the errors of the previous
trees.

~~~
![w=70%,f=right,mh=80%,v=middle](gbt_example.svgz)

If we denote $y_t$ as the prediction function of the $t^\mathrm{th}$
tree, the prediction of the whole collection is then
$$y(â†’x_i) = âˆ‘_{t=1}^T y_t(â†’x_i; â†’W_t),$$
where $â†’W_t$ is a vector of parameters (leaf values, to be concrete) of the
$t^\mathrm{th}$ tree.

---
# Gradient Boosting for Regression

Considering a regression task first, we define the overall loss as

$$ğ“›(â‡‰W) = âˆ‘_i \ell\big(t_i, y(â†’x_i; â‡‰W)\big) + âˆ‘_{t=1}^T \frac{1}{2} Î» \big\|â‡‰W_t\big\|^2,$$
where
~~~
- $â‡‰W = (â†’W_1, â€¦, â†’W_T)$ are the parameters (leaf values) of the trees;

~~~
- $\ell\big(t_i, y(â†’x_i; â‡‰W)\big)$ is an per-example loss, $(t_i - y(â†’x_i;
  â‡‰W))^2$ for regression;
~~~
- the $Î»$ is the usual $L_2$ regularization strength.

---
# Gradient Boosting for Regression

To construct the trees sequentially, we extend the definition to

$$ğ“›^{(t)}(â‡‰W_t; â†’W_{1..t-1}) = âˆ‘_i \Big[\ell\big(t_i, y^{(t-1)}(â†’x_i; â‡‰W_{1..t-1}) + y_t(â†’x_i; â†’W_t)\big)\Big] + \frac{1}{2} Î» \big\|â‡‰W_t\big\|^2.$$

~~~
In the following text, we drop the parameters of $y^{(t-1)}$ and $y_t$ for
brevity.

~~~
The original idea of gradient boosting was to set
$$y_t(â†’x_i) âˆ -\frac{âˆ‚\ell\big(t_i, y^{(t-1)}(â†’x_i)\big)}{âˆ‚y^{(t-1)}(â†’x_i)}$$
as a direction minimizing the residual loss
~~~
and then finding a suitable constant $Î³_t$, which would minimize the loss
$$âˆ‘_i \Big[\ell\big(t_i, y^{(t-1)}(â†’x_i) + Î³_t y_t(â†’x_i)\big)\Big] + \frac{1}{2} Î» \big\|â‡‰W_t\big\|^2.$$

---
section: Newtonâ€™s Method
# First-order and Second-order Methods

Until now, we used mostly SGD for finding a minimum, by performing
$$â†’w â† â†’w - Î± âˆ‡L(â†’w).$$

~~~
A disadvantage of this (so-called **first-order method**) is that we need to
specify the learning rates by ourselves, usually using quite a small one and
performing the update many times.

~~~
However, in some situations we can do better.

---
# Newtonâ€™s Root-Finding Method

Assume we got a function $f: â„ â†’ â„$ and we want to find its root. A SGD-like
algorithm would always move â€œtowardsâ€ zero by taking small steps.

~~~
![w=40%,f=right](newton_iteration.svgz)

Instead, we could consider the a linear local approximation
(i.e., consider a line â€œtouchingâ€ the function in a given point)
and perform a step so that our linear local approximation has
value 0:
$$x' â† x - \frac{f(x)}{f'(x)}.$$

~~~
## Finding Minima

The same method can be used to find minima, because a minimum
is just a root of a derivation, resulting in:
$$x' â† x - \frac{f'(x)}{f''(x)}.$$

---
# Newtonâ€™s Method

The following update is the Newtonâ€™s method of searching for extremes:
$$x' â† x - \frac{f'(x)}{f''(x)}.$$

It is a so-called **second-order** method, but it is in fact a SGD update
with learning rate $\frac{1}{f''(x)}$.

~~~
## Derivation from Taylorâ€™s Expansion

The same update can be derived also from the Taylorâ€™s expansion
$$f(x + Îµ) â‰ˆ f(x) + Îµ f'(x) + \frac{1}{2} Îµ^2 f''(x), \textcolor{gray}{+ ğ“(Îµ^3)}$$

~~~
which we can minimise for $Îµ$ by
$$0 = \frac{âˆ‚f(x + Îµ)}{âˆ‚Îµ} â‰ˆ f'(x) + Îµ f''(x),\textrm{ ~obtaining~ }x + Îµ = x - \frac{f'(x)}{f''(x)}.$$

---
class: dbend
# Training NLPs with the Newtonâ€™s Method

Note that the second-order methods (methods utilizing second derivatives) are
impractical when training MLPs (and GLMs) with many parameters.
~~~
The problem is that there are too many second derivatives â€“ if we consider
weights $â†’w âˆˆ â„^D$,
- the gradient $âˆ‡L(â†’w)$ has $D$ elements;
~~~
- however, we have a $DÃ—D$ matrix with all second derivatives, called the
  **Hessian** $H$:
  $$H_{i,j} â‰ \frac{âˆ‚^2 L(â†’w)}{âˆ‚w_i âˆ‚w_j}.$$

~~~
For completeness, the Taylor expansion than has the following form
$$f(â†’x + â†’Îµ) = f(â†’x) + â†’Îµ^T âˆ‡f(â†’x) + \frac{1}{2} â†’Îµ^T H â†’Îµ,$$
from which we obtain the following second-order method update:
$$â†’x â† â†’x - H^{-1} âˆ‡f(â†’x).$$

---
section: GB Training
# Gradient Boosting

However, a more principled approach was later suggested. Denoting
$$g_i = \frac{âˆ‚\ell\big(t_i, y^{(t-1)}(â†’x_i)\big)}{âˆ‚y^{(t-1)}(â†’x_i)}$$
and
$$h_i = \frac{âˆ‚^2\ell\big(t_i, y^{(t-1)}(â†’x_i)\big)}{âˆ‚y^{(t-1)}(â†’x_i)^2},$$

~~~
we can expand the objective $ğ“›^{(t)}$ using a second-order approximation to
$$ğ“›^{(t)}(â‡‰W_t; â†’W_{1..t-1}) â‰ˆ âˆ‘_i \Big[\ell\big(t_i, y^{(t-1)}(â†’x_i)\big) + g_i y_t(â†’x_i) + \frac{1}{2} h_i y_t^2(â†’x_i)\Big] + \frac{1}{2} Î» \big\|â‡‰W_t\big\|^2.$$

---
# Gradient Boosting

Recall that we denote the indices of instances belonging to a node $ğ“£$ as $I_ğ“£$,
and let us denote the prediction for the node $ğ“£$ as $w_ğ“£$.
~~~
Then we can rewrite

$\displaystyle\kern2em ğ“›^{(t)}(â‡‰W_t; â†’W_{1..t-1})
 â‰ˆ âˆ‘\nolimits_i \Big[g_i y_t(â†’x_i) + \frac{1}{2} h_i y_t^2(â†’x_i)\Big] + \frac{1}{2} Î» \big\|â‡‰W_t\big\|^2 + \textrm{const}$

~~~
$\displaystyle\kern2em\phantom{ğ“›^{(t)}(â‡‰W_t; â†’W_{1..t-1})}
  â‰ˆ âˆ‘_ğ“£ \bigg[\Big(âˆ‘_{i âˆˆ I_ğ“£} g_i\Big) w_ğ“£ + \frac{1}{2} \Big(Î» + âˆ‘_{i âˆˆ I_ğ“£} h_i\Big) w_ğ“£^2\bigg] + \textrm{const}.$

~~~
By setting a derivative with respect to $w_ğ“£$ to zero, we get
$$0 = \frac{âˆ‚ğ“›^{(t)}}{âˆ‚w_ğ“£} = âˆ‘\nolimits_{i âˆˆ I_ğ“£} g_i + \Big(Î» + âˆ‘\nolimits_{i âˆˆ I_ğ“£} h_i\Big) w_ğ“£.$$

~~~
Therefore, the optimal weight for a node $ğ“£$ is
$$w^*_ğ“£ = -\frac{âˆ‘_{i âˆˆ I_ğ“£} g_i}{Î» + âˆ‘_{i âˆˆ I_ğ“£} h_i}.$$

---
# Gradient Boosting

Substituting the optimum weights to the loss, we get
$$ğ“›^{(t)}(â‡‰W) â‰ˆ -\frac{1}{2} âˆ‘_ğ“£ \frac{\left(âˆ‘_{i âˆˆ I_ğ“£} g_i\right)^2}{Î» + âˆ‘_{i âˆˆ I_ğ“£} h_i} + \textrm{const},$$

which can be used as a splitting criterion.

~~~
![w=60%,h=center](gbt_scores.svgz)

---
# Gradient Boosting

When splitting a node, the criterions of all possible splits can be effectively
computed using the following algorithm:

![w=60%,h=center](gbt_algorithm.svgz)

---
# Gradient Boosting

Furthermore, gradient boosting trees frequently use:
- data subsampling: either bagging, or (even more commonly) utilize only
  a fraction of the original training data for training a single tree (with 0.5
  being a common value),

~~~
- feature subsampling;
~~~
- shrinkage: multiply each trained tree by a learning rate $Î±$, which reduces
  influence of each individual tree and leaves space for future optimization.

---
section: GB Classification
# Binary Classification with Gradient Boosting Decision Trees

To perform classification, we train the trees to perform the linear part of
a generalized linear model.

~~~
Specifically, for a binary classification, we perform prediction by
$$Ïƒ\big(y(â†’x_i)\big) = Ïƒ\left(âˆ‘_{t=1}^T y_t(â†’x_i; â†’W_t)\right),$$
~~~
and the per-example loss is defined as
$$\ell\big(t_i, y(â†’x_i)\big) = -\log \Big[Ïƒ\big(y(â†’x_i)\big)^{t_i} \big(1- Ïƒ\big(y(â†’x_i)\big)\big)^{1-t_i}\Big].$$

---
# Multiclass Classification with Gradient Boosting Decision Trees

For multiclass classification, we need to model the full categorical output
distribution. Therefore, for each â€œtimestepâ€ $t$, we train $K$ trees $â‡‰W_{t,k}$,
each predicting a single value of the linear part of a generalized linear model.

~~~
Then, we perform prediction by
$$\softmax\big(â†’y(â†’x_i)\big) = \softmax\left(âˆ‘\nolimits_{t=1}^T y_{t,1}(â†’x_i; â†’W_{t,1}), â€¦, âˆ‘\nolimits_{t=1}^T y_{t,K}(â†’x_i; â†’W_{t,K})\right),$$
~~~
and the per-example loss is defined analogously as
$$\ell\big(t_i, â†’y(â†’x_i)\big) = -\log \Big(\softmax\big(â†’y(â†’x_i)\big)_{t_i}\Big).$$

---
# Multiclass Classification with Gradient Boosting Decision Trees

![w=80%,h=center](gradient_boosting_example_1.svgz)
~~~ ~
# Multiclass Classification with Gradient Boosting Decision Trees

![w=80%,h=center](gradient_boosting_example_2.svgz)
~~~ ~
# Multiclass Classification with Gradient Boosting Decision Trees

![w=80%,h=center](gradient_boosting_example_3.svgz)

---
section: GB Demo
# Gradient Boosting Demo and Implementations

## Playground

You can explore the [Gradient Boosting Trees playground](https://ufal.mff.cuni.cz/~straka/courses/npfl129/2122/slides/10/gbt/playground.html)
and [Gradient Boosting Trees explained](https://ufal.mff.cuni.cz/~straka/courses/npfl129/2122/slides/10/gbt/explained.html).

~~~
## Implementations

Scikit-learn offers an implementation of gradient boosting decision trees,
`sklearn.ensemble.GradientBoostingClassifier` for classification and
`sklearn.ensemble.GradientBoostingRegressor` for regression.

~~~
- Furthermore, the `sklearn.ensemble.HistGradientBoosting{Classifier/Regressor}`
  provides histogram-based splitting (which can be much faster for larger
  datasets â€“ tens of thousands of examples and more) and efficient categorical
  feature splitting.

~~~
There are additional efficient implementations, capable of distributed
processing of data larger than available memory (both offering also scikit-learn
interface):
- XGBoost,
- LightGBM (which is the inspiration for the `HistGradientBoosting*` implementation).

---
section: SupervisedML
# Supervised Machine Learning

This concludes the **supervised machine learning** part of our course.

~~~
We have encountered:
- parametric models

~~~
  - generalized linear models: perceptron algorithm, linear regression, logistic regression,
    multinomial (softmax) logistic regression, Poisson regression
    - linear models, but manual feature engineering allows solving
      non-linear problems
~~~
  - multilayer perceptron: non-linear, perfect approximator â€“ Universal approx. theorem
~~~

- non-parametric models
  - k-nearest neighbors
~~~
  - kernelized linear regression
~~~
  - support vector machines

~~~
- decision trees
  - can be both parametric or non-parametric depending on the constraints

~~~
- generative models
  - naive Bayes

---
# Supervised Machine Learning

When training a model for a new dataset, I start by evaluating two models:

~~~
- an **MLP** with one/two hidden layers
~~~
  - works best for high-dimensional data (images, speech, text), where an
    individual single dimension (feature) does not convey much meaning;
~~~
- **gradient boosted decision tree**
~~~
  - works best for lower-dimensional data, where the input features have
    interpretation on their own.

~~~
However, if the amount of training examples is not too large (tens/hundreds of thousands
at most) and there are lot of features, **SVM** with **RBF** kernel might offer
best performance.

~~~
Furthermore, if there are only a few training examples with a lot of features,
**naive Bayes** might also work well.

~~~
Finally, if your goal is to reach the highest possible performance and you have
a lot of resources, definitely use **ensembling**.
