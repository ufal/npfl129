title: NPFL129, Lecture 10
class: title, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Gradient Boosted Decision Trees

## JindÅ™ich LibovickÃ½ <small>(reusing materials by Milan Straka)</small>

### December 05, 2023

---
class: middle
# Today's Lecture Objectives

After this lecture you should be able to

- Explain second-order optimization methods

- Implement gradient boosted decision trees for regression and classification

- Decide what supervised machine learning approach is suitable for particular
  problems

---
section: Gradient Boosting
# Gradient Boosting Decision Trees

The gradient boosting decision trees also train a collection of decision trees,
but unlike random forests, where the trees are trained independently,
in GBDT they are trained sequentially to correct the errors of the previous
trees.

~~~
![w=70%,f=right,mh=80%,v=middle](gbt_example.svgz)

If we denote $y_t$ as the prediction function of the $t^\mathrm{th}$
tree, the prediction of the whole collection is then
$$y(â†’x_i) = âˆ‘_{t=1}^T y_t(â†’x_i; â†’w_t),$$
where $â†’w_t$ is a vector of parameters (leaf values, to be concrete) of the
$t^\mathrm{th}$ tree.

---
# Gradient Boosting for Regression

Considering a regression task first, we define the overall loss as

$$E(â†’w) = âˆ‘_i \ell\big(t_i, y(â†’x_i; â†’w)\big) + âˆ‘_{t=1}^T \frac{1}{2} Î» \big\|â†’w_t\big\|^2,$$
where
~~~
- $â†’w = (â†’w_1, â€¦, â†’w_T)$ are the parameters (leaf values) of the trees;

~~~
- $\ell\big(t_i, y(â†’x_i; â†’w)\big)$ is an per-example loss, $(t_i - y(â†’x_i;
  â†’w))^2$ for regression;
~~~
- the $Î»$ is the usual $L^2$-regularization strength.

---
# Gradient Boosting for Regression

To construct the trees sequentially, we extend the definition to

$$E^{(t)}(â†’w_t; â†’w_{1..t-1}) = âˆ‘_i \Big[\ell\big(t_i, y^{(t-1)}(â†’x_i; â†’w_{1..t-1}) + y_t(â†’x_i; â†’w_t)\big)\Big] + \frac{1}{2} Î» \big\|â†’w_t\big\|^2.$$

~~~
In the following text, we drop the parameters of $y^{(t-1)}$ and $y_t$ for
brevity.

~~~
The original idea of gradient boosting was to set
$$y_t(â†’x_i) â† -\frac{âˆ‚\ell\big(t_i, y^{(t-1)}(â†’x_i)\big)}{âˆ‚y^{(t-1)}(â†’x_i)}
  \textcolor{gray}{=-\frac{âˆ‚\ell\big(t_i, y\big)}{âˆ‚y}\bigg|_{y=y^{(t-1)}(â†’x_i)}}$$
as a direction minimizing the residual loss
~~~
and then finding a suitable constant $Î³_t$, which would minimize the loss
$$âˆ‘_i \Big[\ell\big(t_i, y^{(t-1)}(â†’x_i) + Î³_t y_t(â†’x_i)\big)\Big] + \frac{1}{2} Î» \big\|â†’w_t\big\|^2.$$

---
section: Newtonâ€™s Method
# First-order and Second-order Methods

Until now, we used mostly SGD for finding a minimum, by performing
$$â†’w â† â†’w - Î± âˆ‡E(â†’w).$$

~~~
A disadvantage of this (so-called **first-order method**) is that we need to
specify the learning rates by ourselves, usually using quite a small one, and
perform the update many times.

~~~
However, in some situations, we can do better.

---
# Newtonâ€™s Root-Finding Method

Assume we have a function $f: â„ â†’ â„$ and we want to find its root. An SGD-like
algorithm would always move â€œtowardsâ€ zero by taking small steps.

~~~
![w=40%,f=right](newton_iteration.svgz)

Instead, we could consider the linear local approximation
(i.e., consider a line â€œtouchingâ€ the function in a given point)
and perform a step so that our linear local approximation has
a value 0:
$$x' â† x - \frac{f(x)}{f'(x)}.$$

~~~
## Finding Minima

The same method can be used to find minima, because a minimum
is just a root of a derivative, resulting in:
$$x' â† x - \frac{f'(x)}{f''(x)}.$$

---
# Newtonâ€™s Method

The following update is the Newtonâ€™s method of searching for extremes:
$x' â† x - \frac{f'(x)}{f''(x)}.$

It is a so-called **second-order** method, but it is just an SGD update with
a learning rate $\frac{1}{f''(x)}$.

~~~
## Derivation from Taylorâ€™s Expansion

The same update can be derived also from the Taylorâ€™s expansion <small>($x$ is a fixed point and $\epsilon$ is not the variable that moves)</small>
$$f(x + Îµ) â‰ˆ f(x) + Îµ f'(x) + \frac{1}{2} Îµ^2 f''(x) \textcolor{gray}{+ ğ“(Îµ^3)},$$

~~~
which we can minimize for $Îµ$ by <small>(i.e., the minimum of the approximation)</small>
$$0 = \frac{âˆ‚f(x + Îµ)}{âˆ‚Îµ} â‰ˆ f'(x) + Îµ f''(x),\textrm{ ~obtaining~ }x + Îµ = x - \frac{f'(x)}{f''(x)}.$$

---
class: dbend
# Training MLPs with the Newtonâ€™s Method

Note that the second-order methods (methods utilizing second derivatives) are
impractical when training MLPs (and GLMs) with many parameters.
~~~
The problem is that there are too many second derivatives â€“ if we consider
weights $â†’w âˆˆ â„^D$,
- the gradient $âˆ‡E(â†’w)$ has $D$ elements;
~~~
- however, we have a $DÃ—D$ matrix with all second derivatives, called the
  **Hessian** $H$:
  $$H_{i,j} â‰ \frac{âˆ‚^2 E(â†’w)}{âˆ‚w_i âˆ‚w_j}.$$

~~~
For completeness, the Taylor expansion of a multivariate function then has the following form:
$$f(â†’x + â†’Îµ) = f(â†’x) + â†’Îµ^T âˆ‡f(â†’x) + \frac{1}{2} â†’Îµ^T â‡‰H â†’Îµ,$$
from which we obtain the following second-order method update:
$$â†’x â† â†’x - â‡‰H^{-1} âˆ‡f(â†’x).$$

---
section: GB Training
# Gradient Boosting

Returning to the gradient boosting decision trees, instead of using
a first-order method, it was later suggested that a second-order method could be
used.
~~~
Denoting
$$g_i = \frac{âˆ‚\ell\big(t_i, y^{(t-1)}(â†’x_i)\big)}{âˆ‚y^{(t-1)}(â†’x_i)}
  \textcolor{gray}{=\frac{âˆ‚\ell\big(t_i, y\big)}{âˆ‚y}\bigg|_{y=y^{(t-1)}(â†’x_i)}}$$
and
$$h_i = \frac{âˆ‚^2\ell\big(t_i, y^{(t-1)}(â†’x_i)\big)}{âˆ‚y^{(t-1)}(â†’x_i)^2}
  \textcolor{gray}{=\frac{âˆ‚^2\ell\big(t_i, y\big)}{âˆ‚y^2}\bigg|_{y=y^{(t-1)}(â†’x_i)}},$$

~~~
we can expand the objective $E^{(t)}$ using a second-order approximation to
$$E^{(t)}(â†’w_t; â†’w_{1..t-1}) â‰ˆ âˆ‘_i \Big[\ell\big(t_i, y^{(t-1)}(â†’x_i)\big) + g_i y_t(â†’x_i) + \frac{1}{2} h_i y_t^2(â†’x_i)\Big] + \frac{1}{2} Î» \big\|â†’w_t\big\|^2.$$

~~~
<small>(y_t(\boldsymbol x_i) became the $\epsilon$ that moves from the already given $x$)</small>

---
# Gradient Boosting

Recall that we denote the indices of instances belonging to a leaf $ğ“£$ as $I_ğ“£$,
and let us denote the prediction for the leaf $ğ“£$ as $w_ğ“£$.
~~~
Then we can rewrite

$\displaystyle\kern2em E^{(t)}(â†’w_t; â†’w_{1..t-1})
 â‰ˆ âˆ‘\nolimits_i \Big[g_i y_t(â†’x_i) + \frac{1}{2} h_i y_t^2(â†’x_i)\Big] + \frac{1}{2} Î» \big\|â†’w_t\big\|^2 + \textrm{const}$

~~~
$\displaystyle\kern2em\phantom{E^{(t)}(â†’w_t; â†’w_{1..t-1})}
  â‰ˆ âˆ‘_ğ“£ \bigg[\Big(âˆ‘_{i âˆˆ I_ğ“£} g_i\Big) w_ğ“£ + \frac{1}{2} \Big(Î» + âˆ‘_{i âˆˆ I_ğ“£} h_i\Big) w_ğ“£^2\bigg] + \textrm{const}.$

~~~
By setting a derivative with respect to $w_ğ“£$ to zero, we get
$$0 = \frac{âˆ‚E^{(t)}}{âˆ‚w_ğ“£} = âˆ‘\nolimits_{i âˆˆ I_ğ“£} g_i + \Big(Î» + âˆ‘\nolimits_{i âˆˆ I_ğ“£} h_i\Big) w_ğ“£.$$

~~~
Therefore, the optimal weight for a node $ğ“£$ is
$$w^*_ğ“£ = -\frac{âˆ‘_{i âˆˆ I_ğ“£} g_i}{Î» + âˆ‘_{i âˆˆ I_ğ“£} h_i}.$$

---
# Gradient Boosting

Substituting the optimum weights to the loss, we get
$$E^{(t)}(â†’w^*) â‰ˆ -\frac{1}{2} âˆ‘_ğ“£ \frac{\left(âˆ‘_{i âˆˆ I_ğ“£} g_i\right)^2}{Î» + âˆ‘_{i âˆˆ I_ğ“£} h_i} + \textrm{const},$$

which can be used as a _splitting criterion._

~~~
![w=60%,h=center](gbt_scores.svgz)

---
# Gradient Boosting

When splitting a node, the criteria of all possible splits can be effectively
computed using the following algorithm:

![w=60%,h=center](gbt_algorithm.svgz)

---
# Gradient Boosting

Furthermore, gradient boosting trees frequently use:
- data subsampling: either bagging or (even more commonly) only a fraction
  of the original training data is utilized for training of a single tree (with
  0.5 being a common value),

~~~
- feature subsampling;
~~~
- shrinkage: multiply each trained tree by a learning rate $Î±$, which reduces
  the influence of each individual tree and leaves space for future optimization.

---
section: GB Classification
# Binary Classification with Gradient Boosting Decision Trees

To perform classification, we train the trees to perform the linear part of
a generalized linear model.

~~~
Specifically, for a binary classification, we perform prediction by
$$Ïƒ\big(y(â†’x_i)\big) = Ïƒ\left(âˆ‘_{t=1}^T y_t(â†’x_i; â†’w_t)\right),$$
~~~
and the per-example loss is defined as
$$\ell\big(t_i, y(â†’x_i)\big) = -\log \Big[Ïƒ\big(y(â†’x_i)\big)^{t_i} \big(1- Ïƒ\big(y(â†’x_i)\big)\big)^{1-t_i}\Big].$$

---
# Multiclass Classification with Gradient Boosting Decision Trees

For multiclass classification, we need to model the full categorical output
distribution. Therefore, for each â€œtimestepâ€ $t$, we train $K$ trees $â†’w_{t,k}$,
each predicting a single value of the linear part of a generalized linear model.

~~~
Then, we perform prediction by
$$\softmax\big(â†’y(â†’x_i)\big) = \softmax\left(âˆ‘\nolimits_{t=1}^T y_{t,1}(â†’x_i; â†’w_{t,1}), â€¦, âˆ‘\nolimits_{t=1}^T y_{t,K}(â†’x_i; â†’w_{t,K})\right),$$
~~~
and the per-example loss for all $K$ trees is defined analogously as
$$\ell\big(t_i, â†’y(â†’x_i)\big) = -\log \Big(\softmax\big(â†’y(â†’x_i)\big)_{t_i}\Big),$$

~~~
so that for a tree $k$ at time $t$,
$$\frac{âˆ‚\ell\big(t_i, â†’y^{(t-1)}(â†’x_i)\big)}{âˆ‚â†’y^{(t-1)}(â†’x_i)_k} = \Big(\softmax\big(â†’y^{(t-1)}(â†’x_i)\big) - â†’1_{t_i}\Big)_k.$$

---
# Multiclass Classification with Gradient Boosting Decision Trees

![w=80%,h=center](gradient_boosting_example_1.svgz)
~~~ ~
# Multiclass Classification with Gradient Boosting Decision Trees

![w=80%,h=center](gradient_boosting_example_2.svgz)
~~~ ~
# Multiclass Classification with Gradient Boosting Decision Trees

![w=80%,h=center](gradient_boosting_example_3.svgz)

---
section: GB Demo
# Gradient Boosting Demo and Implementations

## Playground

You can explore the [Gradient Boosting Trees playground](https://ufal.mff.cuni.cz/~straka/courses/npfl129/2122/slides/10/gbt/playground.html)
and [Gradient Boosting Trees explained](https://ufal.mff.cuni.cz/~straka/courses/npfl129/2122/slides/10/gbt/explained.html).

~~~
## Implementations

Scikit-learn offers an implementation of gradient boosting decision trees,
`sklearn.ensemble.GradientBoostingClassifier` for classification and
`sklearn.ensemble.GradientBoostingRegressor` for regression.

~~~
- Furthermore, `sklearn.ensemble.HistGradientBoosting{Classifier/Regressor}`
  provide histogram-based splitting (which can be much faster for larger
  datasets â€“ tens of thousands of examples and more) and efficient categorical
  feature splitting.

~~~
There are additional efficient implementations, capable of distributed
processing of data larger than available memory (both offering also scikit-learn
interface):
- XGBoost,
- LightGBM (which is the inspiration for the `HistGradientBoosting*` implementation).

---
section: SupervisedML
# Supervised Machine Learning

This concludes the **supervised machine learning** part of our course.

~~~
We have encountered:
- parametric models

~~~
  - generalized linear models: perceptron algorithm, linear regression, logistic regression,
    multinomial (softmax) logistic regression
    - linear models, but manual feature engineering allows solving
      nonlinear problems
~~~
  - multilayer perceptron: nonlinear, perfect approximator â€“ Universal approx. theorem
~~~

- nonparametric models
  - k-nearest neighbors
~~~
  - support vector machines <small>not in this course, but in the state exam</small>

~~~
- decision trees
  - can be both parametric or nonparametric depending on the constraints

~~~
- generative models
  - naive Bayes

---
# Supervised Machine Learning

When training a model for a new dataset, I start by evaluating two models:

~~~
- an **MLP** with one/two hidden layers
~~~
  - works best for high-dimensional data (images, speech, text), where an
    individual single dimension (feature) does not convey much meaning;
~~~
- **gradient boosted decision tree**
~~~
  - works best for lower-dimensional data (â€œtabular dataâ€), where the input
    features have interpretations on their own.

~~~
If there are only a few training examples with a lot of features,
**naive Bayes** might also work well.

~~~
Finally, if your goal is to reach the highest possible performance and you have
a lot of resources, definitely use **ensembling**.

---
class: middle
# Today's Lecture Objectives

After this lecture you should be able to

- Explain second-order optimization methods

- Implement gradient boosted decision trees for regression and classification

- Decide what supervised machine learning approach is suitable for particular
  problems

