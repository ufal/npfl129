title: NPFL129, Lecture 10
class: title, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Gradient Boosted Decision Trees

## Jindřich Libovický <small>(reusing materials by Milan Straka)</small>

### December 01, 2024

---
class: middle
# Today's Lecture Objectives

After this lecture you should be able to

- Explain second-order optimization methods

- Tell how Gradient-Boosted Decision Trees differ from Random Forests

- Implement gradient-boosted decision trees for regression and classification

- Decide which supervised machine learning approach is suitable for particular
  problems

---
section: Gradient Boosting
class: section
# Gradient Boosting
---

# Gradient-Boosted Decision Trees

The gradient-boosted decision trees also train a collection of decision trees,
but unlike random forests, where the trees are trained independently,
in GBDT they are trained sequentially to correct the errors of the previous
trees.

~~~
![w=70%,f=right,mh=80%,v=middle](gbt_example.svgz)

If we denote $y_t$ as the prediction function of the $t^\mathrm{th}$
tree, the prediction of the whole collection is then
$$y(→x_i) = ∑_{t=1}^T y_t(→x_i; →w_t),$$
where $→w_t$ is a vector of parameters (leaf values, to be concrete) of the
$t^\mathrm{th}$ tree.

---
# Gradient Boosting for Regression

Considering a regression task first, we define the overall loss as

$$E(→w) = ∑_i \ell\big(t_i, y(→x_i; →w)\big) + ∑_{t=1}^T \frac{1}{2} λ \big\|→w_t\big\|^2,$$
where
~~~
- $→w = (→w_1, …, →w_T)$ are the parameters (leaf values) of the trees;

~~~
- $\ell\big(t_i, y(→x_i; →w)\big)$ is a per-example loss, $(t_i - y(→x_i;
  →w))^2$ for regression;
~~~
- $λ$ is the usual $L^2$-regularization strength.

---
# Gradient Boosting for Regression

To construct the trees sequentially, we extend the definition to

$$E^{(t)}(→w_t; →w_{1..t-1}) = ∑_i \Big[\ell\big(t_i, y^{(t-1)}(→x_i; →w_{1..t-1}) + y_t(→x_i; →w_t)\big)\Big] + \frac{1}{2} λ \big\|→w_t\big\|^2.$$

~~~
In the following text, we drop the parameters of $y^{(t-1)}$ and $y_t$ for
brevity.

~~~
The original idea of gradient boosting was to set
$$y_t(→x_i) ← -\frac{∂\ell\big(t_i, y^{(t-1)}(→x_i)\big)}{∂y^{(t-1)}(→x_i)}
  \textcolor{gray}{=-\frac{∂\ell\big(t_i, y\big)}{∂y}\bigg|_{y=y^{(t-1)}(→x_i)}}$$
as a direction minimizing the residual loss
~~~
and then to find a suitable constant $γ_t$, which would minimize the loss
$$∑_i \Big[\ell\big(t_i, y^{(t-1)}(→x_i) + γ_t y_t(→x_i)\big)\Big] + \frac{1}{2} λ \big\|→w_t\big\|^2.$$

---
section: Newton’s Method
class: section
# Newton’s Root-Finding Method
---

# First-order and Second-order Methods

Until now, we mostly used SGD for finding a minimum, by performing
$$→w ← →w - α ∇E(→w).$$

~~~
A disadvantage of this (so-called **first-order method**) is that we need to
specify the learning rates by ourselves, usually using quite a small one, and
perform the update many times.

~~~
However, in some situations, we can do better.

---
# Newton’s Root-Finding Method

Assume we have a function $f: ℝ → ℝ$, and we want to find its root. An SGD-like
algorithm would always move “towards” zero by taking small steps.

~~~
![w=40%,f=right](newton_iteration.svgz)

Instead, we could consider the linear local approximation
(i.e., consider a line “touching” the function at a given point),
and perform a step so that our linear local approximation has
the value 0:
$$x' ← x - \frac{f(x)}{f'(x)}.$$

~~~
## Finding Minima

The same method can be used to find minima, because a minimum
is just a root of the derivative, resulting in:
$$x' ← x - \frac{f'(x)}{f''(x)}.$$

---
# Newton’s Method

The following update is the Newton’s method of searching for extremes:
$x' ← x - \frac{f'(x)}{f''(x)}.$

It is a so-called **second-order** method, but it is just an SGD update with
the learning rate $\frac{1}{f''(x)}$.

~~~
## Derivation from Taylor’s Expansion

The same update can be also derived from the Taylor’s expansion <small>($x$ is a fixed point and $\epsilon$ is now the variable that moves)</small>
$$f(x + \varepsilon) ≈ f(x) + \varepsilon f'(x) + \frac{1}{2} \varepsilon^2 f''(x) \textcolor{gray}{+ 𝓞(\varepsilon^3)},$$

~~~
which we can minimize for $\varepsilon$ by <small>(i.e., the minimum of the approximation)</small>
$$0 = \frac{∂f(x + \varepsilon)}{∂\varepsilon} ≈ f'(x) + \varepsilon f''(x),\textrm{ ~obtaining~ }x + \varepsilon = x - \frac{f'(x)}{f''(x)}.$$

---
class: dbend
# Training MLPs with the Newton’s Method

Note that the second-order methods (methods utilizing second derivatives) are
impractical when training MLPs (and GLMs) with many parameters.
~~~
The problem is that there are too many second derivatives – if we consider
weights $→w ∈ ℝ^D$,
- the gradient $∇E(→w)$ has $D$ elements;
~~~
- however, we have a $D×D$ matrix with all the second derivatives, called the
  **Hessian** $H$:
  $$H_{i,j} ≝ \frac{∂^2 E(→w)}{∂w_i ∂w_j}.$$

~~~
For completeness, the Taylor expansion of a multivariate function then has the following form:
$$f(→x + →\varepsilon) = f(→x) + →\varepsilon^T ∇f(→x) + \frac{1}{2} →\varepsilon^T ⇉H →\varepsilon,$$
from which we obtain the following second-order method update:
$$→x ← →x - ⇉H^{-1} ∇f(→x).$$

---
section: GB Training
class: section
# Gradient Boosting: Training
---

# Gradient Boosting

Returning to the gradient-boosted decision trees, instead of using
a first-order method, it was later suggested that a second-order method could be
used.
~~~
Denoting
$$g_i = \frac{∂\ell\big(t_i, y^{(t-1)}(→x_i)\big)}{∂y^{(t-1)}(→x_i)}
  \textcolor{gray}{=\frac{∂\ell\big(t_i, y\big)}{∂y}\bigg|_{y=y^{(t-1)}(→x_i)}}$$
and
$$h_i = \frac{∂^2\ell\big(t_i, y^{(t-1)}(→x_i)\big)}{∂y^{(t-1)}(→x_i)^2}
  \textcolor{gray}{=\frac{∂^2\ell\big(t_i, y\big)}{∂y^2}\bigg|_{y=y^{(t-1)}(→x_i)}},$$

~~~
we can expand the objective $E^{(t)}$ using a second-order approximation to
$$E^{(t)}(→w_t; →w_{1..t-1}) ≈ ∑_i \Big[\ell\big(t_i, y^{(t-1)}(→x_i)\big) + g_i y_t(→x_i) + \frac{1}{2} h_i y_t^2(→x_i)\Big] + \frac{1}{2} λ \big\|→w_t\big\|^2.$$

---
# Gradient Boosting

Recall that we denote the indices of instances belonging to a leaf $𝓣$ as $I_𝓣$,
and let us denote the prediction for the leaf $𝓣$ as $w_𝓣$.
~~~
Then we can rewrite

$\displaystyle\kern2em E^{(t)}(→w_t; →w_{1..t-1})
 ≈ ∑\nolimits_i \Big[g_i y_t(→x_i) + \frac{1}{2} h_i y_t^2(→x_i)\Big] + \frac{1}{2} λ \big\|→w_t\big\|^2 + \textrm{const}$

~~~
$\displaystyle\kern2em\phantom{E^{(t)}(→w_t; →w_{1..t-1})}
  ≈ ∑_𝓣 \bigg[\Big(∑_{i ∈ I_𝓣} g_i\Big) w_𝓣 + \frac{1}{2} \Big(λ + ∑_{i ∈ I_𝓣} h_i\Big) w_𝓣^2\bigg] + \textrm{const}.$

~~~
By setting a derivative with respect to $w_𝓣$ to zero, we get
$$0 = \frac{∂E^{(t)}}{∂w_𝓣} = ∑\nolimits_{i ∈ I_𝓣} g_i + \Big(λ + ∑\nolimits_{i ∈ I_𝓣} h_i\Big) w_𝓣.$$

~~~
Therefore, the optimal weight for a node $𝓣$ is
$$w^*_𝓣 = -\frac{∑_{i ∈ I_𝓣} g_i}{λ + ∑_{i ∈ I_𝓣} h_i}.$$

---
# Gradient Boosting

Substituting the optimum weights to the loss, we get
$$E^{(t)}(→w^*) ≈ -\frac{1}{2} ∑_𝓣 \frac{\left(∑_{i ∈ I_𝓣} g_i\right)^2}{λ + ∑_{i ∈ I_𝓣} h_i} + \textrm{const},$$

which can be used as a _splitting criterion._

~~~
![w=60%,h=center](gbt_scores.svgz)

---
# Gradient Boosting

When splitting a node, the criteria of all possible splits can be effectively
computed using the following algorithm:

![w=60%,h=center](gbt_algorithm.svgz)

---
# Gradient Boosting

Furthermore, gradient-boosted trees frequently use:
- data subsampling: either bagging or (even more commonly) only a fraction
  of the original training data is utilized for training of a single tree (with
  0.5 being a common value),

~~~
- feature subsampling;
~~~
- shrinkage: multiply each trained tree by a learning rate $α$, which reduces
  the influence of each individual tree and leaves space for future optimization.

---
section: GB Classification
class: section
# Gradient Boosting: Classification
---

# Binary Classification with Gradient-Boosted Decision Trees

To perform classification, we train the trees to perform the linear part of
a generalized linear model.

~~~
Specifically, for a binary classification, we perform prediction by
$$σ\big(y(→x_i)\big) = σ\left(∑_{t=1}^T y_t(→x_i; →w_t)\right),$$
~~~
and the per-example loss is defined as
$$\ell\big(t_i, y(→x_i)\big) = -\log \Big[σ\big(y(→x_i)\big)^{t_i} \big(1- σ\big(y(→x_i)\big)\big)^{1-t_i}\Big].$$

---
# Multiclass Classification with Gradient-Boosted Decision Trees

For multiclass classification, we need to model the full categorical output
distribution. Therefore, for each “timestep” $t$, we train $K$ trees $→w_{t,k}$,
each predicting a single value of the linear part of a generalized linear model.

~~~
Then, we perform prediction by
$$\softmax\big(→y(→x_i)\big) = \softmax\left(∑\nolimits_{t=1}^T y_{t,1}(→x_i; →w_{t,1}), …, ∑\nolimits_{t=1}^T y_{t,K}(→x_i; →w_{t,K})\right),$$
~~~
and the per-example loss for all $K$ trees is defined analogously as
$$\ell\big(t_i, →y(→x_i)\big) = -\log \Big(\softmax\big(→y(→x_i)\big)_{t_i}\Big),$$

~~~
so that for a tree $k$ at time $t$,
$$\frac{∂\ell\big(t_i, →y^{(t-1)}(→x_i)\big)}{∂→y^{(t-1)}(→x_i)_k} = \Big(\softmax\big(→y^{(t-1)}(→x_i)\big) - →1_{t_i}\Big)_k.$$

---
# Multiclass Classification with Gradient-Boosted Decision Trees

![w=80%,h=center](gradient_boosting_example_1.svgz)
~~~ ~
# Multiclass Classification with Gradient-Boosted Decision Trees

![w=80%,h=center](gradient_boosting_example_2.svgz)
~~~ ~
# Multiclass Classification with Gradient-Boosted Decision Trees

![w=80%,h=center](gradient_boosting_example_3.svgz)

# Gradient Boosting Demo and Implementations

## Playground

You can explore the [Gradient Boosting Trees playground](https://ufal.mff.cuni.cz/~straka/courses/npfl129/2122/slides/10/gbt/playground.html)
and [Gradient Boosting Trees explained](https://ufal.mff.cuni.cz/~straka/courses/npfl129/2122/slides/10/gbt/explained.html).

~~~
## Implementations

Scikit-learn offers an implementation of gradient boosting decision trees,
`sklearn.ensemble.GradientBoostingClassifier` for classification and
`sklearn.ensemble.GradientBoostingRegressor` for regression.

~~~
- Furthermore, `sklearn.ensemble.HistGradientBoosting{Classifier/Regressor}`
  provide histogram-based splitting (which can be much faster for larger
  datasets – tens of thousands of examples and more) and efficient categorical
  feature splitting.

~~~
There are additional efficient implementations, capable of distributed
processing of data larger than available memory (both offering also scikit-learn
interface):
- XGBoost,
- LightGBM (which is the inspiration for the `HistGradientBoosting*` implementation).

---
section: Supervised ML
class: section
# Supervised Machine Learning
---

# Supervised Machine Learning

This concludes the **supervised machine learning** part of our course.

~~~
We have encountered:
- parametric models

~~~
  - generalized linear models: perceptron algorithm, linear regression, logistic regression,
    multinomial (softmax) logistic regression
    - linear models, but manual feature engineering allows solving
      nonlinear problems
~~~
  - multilayer perceptron: nonlinear, perfect approximator – Universal approx. theorem
~~~

- nonparametric models
  - k-nearest neighbors
~~~
  - support vector machines <small>(will be briefly covered in the practicals, in the state exam)</small>

~~~
- decision trees
  - can be both parametric or nonparametric depending on the constraints

~~~
- generative models
  - Naive Bayes

---
# Supervised Machine Learning

When training a model for a new dataset, a good start is evaluating two models:

~~~
- an **MLP** with one/two hidden layers
~~~
  - works best for high-dimensional data (images, speech, text), where an
    individual single dimension (feature) does not convey much meaning; use pre-trained representation if possible;
~~~
- **gradient boosted decision tree**
~~~
  - works best for lower-dimensional data (“tabular data”), where the input
    features have interpretations on their own.

~~~
If there are only a few training examples with a lot of features,
**Naive Bayes** might also work well.

~~~
Finally, if your goal is to reach the highest possible performance and you have
a lot of resources, definitely use **ensembling**.

---
class: summary
# Today's Lecture Objectives

After this lecture you should be able to

- Explain second-order optimization methods

- Tell how Gradient-Boosted Decision Trees differ from Random Forest

- Implement gradient-boosted decision trees for regression and classification

- Decide which supervised machine learning approach is suitable for particular
  problems

