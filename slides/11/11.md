title: NPFL129, Lecture 11
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# PCA, K-Means

## Milan Straka

### December 12, 2022

---
# Unsupervised Machine Learning

![w=50%,h=center](unsupervised_learning.png)

---
section: LinearAlgebra
# Linear Algebra Refresh â€“ Eigenvalues and Eigenvectors

Let $â‡‰A âˆˆ â„‚^{NÃ—N}$ be an $NÃ—N$ matrix.

~~~
- A vector $â†’v âˆˆ â„‚^N$ is a (right) **eigenvector**, if there exists an
  **eigenvalue** $Î» âˆˆ â„‚$,
  such that
  $$â‡‰A â†’v = Î» â†’v.$$

~~~
- If $â‡‰A âˆˆ â„^{NÃ—N}$ is a real symmetric matrix, then it has $N$ real
  eigenvalues and $N$ real eigenvectors, which can be chosen to be _orthonormal_,
  and we can express $â‡‰A$ using the **eigenvalue decomposition**
  $$â‡‰A = â‡‰V â‡‰Î› â‡‰V^T,$$
  where:
~~~
  - $â‡‰V$ is a matrix, whose columns are the eigenvectors $â†’v_1, â†’v_2, â€¦, â†’v_N$;
~~~
  - $â‡‰Î›$ is a diagonal matrix with the eigenvalues $Î»_1, Î»_2, â€¦, Î»_N$ on the diagonal.

---
# Linear Algebra Refresh â€“ Positive Definiteness

Let $â‡‰A âˆˆ â„^{NÃ—N}$ be a real symmetric matrix. Then
if for all $â†’x â‰  â†’0$:
~~~
- $â†’x^T â‡‰A â†’x > 0$, the matrix is called **positive definite**.

~~~
  Note that this condition is equivalent to all eigenvalues being **positive**.

~~~
- $â†’x^T â‡‰A â†’x â‰¥ 0$, the matrix is called **positive semi-definite**.

  This condition is equivalent to all eigenvalues being **nonnegative**.

~~~
- $â†’x^T â‡‰A â†’x < 0$, the matrix is called **negative definite**.

  This condition is equivalent to all eigenvalues being **negative**.

~~~
- $â†’x^T â‡‰A â†’x â‰¤ 0$, the matrix is called **negative semi-definite**.

  This condition is equivalent to all eigenvalues being **nonpositive**.

~~~
Note that we can compute a â€œsquare rootâ€ of a positive (semi-)definite matrix,
because if $â‡‰A = â‡‰V â‡‰Î› â‡‰V^T$, then for $â‡‰Î›^{1/2} â‡‰V^T$ we get
$$(â‡‰Î›^{1/2} â‡‰V^T)^T â‡‰Î›^{1/2} â‡‰V^T = â‡‰V â‡‰Î›^{1/2} â‡‰Î›^{1/2} â‡‰V^T = â‡‰V â‡‰Î› â‡‰V^T = â‡‰A.$$

---
section: PCA
# Principal Component Analysis

The **principal component analysis**, **PCA**, is a linear transformation used for
- dimensionality reduction,
- feature extraction,
- whitening,
- data visualization.

~~~
To motivate the dimensionality reduction, consider a dataset consisting
of a randomly translated and rotated image.

![w=100%,h=center](manifold_example.png)

~~~
Every member of the dataset can be described just by three quantities â€“
horizontal and vertical offsets and a rotation. We usually say that the
_data lie on a manifold of dimension three_.

---
# Principal Component Analysis

We start by defining the PCA in two ways.

~~~
![w=40%,f=right](pca_variance.svgz)

## Maximum Variance Formulation

Given data $â†’x_1, â€¦, â†’x_N$ with $â†’x_i âˆˆ â„^D$, the goal is to project
them to a space with dimensionality $M < D$, so that the variance
of their projection is maximal.

~~~
We start by considering a projection to one-dimensional space. Such a projection
is defined by a vector $â†’u_1$, and because only the direction of $â†’u_1$ matters,
we assume that $â†’u_1^T â†’u_1 = 1$.

~~~
The projection of $â†’x_i$ to $â†’u_1$ is given by $(â†’u_1^T â†’x_i) â†’u_1$, because the
vectors $â†’u_1$ and $â†’x_i - (â†’u_1^T â†’x_i) â†’u_1$ are orthogonal:

$$â†’u_1^T \big(â†’x_i - (â†’u_1^T â†’x_i) â†’u_1\big) = â†’u_1^T â†’x_i - (â†’u_1^T â†’x_i) â†’u_1^T â†’u_1 = 0.$$

---
# Principal Component Analysis

We therefore use $â†’u_1^T â†’x_i$ as the projection of $â†’x_i$. If we define
$â†’xÌ„ = âˆ‘_i â†’x_i / N$, the mean of the projected data is $â†’u_1^T â†’xÌ„$, and the
variance is given by
$$\frac{1}{N} âˆ‘_{i=1}^N \big(â†’u_1^T â†’x_i - â†’u_1^T â†’xÌ„\big)^2 = â†’u_1^T â‡‰S â†’u_1,$$
where $â‡‰S$ is the data covariance matrix defined as
$$â‡‰S = \frac{1}{N} âˆ‘_{i=1}^N \big(â†’x_i - â†’xÌ„\big)\big(â†’x_i - â†’xÌ„)^T.$$

~~~
We can write the data covariance matrix in matrix form as
$â‡‰S = \frac{1}{N} \big(â‡‰X - â†’xÌ„\big)^T \big(â‡‰X - â†’xÌ„\big)$.

If the original data is centered (it has zero mean), then
$â‡‰S = \frac{1}{N} â‡‰X^T â‡‰X$, which we have already encountered.

---
# Principal Component Analysis

To maximize $â†’u_1^T â‡‰S â†’u_1$, we need to include the constraint $â†’u_1^T â†’u_1$ by
introducing a Lagrange multiplier $Î»_1$ for the constraint $â†’u_1^T â†’u_1 - 1 = 0$
and then maximizing the Lagrangian
$$ğ“› = â†’u_1^T â‡‰S â†’u_1 - Î»_1\big(â†’u_1^T â†’u_1 - 1\big).$$

~~~
By computing a derivative with respect to $â†’u_1$, we get
$$â‡‰S â†’u_1 = Î»_1 â†’u_1.$$

~~~
Therefore, $â†’u_1$ must be an eigenvector of $â‡‰S$ corresponding to eigenvalue
$Î»_1$.

~~~
Because the value to maximize $â†’u_1^T â‡‰S â†’u_1$ is then $â†’u_1^T Î»_1 â†’u_1 = Î»_1 â†’u_1^T â†’u_1 = Î»_1$,
the maximum is attained for eigenvector $â†’u_1$ corresponding to the largest
eigenvalue $Î»_1$.

~~~
The eigenvector $â†’u_1$ is known as the **first principal component**.

~~~
For a given $M$, the principal components are eigenvectors corresponding
to $M$ largest eigenvalues, and maximize the variance of the projected data.

---
# Principal Component Analysis

## Minimum Error Formulation

Assume $â†’u_1, â€¦, â†’u_D$ is some orthonormal set of vectors, therefore,
$â†’u_i^T â†’u_j = \big[i = j\big] = Î´_{i,j}$.

~~~
Every $â†’x_i$ can be then expressed using this basis as
$$â†’x_i = âˆ‘_j \big(â†’x_i^T â†’u_j) â†’u_j,$$
using a similar argument as the one we used to derive the orthogonal projection.

~~~
Because we want to eventually represent the data using $M$ dimensions, we
approximate the data by the first $M$ basis vectors:
$$â†’xÌƒ_i = âˆ‘_{j=1}^M z_{i,j} â†’u_j + âˆ‘_{j=M+1}^D b_j â†’u_j.$$

---
# Principal Component Analysis

We now choose the vectors $â†’u_j$, coordinates $z_{i,j}$ and biases $b_j$ to
minimize the approximation error, which we measure as a loss
$$L = \frac{1}{N} âˆ‘\nolimits_{i=1}^N \|â†’x_i - â†’xÌƒ_i\|^2.$$

~~~
To minimize the error, we compute the derivative of $L$ with respect to
$z_{i,j}$ and $b_j$, and utilizing the orthogonality, we obtain
$$z_{i,j} = â†’x_i^T â†’u_j,~~~~b_j = â†’xÌ„^T â†’u_j.$$

~~~
Therefore, we can rewrite the loss as
$$L = \frac{1}{N} âˆ‘_{i=1}^N âˆ‘_{j=M+1}^D (â†’x_i^T â†’u_j - â†’xÌ„^T â†’u_j)^2 = âˆ‘_{j=M+1}^D â†’u_j^T â‡‰S â†’u_j.$$

~~~
Analogously, we can minimize $L$ by choosing the eigenvectors of $D-M$
smallest eigenvalues.

---
# PCA Applications â€“ Data Compression

We can represent the data $â†’x_i$ by the approximations $â†’xÌƒ_i$
$$ â†’xÌƒ_i = âˆ‘_{j=1}^M \big(â†’x_i^T â†’u_j\big) â†’u_j + âˆ‘_{j=M+1}^D \big(â†’xÌ„^T â†’u_j\big) â†’u_j
        = â†’xÌ„ + âˆ‘_{j=1}^M \big(â†’x_i^T â†’u_j - â†’xÌ„^T â†’u_j\big)â†’u_j.$$

![w=100%,h=center](pca_threes_reconstruction.png)

---
# PCA Applications â€“ Data Compression

![w=80%,h=center](pca_threes_eigenvalues.svgz)

![w=80%,h=center](pca_threes_eigenvectors.png)

---
section: Whitening
# PCA Applications â€“ Whitening aka Sphering

The PCA formula allows us to perform **whitening** aka **sphering**, which is
a linear transformation of the given data so that the resulting dataset
has zero mean and an identity covariance matrix.

~~~
Notably, if $â‡‰U$ are the eigenvectors of $â‡‰S$ and $â‡‰Î›$ is the diagonal matrix of
the corresponding eigenvalues (i.e., $â‡‰S â‡‰U = â‡‰U â‡‰Î›$), we can define the
transformed data as
$$â†’z_i â‰ â‡‰Î›^{-1/2} â‡‰U^T (â†’x_i - â†’xÌ„).$$

~~~
Then, the mean of $â†’z_i$ is zero and the covariance is given by
$$\begin{aligned}
\frac{1}{N} âˆ‘_i â†’z_i â†’z_i^T
  &= \frac{1}{N} âˆ‘_i â‡‰Î›^{-1/2} â‡‰U^T (â†’x_i - â†’xÌ„) (â†’x_i - â†’xÌ„)^T â‡‰U â‡‰Î›^{-1/2} \\
  &= â‡‰Î›^{-1/2} â‡‰U^T â‡‰S â‡‰U â‡‰Î›^{-1/2} = â‡‰Î›^{-1/2} â‡‰Î› â‡‰Î›^{-1/2} = â‡‰I.
\end{aligned}$$

---
# PCA Applications â€“ Whitening or Sphering

![w=100%,v=middle](pca_whitening.svgz)

---
section: PCA
# PCA versus Supervised ML

Note that PCA does not have access to supervised labels, so it may not
give a solution favorable for further classification.

![w=63%,h=center](pca_unsupervised_vs_supervised.svgz)

---
# Principal Component Analysis and MLPs

![w=40%,h=center](pca_ae.svgz)

It can be proven that if we construct an MLP _autoencoder_,
which is a model trying to reconstruct input as close as possible,
then even if the hidden layer uses nonlinear activation, the solution
to an MSE loss is a projection onto the $M$-dimensional subspace defined
by the first $M$ principal components (but is not necessary orthonormal
or orthogonal).

---
# Principal Component Analysis and MLPs

However, nonlinear PCA can be achieved, if both the _encoder_ and the _decoder_
are nonlinear.

![w=70%,h=center](pca_ae_nonlinear.svgz)

---
section: PCA-SVD
# Computing PCA

There are two frequently used algorithms for performing PCA.

~~~
If we want to compute all (or many) principal components, we can directly
compute the eigenvectors and the eigenvalues of the covariance matrix.

~~~
We can even avoid computing the covariance matrix. If we instead compute the
singular value decomposition of $(â‡‰X - â†’xÌ„) = â‡‰U â‡‰D â‡‰V^T$, it holds that
$$\big(â‡‰X - â†’xÌ„\big)^T \big(â‡‰X - â†’xÌ„\big) = â‡‰V â‡‰D â‡‰U^T â‡‰U â‡‰D â‡‰V^T = â‡‰V â‡‰D^2 â‡‰V^T.$$

~~~
Therefore, 
$$\big(â‡‰X - â†’xÌ„\big)^T \big(â‡‰X - â†’xÌ„\big) â‡‰V = â‡‰V â‡‰D^2,$$
which means that $â‡‰V$ are the eigenvectors of $(â‡‰X-â†’xÌ„)^T (â‡‰X-â†’xÌ„)$ and therefore
of the data covariance matrix $â‡‰S$. The eigenvalues of $â‡‰S$ are the squares of
the singular values of $(â‡‰X-â†’xÌ„)$ divided by $N$.

---
section: PowerIteration
style: ul {margin-bottom: 0}
# Computing PCA â€” The Power Iteration Algorithm

If we want only the first (or several first) principal components, we might use
the **power iteration algorithm**.

~~~
The power iteration algorithm can be used to find a **dominant** eigenvalue (an
eigenvalue with an absolute value strictly larger than absolute values of all other
eigenvalues) and the corresponding eigenvector (it is used for example to
compute PageRank). It works as follows:

<div class="algorithm">

**Input**: Real symmetric matrix $â‡‰A$ with a dominant eigenvalue.<br>
**Output**: The dominant eigenvalue $Î»$ and the corresponding eigenvector $â†’v$, with
probability close to 1.

- Initialize $v$ randomly (for example each component from $U[-1,1]$).
~~~
- Repeat until convergence (or for a fixed number of iterations):
  - $â†’v â† â‡‰Aâ†’v$
  - $Î» â† \|â†’v\|$
  - $â†’v â† â†’v / Î»$
</div>

~~~
If the algorithm converges, then $â†’v = â‡‰Aâ†’v / Î»$, so $â†’v$ is an eigenvector with
eigenvalue $Î»$.

---
class: dbend
# Computing PCA â€” The Power Iteration Algorithm

In order to analyze the convergence, let $(Î»_1, Î»_2, Î»_3, â€¦)$ be the eigenvalues
of $â‡‰A$, in the descending order of absolute values, so $|Î»_1| > |Î»_2| â‰¥ |Î»_3| â‰¥ â€¦$,
where the strict equality is the consequence of the dominant eigenvalue
assumption.

~~~
If we express the vector $â†’v$ in the basis of the eigenvectors as
$(a_1, a_2, a_3, â€¦)$, then $â‡‰Aâ†’v / Î»_1$ is in the basis of the eigenvectors:
$$\frac{â‡‰Aâ†’v}{Î»_1}
  = \left(\frac{Î»_1}{Î»_1}a_1, \frac{Î»_2}{Î»_1}a_2, \frac{Î»_3}{Î»_1}a_3, â€¦\right)
  = \left(               a_1, \frac{Î»_2}{Î»_1}a_2, \frac{Î»_3}{Î»_1}a_3, â€¦\right).$$

~~~
Therefore, all but the first coordinates decreased by at least a factor of
$|Î»_2/Î»_1|$.

~~~
If the initial $â†’v$ had a nonzero first coordinate $a_1$ (which has probability
very close to 1), then repeated multiplication with $â‡‰A$ converges to the
eigenvector corresponding to $Î»_1$.

---
# Computing PCA â€” The Power Iteration Algorithm

After we get the largest eigenvalue $Î»_1$ and its eigenvector $â†’v_1$, we can modify the
matrix $â‡‰A$ to â€œremove the eigenvalue $Î»_1$â€.
~~~
Consider $â‡‰A - Î»_1 â†’v_1 â†’v_1^T$:
~~~
- multiplying it by $â†’v_1$ returns zero:
  $$\big(â‡‰A - Î»_1 â†’v_1 â†’v_1^T\big) â†’v_1 = Î»_1 â†’v_1 - Î»_1 â†’v_1 \underbrace{â†’v_1^T â†’v_1}_{1} = 0,$$
~~~
- multiplying it by other eigenvectors $â†’v_i$ gives the same result as
  multiplying $â‡‰A$:
  $$\big(â‡‰A - Î»_1 â†’v_1 â†’v_1^T\big) â†’v_i = â‡‰A â†’v_i - Î»_1 â†’v_1 \underbrace{â†’v_1^T â†’v_i}_{0} = â‡‰A â†’v_i.$$

~~~
Therefore, $â‡‰A - Î»_1 â†’v_1 â†’v_1^T$ has the same set of eigenvectors and
eigenvalues, except for $â†’v_1$, which now has eigenvalue 0.

---
style: ul {margin-bottom: 0}
# Computing PCA â€” The Power Iteration Algorithm

We are now ready to formulate the complete algorithm for computing the PCA.

<div class="algorithm">

**Input**: Matrix $â‡‰X$, desired number of dimensions $M$.

- Compute the mean $â†’Î¼$ of the examples (the rows of $â‡‰X$).

~~~
- Compute the covariance matrix $S â† \frac{1}{N} \big(â‡‰X - â†’Î¼\big)^T\big(â‡‰X - â†’Î¼\big)$.

~~~
- for $i$ in $\{1, 2, â€¦, M\}$:
  - Initialize $â†’v_i$ randomly.
  - Repeat until convergence (or for a fixed number of iterations):
    - $â†’v_i â† â‡‰Sâ†’v_i$
    - $Î»_i â† \|â†’v_i\|$
    - $â†’v_i â† â†’v_i / Î»_i$
~~~
  - $â‡‰S â† â‡‰S - Î»_i â†’v_i â†’v_i^T$
~~~
- Return $â‡‰X â‡‰V$, where the columns of $â‡‰V$ are $â†’v_1, â†’v_2, â€¦, â†’v_M$.
</div>

---
section: Clustering
# Clustering

Clustering is an unsupervised machine learning technique, which given input
data tries to divide them into some number of groups, or **clusters**.

~~~
The number of clusters might be given in advance, or we should infer it.

~~~
When clustering documents, we usually normalize TF-IDF so that each feature
vector has length 1 (i.e., L2 normalization), because then
$$1 - \operatorname{cosine~similarity}(â†’x, â†’y) = \frac{1}{2} \|â†’x - â†’y\|^2.$$

---
section: KMeans
# K-Means Clustering

Let $â†’x_1, â†’x_2, â€¦, â†’x_N$ be a collection of $N$ input examples, each being
a $D$-dimensional vector $â†’x_i âˆˆ â„^D$. Let $K$, the number of target clusters,
be given.

~~~
Let $z_{i, k} âˆˆ \{0, 1\}$ be binary indicator variables describing whether an input
example $â†’x_i$ is assigned to cluster $k$, and let each cluster be specified by
a point $â†’Î¼_1, â€¦, â†’Î¼_K$, usually called the cluster **center**.

~~~
Our objective function $J$, which we aim to minimize, is
$$J = âˆ‘_{i=1}^N âˆ‘_{k=1}^K z_{i, k} \|â†’x_i - â†’Î¼_k\|^2.$$

---
# K-Means Clustering

<div class="algorithm">

**Input**: Input points $â†’x_1, â€¦, â†’x_N$, number of clusters $K$.

- Initialize $â†’Î¼_1, â€¦, â†’Î¼_K$ as $K$ random input points.

~~~
- Repeat until convergence (or until patience runs out):
~~~
  - Compute the best possible $z_{i, k}$. It is easy to see that the smallest $J$
    is achieved by
    $$z_{i,k} = \begin{cases} 1 & \textrm{~~if~}k = \argmin\nolimits_j \|â†’x_i - â†’Î¼_j\|^2, \\
                              0 & \textrm{~~otherwise}.\end{cases}$$

~~~
  - Compute the best possible $â†’Î¼_k = \argmin\nolimits_{â†’Î¼} âˆ‘_i z_{i,k} \|â†’x_i-â†’Î¼\|^2$.
~~~
   By computing a derivative with respect to $â†’Î¼$, we get
   $$â†’Î¼_k = \frac{âˆ‘_i z_{i,k} â†’x_i}{âˆ‘_i z_{i,k}}.$$
</div>

---
# K-Means Clustering

![w=55%,h=center](kmeans_example.svgz)

---
# K-Means Clustering

![w=58%,f=right](kmeans_convergence.svgz)

It is easy to see that:
- updating the cluster assignment $z_{i, k}$ decreases the loss $J$ or keeps it the same;
~~~
- updating the cluster centers again decreases the loss $J$ or keeps it the
  same.

~~~
K-Means clustering therefore converges to a local optimum. However, it
is quite sensitive to the starting initialization:
~~~
- It is common practice to run K-Means algorithm multiple times with different
  initialization and use the result with the lowest $J$ (scikit-learn uses
  `n_init=10` by default).
~~~
- Instead of using random initialization, `k-means++` initialization scheme might
  be used, where the first cluster center is chosen randomly and others are
  chosen proportionally to the square of their distance to the nearest cluster
  center. It can be proven that with this initialization, the solution
  has $ğ“(\log K)$ approximation ratio in expectation.

---
# K-Means Clustering

![w=75%,h=center](kmeans_color_reduction.svgz)

---
# Gaussian Mixture vs K-Means

It could be useful to consider that different clusters might have different
radii or even be ellipsoidal.

![w=100%,h=center](mog_kmeans_comparison.svgz)
