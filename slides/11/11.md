title: NPFL129, Lecture 11
class: title, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }
# SVD, PCA, K-Means

## Jindřich Libovický <small>(reusing some materials by Milan Straka)</small>

### December 9, 2024

---
class: middle
# Today's Lecture Objectives

After this lecture you should be able to

- Theoretically explain Singular Value Decomposition (SVD), prove it exists and explain what the Eckart-Young theorem says

- Theoretically explain Principal Component Analysis (PCA) and say how it explains the variance in the data based on SVD

- Use SVD or PCA for dimensionality reduction and data visualization

- Implement the $k$-means algorithm and use it for clustering


---
# Unsupervised Machine Learning

![w=50%,h=center](unsupervised_learning.png)

---
section: Linear Algebra
class: section
# Recap of Linear Algebra

---
# Matrix Decompositions

$$→X = →A→B$$

- $→B$ tells us how to construct $→X$ using columns of $→A$.

- $→A$ tells us how to construct $→X$ using rows of $→B$.

~~~

<br />

$→X$ could be our (training) data matrix. If we managed to get decomposition
with orthogonal columns/rows, it would tell us something like the (statistically)
independent parts the dataset consists of.

* Rows are data points.

* Columns are features.

---
# Linear Algebra Refresh – Eigenvalues and Eigenvectors

Let $⇉A ∈ ℂ^{N×N}$ be an $N×N$ matrix.

~~~
- A vector $→v ∈ ℂ^N$ is a (right) **eigenvector**, if there exists an
  **eigenvalue** $λ ∈ ℂ$,
  such that
  $$⇉A →v = λ →v.$$

~~~
- If $⇉A ∈ ℝ^{N×N}$ is a real symmetric matrix, then it has $N$ real
  eigenvalues and $N$ real eigenvectors, which can be chosen to be _orthonormal_.

~~~
### Quick (almost) proof of orthogonality
$→A→v_ 1 = \lambda_1 →v_1$, we transpose both sides and get $(→A→v_1)^T = \lambda_1 →v_1^T$.

Multiply by $→v_2$ from the right, we get $→v_1^T →A^T →v_2 = \lambda_1 →v_1^T →v_2$, but $→A$ is symmetric and $→A^T = →A$, so

$→v_1^T (→A →v_2) = \lambda_2 →v_1^T →v_2 \Rightarrow \lambda_2 →v_1^T →v_2 = \lambda_1 →v_1^T →v_2$, resulting in

$$(\lambda_2 - \lambda_1) (→v_1^T →v_2) = 0.$$

---
# Linear Algebra Refresh – Eigenvalue Decomposition
  We can express a real symmetric matrix $⇉A$ using the **eigenvalue decomposition**
  $$⇉A = ⇉V ⇉Λ ⇉V^T,$$
  where:
~~~
  - $⇉V$ is a matrix whose columns are the orthonormal eigenvectors $→v_1, →v_2, …, →v_N$;
~~~
  - $⇉Λ$ is a diagonal matrix with the eigenvalues $λ_1, λ_2, …, λ_N$ on the diagonal.

~~~
### Quick derivation
$$→A→V = →\Lambda →V = →V →\Lambda$$

$$→A→V→V^T = →V →\Lambda→V^T $$

Because $→V$ is orthonormal, $→V^T = →V^{-1}$, so $→V→V^T = \mathbb{1}$, and $→A = →V →\Lambda →V^T$.

---
section: SVD
class: section
# Singular Value Decomposition

---
# Singular Value Decomposition

Every (even a rectangular) matrix $→X$ of dimensions $m \times n$ and rank $r$ can be factorized as

$$ →X = →U →\Sigma →V^T. $$

* $→U$ is an $m \times m$ orthonormal matrix.

* $→\Sigma$ is an $m \times n$ diagonal matrix with non-negative values, so-called singular values, chosen to be in decreasing order.

* $→V$ is an $n \times n$ orthonormal matrix.

$$
→X →V = →U →\Sigma

\quad\quad\Rightarrow\quad\quad

→X →v_k = \sigma_k →u_k \quad \forall k = 1, \ldots, r
$$

$$ →X \left[\begin{array}{c} \vdots \\ →v_1 \cdots →v_r \cdots →v_n \\ \vdots \end{array}\right] = \left[\begin{array}{c} \vdots \\ →u_1 \cdots →u_r \cdots →u_m \\ \vdots \end{array}\right] \left[\begin{array}{ccc|c}\sigma_1 \\ & \ddots & & →0 \\ & & \sigma_r \\ \hline & →0 & & 0 \end{array}\right]$$

---
# SVD: Proof

Assuming SVD exists, we can write $→U$ and $→V$ as the eigenvector decomposition of row and column similarities:

$$
→X →X^T = (→U →\Sigma →V^T) (→U →\Sigma →V^T)^T =
→U →\Sigma (→V^T →V) →\Sigma^T →U^T = →U →\Sigma →\Sigma^T →U^T
$$

$$
→X^T →X = (→U →\Sigma →V^T)^T (→U →\Sigma →V^T) =
→V →\Sigma (→U^T →U) →\Sigma^T →V^T = →V →\Sigma^T →\Sigma →V^T
$$

~~~
Let's take $→V = [→v_1, \ldots, →v_r]$ orthonormal eigenvectors of $→X^T →X$ and set $\sigma_k = \sqrt{\lambda_k}$, then, $→X^T →X →v_k = \sigma_k^2 →v_k$. ($\lambda_k \geq 0$ because $→X →X^T$ is positive semi-definite.)

~~~
The decomposition says that $→X →v_k = \sigma_k →u_k \Rightarrow →u_k = \frac{→X →v_k}{\sigma_k}$. To make it work, we need to show that $→u_k$ is indeed an eigenvector of $→X →X^T$ with the same eigenvalue $\lambda_k$.

~~~
$$
→X →X^T →u_k =

→X →X^T \underbrace{\left( \frac{→X →v_k}{\sigma_k} \right)}_{\text{def. of $→u_k$}} =

→X \underbrace{\left( \frac{→X^T →X →v_k}{\sigma_k} \right)}_{→v_k \text{ is eigenvector of } →X^T →X} =

→X \frac{\sigma_k^2 →v_k}{\sigma_k} =

\sigma_k^2 \underbrace{\left( \frac{→X →v_k}{\sigma_k} \right)}_{\text{def. of $→u_k$}} =

\sigma_k^2 →u_k
$$


---
# Interpretation of SVD

* Vectors of $→U$ are the components the rows consist of, vectors of $→V$ are the same for the columns.

~~~
* It defines a decomposition of $→X$ (with rank $r$) as a sum of rank 1 matrices of dimension $m \times n$

$$ →X = \sigma_1 →u_1 →v_1^T + \sigma_2 →u_2 →v_2^T + \ldots + \sigma_r →u_r →v_r^T. $$

~~~
* Reduced version of SVD: We can throw away $\sigma_k$ for $k > r$ and use smaller $→U$ and $→V$.

~~~
* $\sigma$ are in decreasing order $\Rightarrow$ we can approximate $→X$ by taking $k < \min(m, n)$:

$$\tilde{→X} = \sum_{i=1}^k \sigma_i u_i v_i^T.$$

Eckart-Young theorem: This is the best rank $k$ approximation w.r.t. the Frobenius norm (we flatten the matrix to a vector and do $L^2$ norm).

---
# Eckart-Young Theorem

$→X \in \mathbb{R}^{n \times m}$ and $→X_k = \sigma_1 →u_1 →v_1^T + \ldots +
\sigma_k →u_k →v_k^T$ its approximation using SVD. For each $→B \in
\mathbb{R}^{n \times m}$ of rank $k$,

$$
|| →X - →X_k ||_F \leq || →X - →B ||_F.
$$

## Argument why it is a good idea

* $|| →X ||_F = \sqrt{\sum_{i}^n\sum_{j}^m x_{ij}^2} = \sqrt{\operatorname{trace}(→X^T →X)}$ (trace is the sum of the diagonal)

* Multiplying $→A$ by an orthonormal matrix $→U$ does not change the norm of $→A$:

$$
||→U →A ||_F^2 =
\operatorname{trace}((→U→A)^T →U→A) =
\operatorname{trace}(→A^T \underbrace{→U^T →U}_{→I} →A) =
\operatorname{trace}(→A^T →A) = ||→A||_F^2.
$$

* The Frobenius norm of $→X = →U →\Sigma →V^T$ is the $L^2$ norm of the diagonal of $→\Sigma$.

* The best strategy to keep the most of the norm is removing the smallest values.

---
# Image Compression using SVD

| 400 components | 200 components | 100 components | 50 components | 10 components |
| :--: | :--: | :--: | :--: | :--: |
| ![w=100%](castle-svd-400.png) | ![w=100%](castle-svd-200.png) | ![w=100%](castle-svd-100.png) | ![w=100%](castle-svd-050.png) | ![w=100%](castle-svd-010.png) |
| actually bigger | 1.2× smaller | 2.4× smaller | 4.8× smaller | 24× smaller |


---
class: middle
# SVD in Recommender Systems

* We have a matrix showing which users like which content on a streaming
  platform.

* A user can be represented by a vector of content they liked, content can be
  represented by a vector of users that liked it.

* Such a matrix is **huge** and **noisy**: SVD can be used to reduce the noise
  (throw away small singular values).

* Low-dimensional representation of users and content in terms “eigenusers” and
  “eigencontent”: can be used for **similarity search**.

* In practice, slightly modified versions of SVD are used.

---
section: PCA
class: section
#  Principal Component Analysis

---
# From SVD to Principal Component Analysis

So far, SVD had a geometric interpretation; let's add a statistical interpretation.

When we apply SVD on mean-centered data $→X - \bar{→x}$, singular values get a new interpretation: **components explaining variability in the data**.

$$
||→X - \bar{→x}||_F^2 =
\operatorname{trace}\underbrace{\left((→X - \bar{→x})^T(→X - \bar{→x})\right)}_{N\operatorname{Cov}(→X)}  =
N \sum_i^D \operatorname{Var}( →X_{:,i} )
$$

Approximating the matrix in terms of the Frobenius norm means keeping the most variance of the data. The components are ordered by how much variablity in the data they capture.

~~~
<br />

Let $→S = \frac{1}{N}(→X - \bar{→x})^T(→X - \bar{→x})$, then PCA of $→X$ are the eigenvectors of $→S$, i.e., the $→V$ matrix of the singular value decomposition of $→X - \bar{→x}$.

Note that the $\frac{1}{N}$ term scales down the eigenvalues compared to SVD, but keeps the eigenvectors unchanged.

---
class: middle
# Plot Example

![w=50%,f=left](gaussian_pca.svgz)

Principle components in data sampled from a 2D Gaussian. $→U →\Sigma →V$ is the singular value decomposition of the mean-centered data matrix $\boldsymbol X$.

* Vectors of $(→\Sigma →V)$ define the directions of the components, so called **loadings**.

* Each vector of $→U$ represents one corresponding centered example $\boldsymbol X \in \boldsymbol X - \bar{\boldsymbol x}$ as distances in a coordinate system given by the loadings.

---
# Principal Component Analysis

The **principal component analysis**, **PCA**, is a linear transformation used for
- dimensionality reduction,
- feature extraction,
- data visualization.

~~~
To motivate the dimensionality reduction, consider a dataset consisting
of a randomly translated and rotated image.

![w=100%,h=center](manifold_example.png)

~~~
Every member of the dataset can be described just by three quantities –
horizontal and vertical offsets and a rotation. We usually say that the
_data lie on a manifold of dimension three_.

---
class: center
# Data Visualization

Word embeddings from neural machine translation.

![w=45%](nmt_encoder_pca.png)

---
# PCA versus Supervised ML

Note that PCA does not have access to supervised labels, so it may not
give a solution favorable for further classification. PCA = projecting on the magenta line, which does not help the classification.

![w=55%,h=center](pca_unsupervised_vs_supervised.svgz)

---
section: Power Iteration
class: section
# Power Iteration Algorithm


---
style: ul {margin-bottom: 0}
# Computing PCA — The Power Iteration Algorithm

If we want only the first (or several first) principal components, we might use
the **power iteration algorithm**.

~~~
The power iteration algorithm can be used to find a **dominant** eigenvalue (an
eigenvalue with an absolute value strictly larger than absolute values of all other
eigenvalues) and the corresponding eigenvector (it is used for example to
compute PageRank). It works as follows:

<div class="algorithm">

**Input**: Real symmetric matrix $⇉A$ with a dominant eigenvalue.<br>
**Output**: The dominant eigenvalue $λ$ and the corresponding eigenvector $→v$, with
probability close to 1.

- Initialize $v$ randomly (for example each component from $U[-1,1]$).
~~~
- Repeat until convergence (or for a fixed number of iterations):
  - $→v ← ⇉A→v$
  - $λ ← \|→v\|$
  - $→v ← →v / λ$
</div>

~~~
If the algorithm converges, then $→v = ⇉A→v / λ$, so $→v$ is an eigenvector with
eigenvalue $λ$.

---
class: dbend
# Computing PCA — The Power Iteration Algorithm

In order to analyze the convergence, let $(λ_1, λ_2, λ_3, …)$ be the eigenvalues
of $⇉A$, in descending order of absolute values, so $|λ_1| > |λ_2| ≥ |λ_3| ≥ …$,
where the strict equality is the consequence of the dominant eigenvalue
assumption.

~~~
If we express the vector $→v$ in the basis of the eigenvectors as
$a_1→u_1 + a_2→u_1 + a_3→u_1 …$, then $⇉A→v$ is in the basis of the eigenvectors:
$$⇉A→v = λ_ 1 a_ 1→u_1 + λ_ 2 a_ 2→u_2 + λ_ 3 a_ 3→u_3 … $$
~~~
$$⇉A^k→v = λ_1^k a_1→u_2 + λ_2^k a_2→u_2 + λ_3^k a_3→u_3 … = \lambda_1^k \left(a_1→u_1 + \frac{\lambda_2^k}{\lambda_1^k}a_2→u_2 + \frac{\lambda_3^k}{\lambda_1^k}a_3→u_3 + \ldots\right)$$

~~~
Coordinates $\frac{\lambda_i^k}{\lambda_1^k}$ go to zero with $k \rightarrow
\infty$. Normalization during the algorithm prevents the $\lambda_1^k$ term
from exploding.

~~~
If the initial $→v$ had a nonzero first coordinate $a_1$ (which has probability
very close to 1), then repeated multiplication with $⇉A$ converges to the
eigenvector corresponding to $λ_1$.

---
# Computing PCA — The Power Iteration Algorithm

After we get the largest eigenvalue $λ_1$ and its eigenvector $→v_1$, we can modify the
matrix $⇉A$ to “remove the eigenvalue $λ_1$”.
~~~
Consider $⇉A - λ_1 →v_1 →v_1^T$:
~~~
- multiplying it by $→v_1$ returns zero:
  $$\big(⇉A - λ_1 →v_1 →v_1^T\big) →v_1 = λ_1 →v_1 - λ_1 →v_1 \underbrace{→v_1^T →v_1}_{1} = 0,$$
~~~
- multiplying it by other eigenvectors $→v_i$ gives the same result as
  multiplying $⇉A$:
  $$\big(⇉A - λ_1 →v_1 →v_1^T\big) →v_i = ⇉A →v_i - λ_1 →v_1 \underbrace{→v_1^T →v_i}_{0} = ⇉A →v_i.$$

~~~
Therefore, $⇉A - λ_1 →v_1 →v_1^T$ has the same set of eigenvectors and
eigenvalues, except for $→v_1$, which now has eigenvalue 0.

---
style: ul {margin-bottom: 0}
# Computing PCA — The Power Iteration Algorithm

We are now ready to formulate the complete algorithm for computing the PCA.

<div class="algorithm">

**Input**: Matrix $⇉X$, desired number of dimensions $M$.

- Compute the mean $→μ$ of the examples (the rows of $⇉X$).

~~~
- Compute the covariance matrix $⇉S ← \frac{1}{N} \big(⇉X - →μ\big)^T\big(⇉X - →μ\big)$.

~~~
- for $i$ in $\{1, 2, …, M\}$:
  - Initialize $→v_i$ randomly.
  - Repeat until convergence (or for a fixed number of iterations):
    - $→v_i ← ⇉S→v_i$;
    - $λ_i ← \|→v_i\|$;
    - $→v_i ← →v_i / λ_i$.
~~~
  - $⇉S ← ⇉S - λ_i →v_i →v_i^T$.
~~~
- Return $⇉X ⇉V$, where the columns of $⇉V$ are $→v_1, →v_2, …, →v_M$.
</div>

---
section: Clustering
# Clustering

Clustering is an unsupervised machine learning technique, which given input
data tries to divide them into some number of groups, or **clusters**.

~~~
The number of clusters might be given in advance, or we should infer it.

~~~
When clustering documents, we usually normalize TF-IDF so that each feature
vector has length 1 (i.e., L2 normalization), because then
$$1 - \operatorname{cosine~similarity}(→x, →y) = \frac{1}{2} \|→x - →y\|^2.$$

---
section: K-Means
class: section
# K-Means Clustering
---

# K-Means Clustering

Let $→x_1, →x_2, …, →x_N$ be a collection of $N$ input examples, each being
a $D$-dimensional vector $→x_i ∈ ℝ^D$. Let $K$, the number of target clusters,
be given.

~~~
Let $z_{i, k} ∈ \{0, 1\}$ be binary indicator variables describing whether an input
example $→x_i$ is assigned to cluster $k$, and let each cluster be specified by
a point $→μ_k, k ∈ \{1, ..., K\}$, usually called the cluster **center**.

~~~
Our objective function $J$, which we aim to minimize, is
$$J = ∑_{i=1}^N ∑_{k=1}^K z_{i, k} \|→x_i - →μ_k\|^2.$$

---
# K-Means Clustering

<div class="algorithm">

**Input**: Input points $→x_1, …, →x_N$, number of clusters $K$.

- Initialize $→μ_1, …, →μ_K$ as $K$ random input points.

~~~
- Repeat until convergence (or until patience runs out):
~~~
  - Compute the best possible $z_{i, k}$. It is easy to see that the smallest $J$
    is achieved by
    $$z_{i,k} = \begin{cases} 1 & \textrm{~~if~}k = \argmin\nolimits_j \|→x_i - →μ_j\|^2, \\
                              0 & \textrm{~~otherwise}.\end{cases}$$

~~~
  - Compute the best possible $→μ_k = \argmin\nolimits_{→μ} ∑_i z_{i,k} \|→x_i-→μ\|^2$.
~~~
   By computing the derivative with respect to $→μ$, we get
   $$→μ_k = \frac{∑_i z_{i,k} →x_i}{∑_i z_{i,k}}.$$
</div>

---
# K-Means Clustering

![w=55%,h=center](kmeans_example.svgz)

---
# K-Means Clustering

![w=58%,f=right](kmeans_convergence.svgz)

It is easy to see that:
- updating the cluster assignment $z_{i, k}$ decreases the loss $J$ or keeps it the same;
~~~
- updating the cluster centers again decreases the loss $J$ or keeps it the
  same.

~~~
K-Means clustering therefore converges to a local optimum. However, it
is quite sensitive to the starting initialization:
~~~
- It is common practice to run K-Means algorithm multiple times with different
  initialization and use the result with the lowest $J$ (scikit-learn uses
  `n_init=10` by default).
~~~
- Instead of using random initialization, `k-means++` initialization scheme might
  be used, where the first cluster center is chosen randomly, and others are
  chosen proportionally to the square of their distance to the nearest cluster
  center. It can be proven that with this initialization, the solution
  has $𝓞(\log K)$ approximation ratio in expectation.

---
# K-Means Clustering

![w=75%,h=center](kmeans_color_reduction.svgz)


---
# Gaussian Mixture vs K-Means

Clusters are modeled as Gaussians.

It could be useful to consider that different clusters might have different
radii or even be ellipsoidal.

![w=90%,h=center](mog_kmeans_comparison.svgz)

---
# Other types of clustering

![w=25%,f=right](hierarchical.svgz)

## Hierarchical clustering

* Initially, each point is a singleton cluster

* Merging the most similar clusters until we reach the desired number of clusters

* Dozens of used metrics


## Density based clustering


![w=25%,f=right](dbscan.svgz)
![w=15%,f=right](dbscan_density.svgz)

* Similar to hierarchical clustering

* Finds dense regions where at least `min_points` are in an $\epsilon$ diameter, connects everything within $\epsilon$ distance

---
class: middle
# Clustering algorithms compared (1)

| K-Means | Gaussian Mixture | Hierarchical | DBSCAN |
|:-----:|:-----:|:-----:|:-----:|
| ![w=240px](compare_kmeans.svgz) | ![w=240px](compare_em.svgz) | ![w=240px](compare_hier.svgz) | ![w=240px](compare_dbscan.svgz) |

---
# Clustering algorithms compared (2)

![w=80%,h=center](cluster.svgz)

---
class: summary
# Today's Lecture Objectives

After this lecture you should be able to

- Theoretically explain Singular Value Decomposition (SVD), prove it exists and explain what the Eckart-Young theorem says

- Theoretically explain Principal Component Analysis (PCA) and say how it explains the variance in the data based on SVD

- Use SVD or PCA for dimensionality reduction, data visualization

- Implement the $k$-means algorithm and use it for clustering
