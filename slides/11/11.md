title: NPFL129, Lecture 11
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# PCA, K-Means, Gaussian Mixture

## Milan Straka

### December 14, 2020

---
section: PCA
class: center, middle
# Unsupervised Machine Learning

# Unsupervised Machine Learning

---
section: PCA
# Principal Component Analysis

The **principal component analysis**, **PCA**, is a technique used for
- dimensionality reduction,
- feature extraction,
- whitening,
- data visualization.

~~~
To motivate the dimensionality reduction, consider a dataset consisting
of a randomly translated and rotated image.

![w=100%,h=center](manifold_example.png)

~~~
Every member of the dataset can be described just by three quantities â€“
horizontal and vertical offsets and a rotation. We usually say that the
_data lie on a manifold of dimension three_.

---
# Principal Component Analysis

We start by defining the PCA in two ways.

~~~
![w=40%,f=right](pca_variance.svgz)

## Maximum Variance Formulation

Given data $â†’x_1, â€¦, â†’x_N$ with $â†’x_i âˆˆ â„^D$, the goal is to project
them to a space with dimensionality $M < D$, so that the variance
of their projection is maximal.

~~~
We start by considering a projection to one-dimensional space. Such a projection
is defined by a vector $â†’u_1$, and because only the direction of $â†’u_1$ matters,
we assume that $â†’u_1^T â†’u_1 = 1$.

We start by pointing out that the projection of $â†’x_i$ to $â†’u_1$ is given by
$(â†’u_1^T â†’x_i) â†’u_1$, because the vectors $â†’u_1$ and $â†’x_i - (â†’u_1^T â†’x_i) â†’u_1$
are orthogonal:

$$â†’u_1^T \big(â†’x_i - (â†’u_1^T â†’x_i) â†’u_1\big) = â†’u_1^T â†’x_i - (â†’u_1^T â†’x_i) â†’u_1^T â†’u_1 = 0.$$

---
# Principal Component Analysis

We therefore use $â†’u_1^T â†’x_i$ as the projection of $â†’x_i$. If we define
$â†’xÌ„ = âˆ‘_i â†’x_i / N$, the mean of the projected data is $â†’u_1^T â†’xÌ„$ and the
variance is given by
$$\frac{1}{N} âˆ‘_{i=1}^N \big(â†’u_1^T â†’x_i - â†’u_1^T â†’xÌ„\big)^2 = â†’u_1^T â‡‰S â†’u_1,$$
where $â‡‰S$ is the data covariance matrix defined as
$$â‡‰S = \frac{1}{N} âˆ‘_{i=1}^N \big(â†’x_i - â†’xÌ„\big)\big(â†’x_i - â†’xÌ„)^T.$$

~~~
Note that if the original data is centered (it has zero mean), then
$â‡‰S = \frac{1}{N} â‡‰X^T â‡‰X$.

---
# Principal Component Analysis

To maximize $â†’u_1^T â‡‰S â†’u_1$, we need to include the constraint $â†’u_1^T â†’u_1$ by
introducing a Lagrange multiplier $Î»_1$ for the constraint $â†’u_1^T â†’u_1 - 1 = 0$
and then maximizing
$$â†’u_1^T â‡‰S â†’u_1 + Î»_1\big(â†’u_1^T â†’u_1 - 1\big).$$

~~~
By computing a derivative with respect to $â†’u_1$, we get
$$â‡‰S â†’u_1 = Î»_1 â†’u_1.$$

~~~
Therefore, $â†’u_1$ must be an eigenvector of $â‡‰S$.

~~~
Because the value to maximize $â†’u_1^T â‡‰S â†’u_1$ is then $Î»_1 â†’u_1^T â†’u_1 = Î»_1$,
the maximum will be attained for eigenvector $â†’u_1$ corresponding to the largest
eigenvalue $Î»_1$.

~~~
The eigenvector $â†’u_1$ is know as the **first principal component**.

~~~
For a given $M$, the principal components are eigenvectors corresponding
to $M$ largest eigenvalues, and maximize the variance of the projected data.

---
# Principal Component Analysis

## Minimum Error Formulation

Assume $â†’u_1, â€¦, â†’u_D$ is some orthonormal set of vectors, therefore,
$â†’u_i^T â†’u_j = \big[i == j\big]$.

~~~
Every $â†’x_i$ can be then expressed using this basis as
$$â†’x_i = âˆ‘_j \big(â†’x_i^T â†’u_j) â†’u_j,$$
using a similar argument as the one we used to derive the orthogonal projection.

~~~
Because we want to eventually represent the data using $M$ dimensions, we will
approximate the data by the first $M$ basis vectors:
$$â†’xÌƒ_i = âˆ‘_{j=1}^M z_{i,j} â†’u_j + âˆ‘_{j=M+1}^D b_j â†’u_j.$$

---
# Principal Component Analysis

We now choose the vectors $â†’u_j$, coordinates $z_{i,j}$ and biases $b_j$ to
minimize the approximation error, which we measure as a loss
$$L = \frac{1}{N} âˆ‘_{i=1}^N ||â†’x_i - â†’xÌƒ_i||^2.$$

~~~
To minimize the error, we compute the derivative of $L$ with respect to
$z_{i,j}$ and $b_j$, obtaining
$$z_{i,j} = â†’x_i^T â†’u_j,~~~~b_j = â†’xÌ„^T â†’u_j.$$

~~~
Therefore, we can rewrite the loss as
$$L = \frac{1}{N} âˆ‘_{i=1}^N âˆ‘_{j=M+1}^D (â†’x_i^T â†’u_j - â†’xÌ„^T â†’u_j)^2 = âˆ‘_{j=M+1}^D â†’u_j^T â‡‰S â†’u_j.$$

~~~
Analogously, we can minimize $L$ by choosing eigenvectors of $D-M$
smallest eigenvalues.

---
# PCA Applications â€“ Data Compression

We can represent the data $â†’x_i$ by the approximations $â†’xÌƒ_i$
$$ â†’xÌƒ_i = âˆ‘_{j=1}^M \big(â†’x_i^T â†’u_j\big) â†’u_j + âˆ‘_{j=M+1}^D \big(â†’xÌ„^T â†’u_j\big) â†’u_j
        = â†’xÌ„ + âˆ‘_{j=1}^M \big(â†’x_i^T â†’u_j - â†’xÌ„^T â†’u_j\big)â†’u_j.$$

![w=100%,h=center](pca_threes_reconstruction.png)

---
# PCA Applications â€“ Data Compression

![w=80%,h=center](pca_threes_eigenvalues.svgz)

![w=80%,h=center](pca_threes_eigenvectors.png)

---
# PCA Applications â€“ Whitening or Sphereing

The PCA formula allows us to perform **whitening** or **sphereing**, which is
a linear transformation of the given data, so that the resulting dataset
has zero mean and identity covariance.

~~~
Notably, if
$$â‡‰S â‡‰U = â‡‰U â‡‰Î›,$$
we can define transformed data
$$â†’z_i â‰ â‡‰Î›^{-1/2} â‡‰U^T (â†’x_i - â†’xÌ„).$$

~~~
Then, the mean of $â†’z_i$ is zero and the covariance is given by
$$\begin{aligned}
\frac{1}{N} âˆ‘_i â†’z_i â†’z_i^T
  &= \frac{1}{N} âˆ‘_i â‡‰Î›^{-1/2} â‡‰U^T (â†’x_i - â†’xÌ„) (â†’x_i - â†’xÌ„)^T â‡‰U â‡‰Î›^{-1/2} \\
  &= â‡‰Î›^{-1/2} â‡‰U^T â‡‰S â‡‰U â‡‰Î›^{-1/2} = â‡‰Î›^{-1/2} â‡‰Î› â‡‰Î›^{-1/2} = â‡‰I.
\end{aligned}$$

---
# PCA Applications â€“ Whitening or Sphereing

![w=100%,v=middle](pca_whitening.svgz)

---
# PCA versus Supervised ML

Note that PCA does not have access to supervised labels, so it may not
give a solution favorable for further classification.

![w=63%,h=center](pca_unsupervised_vs_supervised.svgz)

---
# Principal Component Analysis and MLPs

![w=40%,h=center](pca_ae.svgz)

Note that it can be proven that if we construct a MLP _autoencoder_,
which is a model trying to reconstruct input as close as possible,
then even if the hidden layer uses non-linear activation, the solution
to a MSE loss is a projection onto the $M$-dimensional subspace defined
by the first $M$ principal components (but is not necessary orthonormal
or orthogonal).

---
# Principal Component Analysis and MLPs

However, non-linear PCA can be achieved, if both the _encoder_ and the _decoder_
are non-linear.

![w=70%,h=center](pca_ae_nonlinear.svgz)

---
# Computing PCA

There are two frequently used algorithms for performing PCA.

~~~
If we want to compute all (or many) principal components, we can compute
directly the eigenvectors and eigenvalues of the covariance matrix.

~~~
We can even avoid computing the covariance matrix. If we instead compute the
singular value decomposition of $â‡‰X = â‡‰U â‡‰D â‡‰V^T$, it holds that
$$â‡‰X^T â‡‰X = â‡‰V â‡‰D â‡‰U^T â‡‰U â‡‰D â‡‰V^T = â‡‰V â‡‰D^2 â‡‰V^T.$$

~~~
Therefore, 
$$â‡‰X^T â‡‰X â‡‰V = â‡‰V â‡‰D^2,$$
which means that $â‡‰V$ are the eigenvectors of $â‡‰X^T â‡‰X$ and its eigenvalues
are the squares of the singular values of $â‡‰X$.

---
section: PowerIteration
# Computing PCA â€” The Power Iteration Algorithm

If we want only the first (or several first) principal components, we might use
the **power iteration algorithm**.

Details on the practicals.

---
section: Clustering
# Clustering

Clustering is an unsupervised machine learning technique, which given input
data tries to divide them into some number of groups, or _clusters_.

~~~
The number of clusters might be given in advance, or should also be inferred.

~~~
When clustering documents, we usually use TF-IDF normalized so that each
feature vector has length 1 (i.e., L2 normalization).

---
section: KMeans
# K-Means Clustering

Let $â†’x_1, â†’x_2, â€¦, â†’x_N$ be a collection of $N$ input examples, each being
a $D$-dimensional vector $â†’x_i âˆˆ â„^D$. Let $K$, the number of target clusters,
be given.

~~~
Let each cluster be specified by a point $â†’Î¼_1, â€¦, â†’Î¼_K$.
~~~
Further, let $z_{i, k} âˆˆ \{0, 1\}$ be a binary indicator variables describing whether input
example $â†’x_i$ is assigned to cluster $k$, and let each cluster be specified by
a point $â†’Î¼_1, â€¦, â†’Î¼_K$, usually called the cluster _center_.

~~~
Our objective function $J$ which we aim to minimize is
$$J = âˆ‘_{i=1}^N âˆ‘_{k=1}^K z_{i, k} ||â†’x_i - â†’Î¼_k||^2.$$

---
# K-Means Clustering

To find out the cluster centers $â†’Î¼_i$ and input example assignments $z_{i, k}$,
we use the following iterative algorithm (which could be considered a coordinate
descent):

~~~
<div class="algorithm">

**Input**: Input points $x_1, â€¦, x_N$, number of clusters $K$.

Repeat until convergence (or until patience runs out):
1. Compute the best possible $z_{i, k}$. It is easy to see that the smallest $J$
   is achieved by
   $$z_{i,k} = \begin{cases} 1 & \textrm{~~if~}k = \argmin\nolimits_j ||â†’x_i - â†’Î¼_j||^2, \\
                             0 & \textrm{~~otherwise}.\end{cases}$$

~~~
2. Compute the best possible $â†’Î¼_k = \argmin\nolimits_â†’Î¼ âˆ‘_i z_{i,k} ||â†’x_i-â†’Î¼||^2$.
~~~
   By computing a derivative with respect to $â†’Î¼$, we get
   $$â†’Î¼_k = \frac{âˆ‘_i z_{i,k} â†’x_i}{âˆ‘_i z_{i,k}}.$$
</div>

---
# K-Means Clustering

![w=55%,h=center](kmeans_example.svgz)

---
# K-Means Clustering

![w=60%,f=right](kmeans_convergence.svgz)

It is easy to see that:
- updating the cluster assignment $z_{i, k}$ decreases the loss $J$ or keeps it the same;
~~~
- updating the cluster centers again decreases the loss $J$ or keeps it the
  same.

~~~
K-Means clustering therefore converges to a local optimum. However, it
is quite sensitive to the starting initialization:
~~~
- It is common practise to run K-Means algorithm multiple times with different
  initialization and use the result with lowest $J$ (scikit-learn uses
  `n_init=10` by default).
~~~
- Instead of using random initialization, `k-means++` initialization scheme might
  be used, where the first cluster center is chosen randomly and others are
  chosen proportionally to the square of their distance to the nearest cluster
  center. It can be proven that with such initialization, the found solution
  has $ğ“(\log K)$ approximation ratio in expectation.

---
# K-Means Clustering

![w=75%,h=center](kmeans_color_reduction.svgz)

---
section: MultivariateGaussian
# Multivariate Gaussian Distribution

Recall that
$$ğ“(x; Î¼, Ïƒ^2) = \sqrt{\frac{1}{2Ï€Ïƒ^2}} \exp \left(-\frac{(x - Î¼)^2}{2Ïƒ^2}\right).$$

~~~
For $D$-dimensional vector $â†’x$, the multivariate Gaussian distribution takes
the form
$$ğ“(â†’x | â†’Î¼, â‡‰Î£) â‰ \frac{1}{\sqrt{(2Ï€)^D |Î£|}} \exp \left(-\frac{1}{2}(â†’x-â†’Î¼)^T â‡‰Î£^{-1} (â†’x-â†’Î¼) \right).$$

---
# Multivariate Gaussian Distribution

The $â‡‰Î£$ is a covariance matrix, and it must be a _symmetrical positive definite_ matrix. If we represent it
using its _eigenvectors_ $â†’u_i$ and _eigenvalues_ $Î»_i$, we get
$$â‡‰Î£^{-1} = âˆ‘_i \frac{1}{Î»_i} â†’u_i â†’u_i^T,$$
~~~
from which we can see that the constant surfaces of the multivariate Gaussian
distribution are ellipsoids centered at $â†’Î¼$, with axes oriented at $â†’u_i$
with scaling factors $Î»_i ^{1/2}$.

![w=30%](multivariate_gaussian_elipsoids.svgz)![w=60%](multivariate_gaussian_covariance.svgz)

---
section: GaussianMixture
# Gaussian Mixture vs K-Means

It could be useful to consider that different clusters might have different
radii or even be ellipsoidal.

![w=100%,h=center](mog_kmeans_comparison.svgz)

---
# Gaussian Mixture

Let $â†’x_1, â†’x_2, â€¦, â†’x_N$ be a collection of $N$ input examples, each being
a $D$-dimensional vector $â†’x_i âˆˆ â„^D$. Let $K$, the number of target clusters,
be given.

~~~
Our goal is to represent the data as a Gaussian mixture, which is a combination
of $K$ Gaussian in the form
$$p(â†’x) = âˆ‘_{k=1}^K Ï€_k ğ“(â†’x | â†’Î¼_k, â‡‰Î£_k).$$
Therefore, each cluster is parametrized as $ğ“(â†’x | â†’Î¼_k, â‡‰Î£_k)$.

~~~
Let $â†’z âˆˆ \{0, 1\}^K$ be a $K$-dimensional random variable, such that exactly
one $z_k$ is 1, denoting to which cluster a training example belongs.
~~~
Let the marginal distribution of $z_k$ be
$$p(z_k = 1) = Ï€_k.$$

~~~
Therefore, $p(â†’z) = âˆ_k Ï€_k^{z_k}$.

---
# Gaussian Mixture

![w=6%,f=right](mog_latent.svgz)

We can write
$$p(â†’x) = âˆ‘_{â†’z} p(â†’z) p(â†’x | â†’z) = âˆ‘_{k=1}^K Ï€_k ğ“(â†’x | â†’Î¼_k, â‡‰Î£_k)$$
~~~
and the probability of the whole clustering is therefore
$$\log p(â‡‰X | â†’Ï€, â†’Î¼, â†’Î£) = âˆ‘_{i=1}^N \log \left(âˆ‘_{k=1}^K Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)\right).$$

~~~
To fit a Gaussian mixture model, we start with maximum likelihood estimation and
minimize
$$ğ“›(â‡‰X) = - âˆ‘_i \log âˆ‘_{k=1}^K Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)$$

---
# Gaussian Mixture

The derivative of the loss with respect to $â†’Î¼_k$ gives
$$\frac{âˆ‚ğ“›(â‡‰X)}{âˆ‚â†’Î¼_k} = - âˆ‘_i \frac{Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i | â†’Î¼_l, â‡‰Î£_l)} â‡‰Î£_k^{-1} \big(â†’x_i - â†’Î¼_k\big)$$

~~~
Denoting $r(z_{i,k}) = \frac{Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i | â†’Î¼_l, â‡‰Î£_l)}$,
setting the derivative equal to zero and multiplying by $â‡‰Î£_k^{-1}$, we get
$$â†’Î¼_k = \frac{âˆ‘_i r(z_{i,k}) â†’x_i}{âˆ‘_i r(z_{i,k})}.$$

~~~
The $r(z_{i,k})$ are usually called **responsibilities** and denote the
probability $p(z_k = 1 | â†’x_i)$. Note that the responsibilities depend on
$â†’Î¼_k$, so the above equation is not an analytical solution for $â†’Î¼_k$, but
can be used as an _iterative_ algorithm for converting to local optimum.

---
# Gaussian Mixture

For $â‡‰Î£_k$, we again compute the derivative of the loss, which is technically
complicated (we need to compute derivative with respect a matrix, and also we
need to differentiate matrix determinant) and results in an analogous equation
$$â†’Î£_k = \frac{âˆ‘_i r(z_{i,k}) (â†’x_i - â†’Î¼_k) (â†’x_i - â†’Î¼_k)^T}{âˆ‘_i r(z_{i,k})}.$$

~~~
To minimize the loss with respect to $â†’Ï€$, we need to include the constraint
$âˆ‘_k Ï€_k = 1$, so we form a Lagrangian $ğ“›(â‡‰X) + Î»\left(âˆ‘\nolimits_k Ï€_k - 1\right)$,
and get
$$0 = âˆ‘_i \frac{ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i | â†’Î¼_l, â‡‰Î£_l)} + Î»,$$
~~~
from which we get $Ï€_k âˆ âˆ‘_i r(z_{i,k})$ and therefore
$$Ï€_k = 1/N â‹… âˆ‘\nolimits_i r(z_{i,k}).$$

---
# Gaussian Mixture

<div class="algorithm">

**Input**: Input points $x_1, â€¦, x_N$, number of clusters $K$.

- Initialize $â†’Î¼_k, â‡‰Î£_k$ and $Ï€_k$. It is common to start by running the
  K-Means algorithm to obtain $z_{i,k}$, set $r(z_{i,k}) â† z_{i,k}$
  and use the **M step** below.

- Repeat until convergence (or until patience runs out):
  - **E step**. Evaluate the responsibilities as
    $$r(z_{i,k}) = \frac{Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i | â†’Î¼_l, â‡‰Î£_l)}.$$
  - **M step**. Maximize the log-likelihood by setting

    $$â†’Î¼_k = \frac{âˆ‘_i r(z_{i,k}) â†’x_i}{âˆ‘_i r(z_{i,k})},~~
      â†’Î£_k = \frac{âˆ‘_i r(z_{i,k}) (â†’x_i - â†’Î¼_k) (â†’x_i - â†’Î¼_k)^T}{âˆ‘_i r(z_{i,k})},~~
      Ï€_k = \frac{âˆ‘_i r(z_{i,k})}{N}.$$

</div>

---
# Gaussian Mixture

![w=75%,h=center](mog_data.svgz)
![w=75%,h=center](mog_illustration.svgz)

---
# Gaussian Mixture

![w=75%,h=center](mog_example.svgz)

---
section: EM
# EM Algorithm

The algorithm for estimating the Gaussian mixture is an example of an **EM
algorithm**.

~~~
The **EM algorithm** algorithm can be used when given a a joint distribution
$p(â‡‰X, â‡‰Z | â†’w)$ over observed variables $â‡‰X$ and latent (hidden, unseen)
variabled $â‡‰Z$, parametried by $â†’w$,
~~~
we maximize
$$\log p(â‡‰X; â†’w) = \log \left(âˆ‘_â‡‰Z p(â‡‰X, â‡‰Z; â†’w)\right)$$
with respect to $â†’w$.

~~~
The main idea is to approximate the sum over all latent variables by
the expectation of the joint probability under the posterior latent
variable distribution $p(â‡‰Z | â‡‰X; â†’w)$.

~~~
Usually, the latent variables $â‡‰Z$ indicate membership of the data in one of the
set of groups.

---
# EM Algorithm

<div class="algorithm">

- Initialize the parameters $â†’w^\mathrm{old}$.

- Repeat until convergence (or until patience runs out):
  - **E step**. Evaluate
    $$Q(â†’w | â†’w^\mathrm{old}) = ğ”¼_{â‡‰Z|â‡‰X, â†’w} \big[\log p(â‡‰X, â‡‰Z; â†’w)\big].$$
  - **M step**. Maximize the log-likelihood by computing
    $$â†’w^\mathrm{new} â† \argmax_â†’w Q(â†’w | â†’w^\mathrm{old}).$$
</div>
