title: NPFL129, Lecture 2
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Linear Regression II, SGD

## Milan Straka

### October 10, 2022

---
section: Refresh
# Linear Regression

Given an input value $â†’x âˆˆ â„^D$, **linear regression** computes predictions as:
$$y(â†’x; â†’w, b) = x_1 w_1 + x_2 w_2 + â€¦ + x_D w_D + b = âˆ‘_{i=1}^D x_i w_i + b = â†’x^T â†’w + b.$$
The _bias_ $b$ can be considered one of the _weights_ $â†’w$ if convenient.

~~~
We train the weights by minimizing an **error function** between the real target
values and their predictions, notably _sum of squares_:
$$\frac{1}{2} âˆ‘_{i=1}^N \big(y(â†’x_i; â†’w) - t_i\big)^2$$

~~~
There are various approaches to minimize it, but for linear regression an explicit solution exists:
$$â†’w = (â‡‰X^Tâ‡‰X)^{-1}â‡‰X^Tâ†’t.$$

---
# Linear Regression Example

Assume our input vectors comprise of $â†’x = (x^0, x^1, â€¦, x^M)$, for $M â‰¥ 0$.

![w=60%,h=center](../01/sin_lr.svgz)

---
# Linear Regression Example

To plot the error, the _root mean squared error_
$\operatorname{RMSE}=\sqrt{\operatorname{MSE}}$ is frequently used.

![w=60%,f=right](../01/sin_errors.svgz)
~~~

The displayed error nicely illustrates two main challenges in machine learning:
- _underfitting_
- _overfitting_

---
section: Regularization
# Model Capacity
We can control whether a model underfits or overfits by modifying its **capacity**.
~~~
- representational capacity
- effective capacity

~~~
![w=80%,h=center](generalization_error.svgz)

---
# Linear Regression Overfitting

Note that employing more data usually alleviates overfitting (the relative
capacity of the model is decreased).

![w=100%](sin_overfitting.svgz)

---
# Regularization

**Regularization**, in a broad sense, is any change that is designed to _reduce generalization error_ (but not necessarily
its training error) in a machine learning
algorithm.

~~~
We already saw that **limiting model capacity** can work as regularization.

![w=38%,h=center](classification_overfitting.svgz)

---
# L2 Regularization

Being one of the oldest regularization techniques, it tries to prefer â€œsimplerâ€ models
by endorsing models with **smaller weights**.

~~~
Concretely, **$\boldsymbol{L^2}$-regularization** (also called **weight decay**) penalizes
models with large weights by utilizing the following error function:

$$\frac{1}{2} âˆ‘_{i=1}^N \big(y(â†’x_i; â†’w) - t_i\big)^2 + \textcolor{red}{\frac{Î»}{2} \|â†’w\|^2}.$$

~~~
Note that the $L^2$-regularization is usually not applied to the _bias_, only to the
â€œproperâ€ weights, because we cannot really overfit via the bias.
~~~
Also, without penalizing the bias, $L^2$-regularization is invariant to shifts
(i.e., adding a constant to all the targets would result in the same solution, only with
the bias increased by that constant; if the bias were to be penalized, this would not
be true).

~~~
For simplicity, we will not explicitly exclude the bias from the
$L^2$-regularization penalty in the slides (several textbooks also take the
same approach).

---
# L2 Regularization

![w=25%,f=right](l2_smoothness_data.png)

One way to look at $L^2$-regularization is as a promoter of smaller
changes of the model (the gradient of linear regression with respect to the
inputs are exactly the weights, i.e., $âˆ‡_{â†’x} y(â†’x; â†’w) = â†’w$).

~~~
Considering the data points on the right, we present mean squared errors
and $L^2$ norms of the weights for three linear regression models:

![w=70%,h=center](l2_smoothness.png)
![w=70%,h=center](l2_smoothness_equations.png)

---
# L2 Regularization

The effect of $L^2$-regularization can be seen as limiting the _effective
capacity_ of the model.

![w=66%,mh=70%,v=bottom](sin_regularization.svgz)
~~~
![w=32.5%,mh=70%,v=bottom](sin_regularization_ablation.svgz)

---
# Regularizing Linear Regression

In a matrix form, the regularized "sum of squares error" for linear regression amounts
to
$$\tfrac{1}{2} \|â‡‰Xâ†’w - â†’t\|^2 + \tfrac{Î»}{2} \|â†’w\|^2.$$

When repeating the same calculation as in the unregularized case, we arrive at
$$(â‡‰X^Tâ‡‰X + Î»â‡‰I)â†’w = â‡‰X^Tâ†’t,$$
where $â‡‰I$ is an identity matrix.

~~~
<div class="algorithm">

**Input**: Dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$), constant $Î» âˆˆ â„^+$.<br>
**Output**: Weights $â†’w âˆˆ â„^D$ minimizing MSE of regularized linear regression.

- $â†’w â† (â‡‰X^Tâ‡‰X + Î»â‡‰I)^{-1}â‡‰X^Tâ†’t.$
</div>

~~~
Note that the matrix $â‡‰X^Tâ‡‰X + Î»â‡‰I$ is always regular for $Î»>0$ (you can show that the matrix is positive definite). So, another effect of $L^2$-regularization is that the
inverse always exists.

---
section: Hyperparameters
# Choosing Hyperparameters

**Hyperparameters** are not adapted by the learning algorithm itself.

~~~
Usually a **validation set** or **development set** is used to
estimate the generalization error, allowing us to update hyperparameters accordingly.
If there is not enough data (well, there is **always** not enough data),
more sophisticated approaches can be used.

~~~
So far, we have seen two hyperparameters, $M$ and $Î»$.

~~~
![w=88%,mw=50%,h=center](../01/sin_errors.svgz)![w=87%,mw=50%,h=left](sin_regularization_ablation.svgz)


---
section: SVD Solution
# Linear Regression

When training a linear regression model, we minimized the "sum of squares error
function" by computing its gradient (partial derivatives with respect to all weights). We found a solution when the gradient is equal to zero, arriving at the following equation
for optimal weights:
$$â‡‰X^Tâ‡‰Xâ†’w = â‡‰X^T â†’t.$$

~~~
If $â‡‰X^T â‡‰X$ is regular, we can invert it and compute the weights as $â†’w = (â‡‰X^Tâ‡‰X)^{-1}â‡‰X^Tâ†’t$.

~~~
It can be proven (see next slide) that $\operatorname{rank}(â‡‰X) = \operatorname{rank}(â‡‰X^T â†’X)$.
So, the matrix $â‡‰X^T â‡‰X âˆˆ â„^{DÃ—D}$ is regular if and only if $â‡‰X$ has rank $D$, which is
equivalent to the columns of $â‡‰X$ being linearly independent.

---
class: dbend
# Linear Regression Solution Always Exists

We now show that the solution of $â‡‰X^Tâ‡‰Xâ†’w = â‡‰X^T â†’t$ always exists.

~~~
Recall that the rank-nullity theorem states that for a matrix $â‡‰A âˆˆ â„^{VÃ—W}$,
$$\operatorname{rank}(â‡‰A) + \operatorname{nullity}(â‡‰A) â‰ \dim(\operatorname{im}(â‡‰A)) + \dim(\operatorname{ker}(â‡‰A)) = W.$$

~~~
Our goal is to show that $\operatorname{im}(â‡‰X^T â‡‰X) = \operatorname{im}(â‡‰X^T)$.
This would imply that the solution will always exist because for any $â†’t$, $â‡‰X^T â†’t âˆˆ \operatorname{im}(â‡‰X^T â‡‰X)$.
~~~
- We first show that $\operatorname{ker}(â‡‰X^T â‡‰X) = \operatorname{ker}(â‡‰X)$.
~~~
  - If $â‡‰X â†’t = 0$, then also $â‡‰X^T â‡‰X â†’t = 0$, so $\operatorname{ker}(â‡‰X^T â‡‰X) âŠ‡ \operatorname{ker}(â‡‰X)$.
~~~
  - If $â‡‰X^T â‡‰X â†’t = 0$, then also $â†’t^T â‡‰X^T â‡‰X â†’t = 0$. Therefore $(â‡‰X â†’t)^T (â‡‰X â†’t) = 0$, which implies
    $â‡‰X â†’t = 0$, resulting in $\operatorname{ker}(â‡‰X^T â‡‰X) âŠ† \operatorname{ker}(â‡‰X)$.
~~~
- Therefore, the rank-nullity theorem implies that
  $\operatorname{rank}(â‡‰X^T â‡‰X) = \operatorname{rank}(â‡‰X) = \operatorname{rank}(â‡‰X^T)$.
~~~
- Finally, it is easy to see that $\operatorname{im}(â‡‰X^T â‡‰X) âŠ† \operatorname{im}(â‡‰X^T)$. This,
  combined with the equality of ranks above, proves the required equation $\operatorname{im}(â‡‰X^T â‡‰X) = \operatorname{im}(â‡‰X^T)$.

---
class: dbend
# SVD Solution of Linear Regression

Now consider the case that $â‡‰X^T â‡‰X$ is singular. We already know that $â‡‰X^Tâ‡‰Xâ†’w
= â‡‰X^T â†’t$ is solvable, but it does not have a unique solution (it has many
solutions). Our goal in this case will be to find the $â†’w$ with the minimum
$\|â†’w\|$ fulfilling the equation.

~~~
We now consider _singular value decomposition (SVD)_ of $â‡‰X$, writing $â‡‰X = â‡‰U â‡‰Î£ â‡‰V^T$,
where
- $â‡‰U âˆˆ â„^{NÃ—N}$ is an orthogonal matrix, i.e., $â†’u_i^T â†’u_j = [i=j] â‡” â‡‰U^T â‡‰U = â‡‰I â‡” â‡‰U^{-1} = â‡‰U^T$,
- $â‡‰Î£ âˆˆ â„^{NÃ—D}$ is a diagonal matrix,
- $â‡‰V âˆˆ â„^{DÃ—D}$ is again an orthogonal matrix.

~~~
Assuming the diagonal matrix $â‡‰Î£$ has a rank $r$, we can write the matrix as
$$â‡‰Î£ = \begin{bmatrix} â‡‰Î£_r & â‡‰0 \\ â‡‰0 & â‡‰0 \end{bmatrix},$$
where $â‡‰Î£_râˆˆâ„^{rÃ—r}$ is a regular diagonal matrix.
~~~
Denoting the matrices of the first $r$ columns of $â‡‰U$ and $â‡‰V$ as $â‡‰U_r$ and $â‡‰V_r$, respectively,
we can write $â‡‰X = â‡‰U_r â‡‰Î£_r â‡‰V_r^T$.

---
class: dbend
# SVD Solution of Linear Regression

Using the decomposition $â‡‰X = â‡‰U_r â‡‰Î£_r â‡‰V_r^T$, we can rewrite the goal equation
as
$$\big(â‡‰V_r â‡‰Î£_r^T â‡‰U_r^T\big) \big(â‡‰U_r â‡‰Î£_r â‡‰V_r^T\big) â†’w = \big(â‡‰V_r â‡‰Î£_r^T â‡‰U_r^T\big) â†’t.$$

~~~
The transposition of an orthogonal matrix is its inverse. Therefore, our
submatrix $â‡‰U_r$ fulfills $\textcolor{darkred}{â‡‰U_r^T â‡‰U_r} = â‡‰I$, because
$â‡‰U_r^T â‡‰U_r$ is the top left submatrix of $â‡‰U^T â‡‰U$. Analogously,
$\textcolor{blue}{â‡‰V_r^T â‡‰V_r} = â‡‰I$.

~~~
We therefore simplify the goal equation to
$$\textcolor{blue}{â‡‰V_r^T â‡‰V_r} â‡‰Î£_r^T \textcolor{darkred}{â‡‰U_r^T â‡‰U_r} â‡‰Î£_r â‡‰V_r^T â†’w = \textcolor{blue}{â‡‰V_r^T â‡‰V_r} â‡‰Î£_r^T â‡‰U_r^T â†’t.$$
~~~
$$â‡‰Î£_r^T â‡‰Î£_r â‡‰V_r^T â†’w = â‡‰Î£_r â‡‰U_r^T â†’t$$

~~~
Because the diagonal matrix $â‡‰Î£_r=â‡‰Î£_r^T$ is regular, we can divide by it and obtain
$$â‡‰V_r^T â†’w = â‡‰Î£_r^{-1} â‡‰U_r^T â†’t.$$

---
class: dbend
# SVD Solution of Linear Regression

We have $â‡‰V_r^T â†’w = â‡‰Î£_r^{-1} â‡‰U_r^T â†’t$. If the original matrix $â‡‰X^T â‡‰X$ was
regular, then $r=D$ and $â‡‰V_r$ is a square regular orthogonal matrix, in which case
$$â†’w = â‡‰V_r â‡‰Î£_r^{-1} â‡‰U_r^T â†’t.$$

~~~
If we denote $â‡‰Î£_r^{-1} âˆˆ â„^{DÃ—N}$, the diagonal matrix with $Î£_{i,i}^{-1}$ on its
diagonal, as $â‡‰Î£^+$, we can rewrite $â†’w$ to
$$â†’w = â‡‰V â‡‰Î£^+ â‡‰U^T â†’t.$$

~~~
Now if $r < D$, $â‡‰V_r^T â†’w = â†’y$ is undetermined and has infinitely many
solutions. To find the one with the smallest norm $\|â†’w\|$, consider the full
product $â‡‰V^Tâ†’w$. Because $â‡‰V$ is orthogonal, $\|â‡‰V^Tâ†’w\|=\|â†’w\|$, and it is
sufficient to find $â†’w$ with the smallest $\|â‡‰V^Tâ†’w\|$.
~~~
We know that the first $r$ elements of $â‡‰V^Tâ†’w$ are fixed by the above equation
â€“ the smallest $\|â‡‰V^Tâ†’w\|$ can be therefore obtained by setting the last $D-r$
elements to zero.
~~~
Finally, note that $â‡‰Î£^+ â‡‰U^T â†’t$ is exactly $â‡‰Î£_r^{-1} â‡‰U_r^T â†’t$ padded
with $D-r$ zeros, obtaining the same solution $â†’w = â‡‰V â‡‰Î£^+ â‡‰U^T â†’t$.

---
class: dbend
# SVD Solution of Linear Regression and Pseudoinverses

The solution to a linear regression problem with a "sum of squares error function" is
tightly connected to matrix pseudoinverses. If a matrix $â‡‰X$ is singular or
rectangular, it does not have an exact inverse, and $â‡‰Xâ†’w=â†’b$ does not
have an exact solution.

~~~
However, we can consider the so-called _Moore-Penrose pseudoinverse_
$$â‡‰X^+ â‰ â‡‰V â‡‰Î£^+ â‡‰U^T$$
to be the closest approximation to an inverse, in the sense that we can find
the best solution (with smallest MSE) to the equation $â‡‰Xâ†’w=â†’b$ by setting $â†’w=â‡‰X^+ â†’b$.

~~~
Alternatively, we can define the pseudoinverse of a matrix $â‡‰X$ as
$$â‡‰X^+ = \argmin_{â‡‰Yâˆˆâ„^{DÃ—N}} \big\|â‡‰X â‡‰Y - â‡‰I_N\big\|_F = \argmin_{â‡‰Yâˆˆâ„^{DÃ—N}} \big\|â‡‰Y â‡‰X - â‡‰I_D\big\|_F$$
which can be verified to be the same as our SVD formula.

---
section: Random Variables
# Random Variables
A random variable $â‡x$ is a result of a random process. The random variable can be discrete or
continuous.

~~~
## Probability Distribution
A probability distribution describes how likely are the individual values that a random
variable can take.

The notation $â‡x âˆ¼ P$ stands for a random variable $â‡x$ having a distribution $P$.

~~~
For discrete variables, the probability that $â‡x$ takes a value $x$ is denoted as
$P(x)$ or explicitly as $P(â‡x = x)$. All probabilities are non-negative, and the sum
of the probabilities of all possible values of $â‡x$ is $âˆ‘_x P(â‡x=x) = 1$.

~~~
For continuous variables, the probability that the value of $â‡x$ lies in the interval
$[a, b]$ is given by $âˆ«_a^b p(x)\d x$, where $p(x)$ is the _probability density
function_, which is always non-negative and integrates to 1 over the range of
all values of $â‡x$.

---
style: .katex-display { margin: 0.8em 0 }
# Joint, Conditional, Marginal Probability

![w=44%,f=right](joint_probability.svgz)

For two random variables, a **joint probability distribution** is a distribution
of all possible pairs of outputs (and analogously for more than two):

$$P(â‡x = x_2, â‡y = y_1).$$

~~~
**Marginal distribution** is a distribution of one (orÂ aÂ subset) of the random
variables and can be obtained by summing over the other variable(s):
$$P(â‡x=x_2) = {\small âˆ‘\nolimits}_y P(â‡x = x_2, â‡y = y).$$

~~~
**Conditional distribution** is a distribution of one (or a subset) of the
random variables, given that another event has already occurred:
$$P(â‡x=x_2 | â‡y=y_1) = P(â‡x = x_2, â‡y = y_1) / P(â‡y = y_1).$$

~~~
If $P(â‡x, â‡y) = P(â‡x) â‹… P(â‡y)$ or $P(â‡x | â‡y) = P(â‡x)$, random variables $â‡x$ and $â‡y$ are **independent**.

---
# Random Variables

## Expectation
The expectation of a function $f(x)$ with respect to a discrete probability
distribution $P(x)$ is defined as:
$$ğ”¼_{â‡x âˆ¼ P}[f(x)] â‰ âˆ‘_x P(x)f(x).$$

For continuous variables, it is computed as:
$$ğ”¼_{â‡x âˆ¼ p}[f(x)] â‰ âˆ«_x p(x)f(x)\d x.$$

~~~
If the random variable is obvious from context, we can write only $ğ”¼_P[x]$
or even $ğ”¼[x]$.

~~~
Expectation is linear, i.e.,
$$ğ”¼_â‡x [Î±f(x) + Î²g(x)] = Î±ğ”¼_â‡x [f(x)] + Î²ğ”¼_â‡x [g(x)].$$

---
# Random Variables

## Variance
Variance measures how much the values of a random variable differ from its
mean $Î¼ = ğ”¼[x]$.

$$\begin{aligned}
  \Var(x) &â‰ ğ”¼\left[\big(x - ğ”¼[x]\big)^2\right]\textrm{, or more generally,} \\
  \Var(f(x)) &â‰ ğ”¼\left[\big(f(x) - ğ”¼[f(x)]\big)^2\right].
\end{aligned}$$

~~~
It is easy to see that
$$\Var(x) = ğ”¼\left[x^2 - 2xğ”¼[x] + \big(ğ”¼[x]\big)^2\right] = ğ”¼\left[x^2\right] - \big(ğ”¼[x]\big)^2,$$
because $ğ”¼\big[2xğ”¼[x]\big] = 2(ğ”¼[x])^2$.

~~~
Variance is connected to $E[x^2]$, the **second moment** of a random
variable â€“ it is in fact a **centered** second moment.

---
# Estimators and Bias

An **estimator** is a rule for computing an estimate of a given value, often an
expectation of some random value(s).

~~~
For example, we might estimate _mean_ of a random variable by sampling a value
according to its probability distribution.

~~~
**Bias** of an estimator is the difference of the expected value of the estimator
and the true value being estimated:
$$\textrm{bias} = ğ”¼[\textrm{estimate}] - \textrm{true estimated value}.$$

~~~
If the bias is zero, we call the estimator **unbiased**; otherwise, we call it
**biased**.

~~~
As an example, consider estimating $ğ”¼_P [f(x)]$ by generating a single sample
$x$ from $P$ and returning $f(x)$. Such an estimate is unbiased, because
$ğ”¼[\textrm{estimate}] = ğ”¼_P [f(x)]$, which is exactly the true estimated value.

---
# Estimators and Bias

If we have a sequence of estimates, it might also happen that the bias converges
to zero. Consider the well-known sample estimate of variance. Given $â‡x_1,
\ldots, â‡x_n$â€”independent and identically distributed random variablesâ€”we might
estimate the mean and variance as
$$Î¼Ì‚ = \frac{1}{n} âˆ‘\nolimits_i x_i,~~~ÏƒÌ‚^2 = \frac{1}{n} âˆ‘\nolimits_i (x_i - Î¼Ì‚)^2.$$
~~~
Such an estimate is biased, because $ğ”¼[ÏƒÌ‚^2] = (1 - \frac{1}{n})Ïƒ^2$, but the bias
converges to zero with increasing $n$.

~~~
Also, an unbiased estimator does not necessarily have a small variance â€“ in some
cases, it can have a large variance. So, a biased estimator with a smaller variance
might be preferred.

---
section: SGD
# Gradient Descent

Sometimes it is more practical to search for the best model weights
in an iterative/incremental/sequential fashion. Either because there is too much
data, or the direct optimization is not feasible.

~~~
![w=50%,f=right](gradient_descent.svgz)

Assuming we are minimizing an error function
$$\argmin_{â†’w} E(â†’w),$$
we may use _gradient descent_:
$$â†’w â† â†’w - Î±âˆ‡_{â†’w} E(â†’w)$$

~~~
The constant $Î±$ is called a **learning rate** and specifies the â€œlengthâ€
of a step we perform in every iteration of the gradient descent.

---
# Gradient Descent Variants

Let $â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$ be the training data, and denote
$pÌ‚_\textrm{data}(â†’x, t) â‰ \frac{|\{i: (â†’x, t) = (â†’x_i, t_i)\}|}{N}$.
~~~
Assume that the error function can be computed as an expectation over the dataset:
$$E(â†’w) = ğ”¼_{(â†’x, t)âˆ¼pÌ‚_\textrm{data}} L\big(y(â†’x; â†’w), t\big),\textrm{~~so that~~}
  âˆ‡_{â†’w} E(â†’w) = ğ”¼_{(â†’x, t)âˆ¼pÌ‚_\textrm{data}} âˆ‡_{â†’w} L\big(y(â†’x; â†’w), t\big).$$

~~~
- **(Standard/Batch) Gradient Descent**: We use all training data to compute $âˆ‡_{â†’w} E(â†’w)$.

~~~
- **Stochastic (or Online) Gradient Descent**: We estimate $âˆ‡_{â†’w} E(â†’w)$ using
  a single random example from the training data. Such an estimate is unbiased,
  but very noisy.

$$âˆ‡_{â†’w} E(â†’w) â‰ˆ âˆ‡_{â†’w} L\big(y(â†’x; â†’w), t\big)\textrm{~~for randomly chosen~~}(â†’x, t)\textrm{~~from~~}pÌ‚_\textrm{data}.$$

~~~
- **Minibatch SGD**: Trade-off between gradient descent and SGD â€“ the
  expectation in $âˆ‡_{â†’w} E(â†’w)$ is estimated using $B$ random independent
  examples from the training data.

$$âˆ‡_{â†’w} E(â†’w) â‰ˆ \frac{1}{B} âˆ‘\nolimits_{i=1}^B âˆ‡_{â†’w} L\big(y(â†’x_i; â†’w), t_i\big)
               \textrm{~~for randomly chosen~~}(â†’x_i, t_i)\textrm{~~from~~}pÌ‚_\textrm{data}.$$

---
# Gradient Descent Convergence

Assume that we perform a stochastic gradient descent, using a sequence
of learning rates $Î±_i$, and using a noisy estimate $J(â†’w)$ of the real
gradient $âˆ‡_{â†’w} E(â†’w)$:
$$â†’w_{i+1} â† â†’w_i - Î±_i J(â†’w_i).$$

~~~
It can be proven (under some reasonable conditions; see Robbins and Monro algorithm, 1951) that if
the loss function $L$ is convex and continuous, then SGD converges to the unique
optimum almost surely if the sequence of learning rates $Î±_i$ fulfills the
following conditions:
$$âˆ€i: Î±_i > 0,~~~âˆ‘_i Î±_i = âˆ,~~~âˆ‘_i Î±_i^2 < âˆ.$$

~~~
Note that the third condition implies that $Î±_i â†’ 0$.

~~~
For non-convex loss functions, we can get guarantees of converging to a _local_
optimum only. However, note that finding the global minimum of an arbitrary
function is _at least NP-hard_.

---
# Gradient Descent Convergence

Convex functions mentioned on the previous slide are such that for $â†’u, â†’v$
and real $0 â‰¤ t â‰¤ 1$,
$$f(tâ†’u + (1-t)â†’v) â‰¤ tf(â†’u) + (1-t)f(â†’v).$$

![w=88%,mw=50%,h=center](convex_2d.svgz)![w=66.5%,mw=50%,h=center](convex_3d.svgz)

~~~
A twice-differentiable function of a single variable is convex iff its second
derivative is always non-negative. (For functions of multiple variables,
the Hessian must be positive semi-definite.)

~~~
A local minimum of a convex function is always the unique global minimum.

~~~
Well-known examples of convex functions are $x^2$, $e^x$, $-\log x$, and also
the _sum of squares_.

---
section: LR-SGD
# Solving Linear Regression using SGD

To apply SGD on linear regression, we usually minimize one half of the mean squared error:
$$E(â†’w) = ğ”¼_{(â†’x,t)âˆ¼pÌ‚_\textrm{data}}\big[\tfrac{1}{2} (y(â†’x; â†’w) - t)^2\big] = ğ”¼_{(â†’x,t)âˆ¼pÌ‚_\textrm{data}}\big[\tfrac{1}{2} (â†’x^T â†’w - t)^2\big].$$

~~~
If we also include $L^2$ regularization, we get
$$E(â†’w) = ğ”¼_{(â†’x,t)âˆ¼pÌ‚_\textrm{data}}\big[\tfrac{1}{2} (â†’x^T â†’w - t)^2\big] + \tfrac{Î»}{2}\|â†’w\|^2.$$

~~~
We then estimate the expectation by a minibatch of examples with indices $â†’b$ as
$$âˆ‘_{i âˆˆ â†’b} \frac{1}{|â†’b|} \Big(\tfrac{1}{2} (â†’x_i^T â†’w - t_i)^2\Big) + \tfrac{Î»}{2}\|â†’w\|^2,$$
~~~
which gives us an estimate of a gradient
$$âˆ‡_{â†’w} E(â†’w) â‰ˆ âˆ‘_{i âˆˆ â†’b} \frac{1}{|â†’b|} \Big((â†’x_i^T â†’w - t_i) â†’x_i\Big) + Î»â†’w.$$

---
# Solving Linear Regression using SGD

The computed gradient allows us to formulate the following algorithm for solving
linear regression with minibatch SGD.

~~~
<div class="algorithm">

**Input**: Dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$), learning rate $Î± âˆˆ â„^+$, $L^2$ strength $Î» âˆˆ â„$.<br>
**Output**: Weights $â†’w âˆˆ â„^D$ which hopefully minimize the regularized MSE of linear regression.

- $â†’w â† 0$ or we initialize $â†’w$ randomly
~~~
- repeat until convergence (or until our patience runs out):
  - sample a minibatch of examples with indices $â†’b$
~~~
    - either uniformly randomly,
~~~
    - or we may want to process all training instances before repeating them,
      which can be implemented by generating a random permutation and then
      splitting it into minibatch-sizes chunks
      - the most common option; one pass through the data is called an **epoch**
~~~
  - $â†’w â† â†’w - Î± âˆ‘_{i âˆˆ â†’b} \tfrac{1}{|â†’b|} \big((â†’x_i^Tâ†’w-t_i)â†’x_i\big) - Î±Î»â†’w$
</div>

---
section: Features
# Features

Recall that the _input_ instance values are usually the raw observations and are
given. However, we might extend them suitably before running a machine learning
algorithm, especially if the algorithm is linear or otherwise limited and cannot
represent an arbitrary function. Such instance representations are called _features_.

~~~
We already saw this in the example from the previous lecture, where even if
our training examples were $x$ and $t$, we performed the linear regression
using features $(x^0, x^1, â€¦, x^M)$:
![w=40%,h=center](../01/sin_lr.svgz)

---
# Features

Generally, it would be best if we have machine learning algorithms processing
only the raw inputs. However, many algorithms are capable of representing
only a limited set of functions (for eg, linear ones), and in that case,
_feature engineering_ plays a major part in the final model performance.
Feature engineering is the process of constructing features from raw inputs.

Commonly used features are:
~~~
- **polynomial features** of degree $p$: Given features $(x_1, x_2, â€¦, x_D)$, we
  might consider _all_ products of $p$ input values. Therefore, polynomial
  features of degree 2 would consist of $x_i^2 \,âˆ€i$ and of $x_i x_j \,âˆ€iâ‰ j$.

~~~
- **categorical one-hot features**: Assume, for example, that a day in a week is
  represented in the input as an integer value of 1 to 7, or a breed of a dog is
  expressed as an integer value of 0 to 366.
~~~
  Using these integral values as an input to linear regression makes little sense
  â€“ instead, it might be better to learn weights for individual days in a week or
  for individual dog breeds.
~~~
  We might therefore represent input classes by binary indicators for every
  class, giving rise to a **one-hot** representation, where an input integral value
  $1 â‰¤ v â‰¤ L$ is represented as $L$ binary values, which are all zero except for
  the $v$-th one, which is one.

