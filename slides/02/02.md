title: NPFL129, Lecture 2
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Linear Regression II, SGD

## Milan Straka

### October 12, 2020

---
section: Refresh
# Linear Regression

Given an input value $→x ∈ ℝ^D$, **linear regression** computes predictions as:
$$y(→x; →w, b) = x_1 w_1 + x_2 w_2 + … + x_D w_D + b = ∑_{i=1}^D x_i w_i + b = →x^T →w + b.$$
The _bias_ $b$ can be considered one of the _weights_ $→w$ if convenient.

~~~
We train the weights by minimizing an **error function** between the real target
values and their predictions, notably _sum of squares_:
$$\frac{1}{2} ∑_{i=1}^N \big(y(→x_i; →w) - t_i\big)^2$$

~~~
There are several ways how to minimize it, but in our case, there exists an
explicit solution:
$$→w = (⇉X^T⇉X)^{-1}⇉X^T→t.$$

---
# Linear Regression Example

Assume our input vectors comprise of $→x = (x^0, x^1, …, x^M)$, for $M ≥ 0$.

![w=60%,h=center](../01/sin_lr.svgz)

---
# Linear Regression Example

To plot the error, the _root mean squared error_
$\operatorname{RMSE}=\sqrt{\operatorname{MSE}}$ is frequently used.

![w=60%,f=right](../01/sin_errors.svgz)
~~~

The displayed error nicely illustrates two main challenges in machine learning:
- _underfitting_
- _overfitting_

---
section: Regularization
# Model Capacity
We can control whether a model underfits or overfits by modifying its _capacity_.
~~~
- representational capacity
- effective capacity

~~~
![w=80%,h=center](generalization_error.svgz)

---
# Linear Regression Overfitting

Note that employing more data also usually alleviates overfitting (the relative
capacity of the model is decreased).

![w=100%](sin_overfitting.svgz)

---
# Regularization

**Regularization** in a broad sense is any change in a machine learning
algorithm that is designed to _reduce generalization error_ (but not necessarily
its training error).

~~~
$L_2$ regularization (also called weighted decay) penalizes models
with large weights:

$$\frac{1}{2} ∑_{i=1}^N \big(y(→x_i; →w) - t_i\big)^2 + \frac{λ}{2} \|→w\|^2$$

![w=66%](sin_regularization.svgz)
~~~
![w=32.5%](sin_regularization_ablation.svgz)

---
# Regularizing Linear Regression

In matrix form, regularized sum of squares error for linear regression amounts
to
$$\tfrac{1}{2} \|⇉X→w - →t\|^2 + \tfrac{λ}{2} \|→w\|^2.$$

When repeating the same calculation as in the unregularized case, we arrive at
$$(⇉X^T⇉X + λ⇉I)→w = ⇉X^T→t,$$
where $⇉I$ is an identity matrix.

~~~
<div class="algorithm">

**Input**: Dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ ℝ^N$), constant $λ ∈ ℝ^+$.<br>
**Output**: Weights $→w ∈ ℝ^D$ minimizing MSE of regularized linear regression.

- $→w ← (⇉X^T⇉X + λ⇉I)^{-1}⇉X^T→t.$
</div>

---
section: Hyperparameters
# Choosing Hyperparameters

_Hyperparameters_ are not adapted by the learning algorithm itself.

Usually a **validation set** or **development set** is used to
estimate the generalization error, allowing to update hyperparameters accordingly.
If there is not enough data (well, there is **always** not enough data),
more sophisticated approaches can be used.

~~~
So far, we have seen two hyperparameters, $M$ and $λ$.

~~~
![w=88%,mw=50%,h=center](../01/sin_errors.svgz)![w=87%,mw=50%,h=left](sin_regularization_ablation.svgz)


---
section: SVD Solution
# Linear Regression

When training a linear regression mode, we minimized the sum of squares error
function by computing its gradient (partial derivatives with respect to all weights)
and found solution when it is equal to zero, arriving at the following equation
for optimal weights:
$$⇉X^T⇉X→w = ⇉X^T →t.$$

~~~
If $⇉X^T ⇉X$ is regular, we can invert it and compute the weights as $→w = (⇉X^T⇉X)^{-1}⇉X^T→t$.

~~~
If you recall that $\operatorname{rank}(⇉X) = \operatorname{rank}(⇉X^T →X)$,
matrix $⇉X^T ⇉X ∈ ℝ^{D×D}$ is regular if and only if $⇉X$ has rank $D$, which is
equivalent to the columns of $⇉X$ being linearly independent.

---
class: dbend
# SVD Solution of Linear Regression

Now consider the case that $⇉X^T ⇉X$ is singular. We will show that
$⇉X^T⇉X→w = ⇉X^T →t$ is still solvable, but it does not have a unique
solution. Our goal in this case will be to find the $→w$ with minimum $\|→w\|^2$
fulfilling the equation.

~~~
We now consider _singular value decomposition (SVD)_ of $⇉X$, writing $⇉X = ⇉U ⇉Σ ⇉V^T$,
where
- $⇉U ∈ ℝ^{N×N}$ is an orthogonal matrix, i.e., $→u_i^T →u_j = [i=j]$,
- $⇉Σ ∈ ℝ^{N×D}$ is a diagonal matrix,
- $⇉V ∈ ℝ^{D×D}$ is again an orthogonal matrix.

~~~
Assuming the diagonal matrix $⇉Σ$ has rank $r$, we can write it as
$$⇉Σ = \begin{bmatrix} ⇉Σ_r & ⇉0 \\ ⇉0 & ⇉0 \end{bmatrix},$$
where $⇉Σ_r∈ℝ^{r×r}$ is a regular diagonal matrix.
~~~
Denoting $⇉U_r$ and $⇉V_r$ the matrix of first $r$ columns of $⇉U$ and $⇉V$, respectively,
we can write $⇉X = ⇉U_r ⇉Σ_r ⇉V_r^T$.

---
class: dbend
# SVD Solution of Linear Regression

Using the decomposition $⇉X = ⇉U_r ⇉Σ_r ⇉V_r^T$, we can rewrite the goal equation
as
$$⇉V_r ⇉Σ_r^T ⇉U_r^T ⇉U_r ⇉Σ_r ⇉V_r^T →w = ⇉V_r ⇉Σ_r^T ⇉U_r^T →t.$$

~~~
A transposition of an orthogonal matrix is its inverse. Therefore, our
submatrix $⇉U_r$ fulfils that $⇉U_r^T ⇉U_r = ⇉I$, because $⇉U_r^T ⇉U_r$
is a top left submatrix of $⇉U^T ⇉U$. Analogously, $⇉V_r^T ⇉V_r = ⇉I$.

~~~
We therefore simplify the goal equation to
$$⇉Σ_r ⇉Σ_r ⇉V_r^T →w = ⇉Σ_r ⇉U_r^T →t$$

~~~
Because the diagonal matrix $⇉Σ_r$ is regular, we can divide by it and obtain
$$⇉V_r^T →w = ⇉Σ_r^{-1} ⇉U_r^T →t.$$

---
class: dbend
# SVD Solution of Linear Regression

We have $⇉V_r^T →w = ⇉Σ_r^{-1} ⇉U_r^T →t$. If he original matrix $⇉X^T ⇉X$ was
regular, then $r=D$ and $⇉V_r$ is a square regular orthogonal matrix, in which case
$$→w = ⇉V_r ⇉Σ_r^{-1} ⇉U_r^T →t.$$

~~~
If we denote $⇉Σ^+ ∈ ℝ^{D×N}$ the diagonal matrix with $Σ_{i,i}^{-1}$ on
diagonal, we can rewrite to
$$→w = ⇉V ⇉Σ^+ ⇉U^T →t.$$

~~~
Now if $r < D$, $⇉V_r^T →w = →y$ is undetermined and has infinitely many
solutions. To find the one with smallest norm $\|→w\|$, consider the full
product $⇉V^T→w$. Because $⇉V$ is orthogonal, $\|⇉V^T→w\|=\|→w\|$, and it is
sufficient to find $→w$ with smallest $\|⇉V^T→w\|$.
~~~
We know that the first $r$ elements of $\|⇉V^T→w\|$ are fixed by the above equation
– the smallest $\|⇉V^T→w\|$ can be therefore obtained by setting the last $D-r$
elements to zero.
~~~
Finally, we note that $⇉Σ^+ ⇉U^T →t$ is exactly $⇉Σ_r^{-1} ⇉U_r^T →t$ padded
with $D-r$ zeros, obtaining the same solution $→w = ⇉V ⇉Σ^+ ⇉U^T →t$.

---
class: dbend
# SVD Solution of Linear Regression and Pseudoinverses

The solution to a linear regression with sum of squares error function is
tightly connected to matrix pseudoinverses. If a matrix $⇉X$ is singular or
rectangular, it does not have an exact inverse, and $⇉X→w=→b$ does not
have an exact solution. 

~~~
However, we can consider the so-called _Moore-Penrose pseudoinverse_
$$⇉X^+ ≝ ⇉V ⇉Σ^+ ⇉U^T$$
to be the closest approximation to an inverse, in the sense that we can find
the best solution (with smallest MSE) to the equation $⇉X→w=→b$ by setting $→w=⇉X^+ →b$.

~~~
Alternatively, we can define the pseudoinverse of a matrix $⇉X$ as
$$⇉X^+ = \argmin_{⇉Y∈ℝ^{D×N}} \big\|⇉X ⇉Y - ⇉I_N\big\|_F = \argmin_{⇉Y∈ℝ^{N×D}} \big\|⇉Y ⇉X - ⇉I_D\big\|_F$$
which can be verified to be the same as our SVD formula.

---
section: Random Variables
# Random Variables
A random variable $⁇x$ is a result of a random process. It can be discrete or
continuous.

~~~
## Probability Distribution
A probability distribution describes how likely are individual values a random
variable can take.

The notation $⁇x ∼ P$ stands for a random variable $⁇x$ having a distribution $P$.

~~~
For discrete variables, the probability that $⁇x$ takes a value $x$ is denoted as
$P(x)$ or explicitly as $P(⁇x = x)$. All probabilities are non-negative and sum
of probabilities of all possible values of $⁇x$ is $∑_x P(⁇x=x) = 1$.

~~~
For continuous variables, the probability that the value of $⁇x$ lies in the interval
$[a, b]$ is given by $∫_a^b p(x)\d x$.

---
# Random Variables

## Expectation
The expectation of a function $f(x)$ with respect to discrete probability
distribution $P(x)$ is defined as:
$$𝔼_{⁇x ∼ P}[f(x)] ≝ ∑_x P(x)f(x)$$

For continuous variables it is computed as:
$$𝔼_{⁇x ∼ p}[f(x)] ≝ ∫_x p(x)f(x)\d x$$

~~~
If the random variable is obvious from context, we can write only $𝔼_P[x]$
or even $𝔼[x]$.

~~~
Expectation is linear, i.e.,
$$𝔼_⁇x [αf(x) + βg(x)] = α𝔼_⁇x [f(x)] + β𝔼_⁇x [g(x)]$$

---
# Random Variables

## Variance
Variance measures how much the values of a random variable differ from its
mean $μ = 𝔼[x]$.

$$\begin{aligned}
  \Var(x) &≝ 𝔼\left[\big(x - 𝔼[x]\big)^2\right]\textrm{, or more generally} \\
  \Var(f(x)) &≝ 𝔼\left[\big(f(x) - 𝔼[f(x)]\big)^2\right]
\end{aligned}$$

~~~
It is easy to see that
$$\Var(x) = 𝔼\left[x^2 - 2x𝔼[x] + \big(𝔼[x]\big)^2\right] = 𝔼\left[x^2\right] - \big(𝔼[x]\big)^2,$$
because $𝔼\big[2x𝔼[x]\big] = 2(𝔼[x])^2$.

~~~
Variance is connected to $E[x^2]$, a _second moment_ of a random
variable – it is in fact a _centered_ second moment.

---
# Estimators and Bias

An _estimator_ is a rule for computing an estimate of a given value, often an
expectation of some random value(s).

~~~
For example, we might estimate _mean_ of random variable by sampling a value
according to its probability distribution.

~~~
_Bias_ of an estimator is the difference of the expected value of the estimator
and the true value being estimated:
$$\textrm{bias} = 𝔼[\textrm{estimate}] - \textrm{true estimated value}.$$

~~~
If the bias is zero, we call the estimator _unbiased_, otherwise we call it
_biased_.

---
# Estimators and Bias

If we have a sequence of estimates, it also might happen that the bias converges
to zero. Consider the well known sample estimate of variance. Given $⁇x_1,
\ldots, ⁇x_n$ independent and identically distributed random variables, we might
estimate mean and variance as
$$μ̂ = \frac{1}{n} ∑\nolimits_i x_i,~~~σ̂^2 = \frac{1}{n} ∑\nolimits_i (x_i - μ̂)^2.$$
~~~
Such estimate is biased, because $𝔼[σ̂^2] = (1 - \frac{1}{n})σ^2$, but the bias
converges to zero with increasing $n$.

~~~
Also, an unbiased estimator does not necessarily have small variance – in some
cases it can have large variance, so a biased estimator with smaller variance
might be preferred.

---
section: SGD
# Gradient Descent

Sometimes it is more practical to search for the best model weights
in an iterative/incremental/sequential fashion. Either because there is too much
data, or the direct optimization is not feasible.

~~~
![w=50%,f=right](gradient_descent.svgz)

Assuming we are minimizing an error function
$$\argmin_→w E(→w),$$
we may use _gradient descent_:
$$→w ← →w - α∇_→wE(→w)$$

~~~
The constant $α$ is called a _learning rate_ and specifies the “length”
of a step we perform in every iteration of the gradient descent.

---
# Gradient Descent Variants

Consider an error function computed as an expectation over the dataset:
$$∇_→w E(→w) = ∇_→w 𝔼_{(→x, t)∼p̂_\textrm{data}} L\big(y(→x; →w), t\big).$$

~~~
- **(Regular) Gradient Descent**: We use all training data to compute $∇_→w E(→w)$
  exactly.

~~~
- **Online (or Stochastic) Gradient Descent**: We estimate $∇_→w E(→w)$ using
  a single random example from the training data. Such an estimate is unbiased,
  but very noisy.

$$∇_→w E(→w) ≈ ∇_→w L\big(y(→x; →w), t\big)\textrm{~~for randomly chosen~~}(→x, t)\textrm{~~from~~}p̂_\textrm{data}.$$

~~~
- **Minibatch SGD**: The minibatch SGD is a trade-off between gradient descent
  and SGD – the expectation in $∇_→w E(→w)$ is estimated using $m$ random independent
  examples from the training data.

$$∇_→w E(→w) ≈ \frac{1}{m} ∑_{i=1}^m ∇_→w L\big(y(→x_i; →w), t_i\big)
               \textrm{~~for randomly chosen~~}(→x_i, t_i)\textrm{~~from~~}p̂_\textrm{data}.$$

---
# Gradient Descent Convergence

Assume that we perform a stochastic gradient descent, using a sequence
of learning rates $α_i$, and using a noisy estimate $J(→w)$ of the real
gradient $∇_→w E(→w)$:
$$→w_{i+1} ← →w_i - α_i J(→w_i).$$

~~~
It can be proven (under some reasonable conditions; see Robbins and Monro algorithm, 1951) that if
the loss function $L$ is convex and continuous, then SGD converges to the unique
optimum almost surely if the sequence of learning rates $α_i$ fulfills the
following conditions:
$$∀i: α_i > 0,~~~∑_i α_i = ∞,~~~∑_i α_i^2 < ∞.$$

~~~
Note that the third condition implies that $α_i → 0$.

~~~
For non-convex loss functions, we can get guarantees of converging to a _local_
optimum only. However, note that finding a global minimum of an arbitrary
function is _at least NP-hard_.

---
# Gradient Descent Convergence

Convex functions mentioned on a previous slide are such that for $x_1, x_2$
and real $0 ≤ t ≤ 1$,
$$f(tx_1 + (1-t)x_2) ≤ tf(x_1) + (1-t)f(x_2).$$

![w=90%,mw=50%,h=center](convex_2d.svgz)![w=68%,mw=50%,h=center](convex_3d.svgz)

~~~
A twice-differentiable function is convex if its second derivative is always
non-negative.

~~~
A local minimum of a convex function is always the unique global minimum.

~~~
Well-known examples of convex functions are $x^2$, $e^x$ and $-\log x$.

---
section: LR-SGD
# Solving Linear Regression using SGD

To apply SGD on linear regression, we minimize the sum of squares error function
$$E(→w) = 𝔼_{(→x,t)∼p̂_\textrm{data}}\big[\tfrac{1}{2} (y(→x; →w) - t)^2\big] = 𝔼_{(→x,t)∼p̂_\textrm{data}}\big[\tfrac{1}{2} (→x^T →w - t)^2\big].$$

~~~
If we also include $L_2$ regularization, we get
$$E(→w) = 𝔼_{(→x,t)∼p̂_\textrm{data}}\big[\tfrac{1}{2} (→x^T →w - t)^2\big] + \tfrac{λ}{2}\|→w\|^2.$$

~~~
We then estimate the expectation by a minibatch $→b$ of examples as
$$∑_{i ∈ →b} \frac{1}{|→b|} \Big(\tfrac{1}{2} (→x_i^T →w - t_i)^2\Big) + \tfrac{λ}{2}\|→w\|^2,$$
which gives us an estimate of a gradient
$$∇_→w E(→w) ≈ ∑_{i ∈ →b} \frac{1}{|→b|} \Big((→x_i^T →w - t_i) →x_i\Big) + λ→w.$$

---
# Solving Linear Regression using SGD

The computed gradient allows us to formulate the following algorithm for solving
linear regression with SGD.

~~~
<div class="algorithm">

**Input**: Dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ ℝ^N$), learning rate $α ∈ ℝ^+$, $L_2$ strength $λ ∈ ℝ$.<br>
**Output**: Weights $→w ∈ ℝ^D$ which hopefully minimize regularized MSE of linear regression.

- $→w ← 0$
- repeat until convergence (or until our patience runs out):
  - sample a batch $→b$ (either uniformly randomly; or we may want to process
    all training instances before repeating them, which can be implemented by
    generating a random permutation and then splitting it to batch-sizes chunks)
  - $→w ← →w - α ∑_{i ∈ →b} \tfrac{1}{|→b|} \big((→x_i^T→w-t_i)→x_i\big) - αλ→w$
</div>

---
section: Features
# Features

Recall that the _input_ instance values are usually the raw observations and are
given. However, we might extend them suitably before running a machine learning
algorithm, especially if the algorithm is linear or otherwise limited and cannot
represent arbitrary function. Such instance representations is called _features_.

~~~
We already saw this in the example from the previous lecture, where even if
our training examples were $x$ and $t$, we performed the linear regression
using features $(x^0, x^1, …, x^M)$:
![w=40%,h=center](../01/sin_lr.svgz)

---
# Features

Generally, it would be best if we have machine learning algorithms processing
only the raw inputs. However, many algorithms are capable of representing
only a limited set of functions (for example linear ones), and in that case,
_feature engineering_ plays a major part in the final model performance.
Feature engineering is a process of constructing features from raw inputs.

Commonly used features are:
~~~
- **polynomial features** of degree $p$: Given features $(x_1, x_2, …, x_D)$, we
  might consider _all_ products of $p$ input values. Therefore, polynomial
  features of degree 2 would consist of $x_i^2 \,∀i$ and of $x_i x_j \,∀i≠j$.

~~~
- **categorical one-hot features**: Assume for example that a day in a week is
  represented on the input as an integer value of 1 to 7, or a breed of a dog is
  expressed as an integer value of 0 to 366.
~~~
  Using these integral values as input to linear regression makes little sense
  – instead it might be better to learn weights for individual days in a week or
  for individual dog breeds.
~~~
  We might therefore represent input classes by binary indicators for every
  class, giving rise to **one-hot** representation, where input integral value
  $1 ≤ v ≤ L$ is represented as $L$ binary values, which are all zero except for
  the $v$-th one, which is one.

