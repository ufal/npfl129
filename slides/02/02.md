title: NPFL129, Lecture 1
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Introduction to Machine Learning

## Milan Straka

### October 07, 2019

---
section: SGD
# Gradient Descent

Sometimes it is more practical to search for the best model weights
in an iterative/incremental/sequential fashion. Either because there is too much
data, or the direct optimization is not feasible.

~~~
![w=53%,f=right](gradient_descent.pdf)

Assuming we are minimizing an error function
$$\argmin_â†’w E(â†’w),$$
we may use _gradient descent_:
$$â†’w â† â†’w - Î±âˆ‡_â†’wE(â†’w)$$

~~~
The name _stochastic gradient descent_ or _online_ gradient descent
is a variant when we iteratively update the weights one training example
at a time.

---
# Gradient Descent of Linear Regression

For linear regression and sum of squares, we can update the weights as
$$â†’w â† â†’w - Î±âˆ‡_â†’wE(â†’w) = â†’w - Î±(â†’x^Tâ†’w-t)â†’x.$$

~~~
<div class="algorithm">

**Input**: Dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$), learning rate $Î± âˆˆ â„^+$.<br>
**Output**: Weights $â†’w âˆˆ â„^D$ which hopefully minimize MSE of linear regression.

- $â†’w â† 0$
- for $i = 1, \ldots, n$:
  - $â†’w â† â†’w - Î±(â†’x_i^Tâ†’w-t_i)â†’x_i.$
</div>

---
section: Classification
# Binary Classification

Binary classification is a classification in two classes.

~~~
To extend linear regression to binary classification, we might seek
a _threshold_ and the classify an input as negative/positive
depending whether $â†’x^Tâ†’w$ is smaller/larger than a given threshold.

~~~
The specific threshold value does not really matter, because the
_bias_ parameter acts as a trainable threshold anyway.

---
# Perceptron

The perceptron algorithm is probably the oldest one for training
weights of a binary classification. It can be proven that it finds
weights $â†’w$ classifying training set with 100% accuracy if such
weights exist. Such a dataset is called _linearly separable_.

~~~
<div class="algorithm">

**Input**: Linearly separable dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{-1, +1\}$).<br>
**Output**: Weights $â†’w âˆˆ â„^D$ such that $t_i â†’x_i^Tâ†’w > 0$ for all $i$.

- $â†’w â† 0$
- until all examples are classified correctly, process example $i$:
  - $y â† â†’w^Tâ†’x_i$
  - if $t_i y < 0$ (incorrectly classified example):
    - $â†’w â† â†’w + t_i â†’x_i$
</div>

---
# Logistic Regression

Extends perceptron by
- allowing more than two classes
~~~
- producing full probability distribution.

~~~
For binary case, it employs a formula
$$\begin{aligned}
  P(C_1 | â†’x) &= Ïƒ(â†’x^t â†’w + â†’b) \\
  P(C_0 | â†’x) &= 1 - P(C_1 | â†’x),
\end{aligned}$$
where $Ïƒ$ is a _sigmoid function_
$$Ïƒ(x) = \frac{1}{1+e^{-x}}.$$

~~~
Can be trained using an SGD algorithm.

---
section: Backup
# Random Variables
A random variable $â‡x$ is a result of a random process. It can be discrete or
continuous.

~~~
## Probability Distribution
A probability distribution describes how likely are individual values a random
variable can take.

The notation $â‡x âˆ¼ P$ stands for a random variable $â‡x$ having a distribution $P$.

~~~
For discrete variables, the probability that $â‡x$ takes a value $x$ is denoted as
$P(x)$ or explicitly as $P(â‡x = x)$.

~~~
For continuous variables, the probability that the value of $â‡x$ lies in the interval
$[a, b]$ is given by $âˆ«_a^b p(x)\d x$.

---
# Random Variables

## Expectation
The expectation of a function $f(x)$ with respect to discrete probability
distribution $P(x)$ is defined as:
$$ğ”¼_{â‡x âˆ¼ P}[f(x)] â‰ âˆ‘_x P(x)f(x)$$

For continuous variables it is computed as:
$$ğ”¼_{â‡x âˆ¼ p}[f(x)] â‰ âˆ«_x p(x)f(x)\d x$$

~~~
If the random variable is obvious from context, we can write only $ğ”¼_P[x]$
of even $ğ”¼[x]$.

~~~
Expectation is linear, i.e.,
$$ğ”¼_â‡x [Î±f(x) + Î²g(x)] = Î±ğ”¼_â‡x [f(x)] + Î²ğ”¼_â‡x [g(x)]$$

---
# Random Variables

## Variance
Variance measures how much the values of a random variable differ from its
mean $Î¼ = ğ”¼[x]$.

$$\begin{aligned}
  \Var(x) &â‰ ğ”¼\left[\big(x - ğ”¼[x]\big)^2\right]\textrm{, or more generally} \\
  \Var(f(x)) &â‰ ğ”¼\left[\big(f(x) - ğ”¼[f(x)]\big)^2\right]
\end{aligned}$$

~~~
It is easy to see that
$$\Var(x) = ğ”¼\left[x^2 - 2xğ”¼[x] + \big(ğ”¼[x]\big)^2\right] = ğ”¼\left[x^2\right] - \big(ğ”¼[x]\big)^2.$$

~~~
Variance is connected to $E[x^2]$, a _second moment_ of a random
variable â€“ it is in fact a _centered_ second moment.

---
# Common Probability Distributions
## Bernoulli Distribution
The Bernoulli distribution is a distribution over a binary random variable.
It has a single parameter $Ï† âˆˆ [0, 1]$, which specifies the probability of the random
variable being equal to 1.

~~~
$$\begin{aligned}
  P(x) &= Ï†^x (1-Ï†)^{1-x} \\
  ğ”¼[x] &= Ï† \\
  \Var(x) &= Ï†(1-Ï†)
\end{aligned}$$

~~~
## Categorical Distribution
Extension of the Bernoulli distribution to random variables taking one of $k$ different
discrete outcomes. It is parametrized by $â†’p âˆˆ [0, 1]^k$ such that $âˆ‘_{i=1}^{k} p_{i} = 1$.
$$\begin{aligned}
  P(â†’x) &= âˆ\nolimits_i^k p_i^{x_i} \\
  ğ”¼[x_i] &= p_i, \Var(x_i) = p_i(1-p_i) \\
\end{aligned}$$

---
# Information Theory

## Self Information

Amount of _surprise_ when a random variable is sampled.
~~~
- Should be zero for events with probability 1.
~~~
- Less likely events are more surprising.
~~~
- Independent events should have _additive_ information.

~~~
$$I(x) â‰ -\log P(x) = \log \frac{1}{P(x)}$$

~~~
## Entropy

Amount of _surprise_ in the whole distribution.
$$H(P) â‰ ğ”¼_{â‡xâˆ¼P}[I(x)] = -ğ”¼_{â‡xâˆ¼P}[\log P(x)]$$

~~~
- for discrete $P$: $H(P) = -âˆ‘_x P(x) \log P(x)$
- for continuous $P$: $H(P) = -âˆ« P(x) \log P(x)\,\mathrm dx$

---
# Information Theory

## Cross-Entropy

$$H(P, Q) â‰ -ğ”¼_{â‡xâˆ¼P}[\log Q(x)]$$

~~~
- Gibbs inequality
    - $H(P, Q) â‰¥ H(P)$
    - $H(P) = H(P, Q) â‡” P = Q$
~~~
    - Proof: Using Jensen's inequality, we get
      $$âˆ‘_x P(x) \log \frac{Q(x)}{P(x)} â‰¤ \log âˆ‘_x P(x) \frac{Q(x)}{P(x)} = \log âˆ‘_x Q(x) = 0.$$
~~~
    - Corollary: For a categorical distribution with $n$ outcomes, $H(P) â‰¤ \log n$,
    because for $Q(x) = 1/n$ we get $H(P) â‰¤ H(P, Q) = -âˆ‘_x P(x) \log Q(x) = \log n.$
~~~
- generally $H(P, Q) â‰  H(Q, P)$

---
# Information Theory

## Kullback-Leibler Divergence (KL Divergence)

Sometimes also called _relative entropy_.

$$D_\textrm{KL}(P || Q) â‰ H(P, Q) - H(P) = ğ”¼_{â‡xâˆ¼P}[\log P(x) - \log Q(x)]$$

~~~
- consequence of Gibbs inequality: $D_\textrm{KL}(P || Q) â‰¥ 0$
- generally $D_\textrm{KL}(P || Q) â‰  D_\textrm{KL}(Q || P)$

---
# Nonsymmetry of KL Divergence

![w=100%,v=middle](kl_nonsymmetry.pdf)

---
# Common Probability Distributions
## Normal (or Gaussian) Distribution
Distribution over real numbers, parametrized by a mean $Î¼$ and variance $Ïƒ^2$:
$$ğ“(x; Î¼, Ïƒ^2) = \sqrt{\frac{1}{2Ï€Ïƒ^2}} \exp \left(-\frac{(x - Î¼)^2}{2Ïƒ^2}\right)$$

For standard values $Î¼=0$ and $Ïƒ^2=1$ we get $ğ“(x; 0, 1) = \sqrt{\frac{1}{2Ï€}} e^{-\frac{x^2}{2}}$.

![w=45%,h=center](normal_distribution.pdf)

---
# Why Normal Distribution

## Central Limit Theorem
The sum of independent identically distributed random variables
with finite variance converges to normal distribution.

~~~
## Principle of Maximum Entropy
Given a set of constraints, a distribution with maximal entropy fulfilling the
constraints can be considered the most general one, containing as little
additional assumptions as possible.

~~~
Considering distributions with a given mean and variance, it can be proven
(using variational inference) that such a distribution with _maximal entropy_
is exactly the normal distribution.
