title: NPFL129, Lecture 2
class: title, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Linear Regression II, SGD

## JindÅ™ich LibovickÃ½ <small>(reusing materials by Milan Straka)</small>

### October 7, 2024

---
class: middle
# Today's Lecture Objectives

After this lecture you should be able to

- Reason about **overfitting** in terms of **model capacity**.

- Use **$L^2$-regularization** to control model capacity.

- Explain what the difference between **parameters and hyperparameters** is.

- Tell what the **basic probability concepts** are (joint, marginal,
  conditional probability; expected value, mean, variance).

- Mathematically describe and implement the **stochastic gradient descent**
  algorithm.

- Use both **numerical and categorical features** in linear regression.

---
section: Refresh
class: section
# Refresh from last week

---
# Linear Regression

Given an input value $â†’x âˆˆ â„^D$, **linear regression** computes predictions as:
$$y(â†’x; â†’w, b) = x_1 w_1 + x_2 w_2 + â€¦ + x_D w_D + b = âˆ‘_{i=1}^D x_i w_i + b = â†’x^T â†’w + b.$$
The _bias_ $b$ can be considered one of the _weights_ $â†’w$ if convenient.

~~~
We learn the weights by minimizing an **error function** between the real target
values and their predictions, notably _sum of squares_:
$$\frac{1}{2} âˆ‘_{i=1}^N \big(y(â†’x_i; â†’w) - t_i\big)^2$$

~~~
Various minimization approaches exist, but for linear regression an explicit solution exists:
$$â†’w = (â‡‰X^Tâ‡‰X)^{-1}â‡‰X^Tâ†’t.$$

---
# Linear Regression Example

Assume we want to predict a $t âˆˆ â„$ for a given $x âˆˆ â„$. If we train
the linear regression with â€œrawâ€ input vectors $â†’x = (x)$, only straight lines
could be modeled.

![w=60%,f=right](../01/sin_lr.svgz)

However, if we consider input vectors $â†’x = (x^0, x^1, â€¦, x^M)$ for a given
$M â‰¥ 0$, the linear regression can model polynomials of degree $M$,
because the prediction is then computed as
$$w_0 x^0 + w_1 x^1 + â€¦ + w_M x^M.$$

The weights are coefficients of a polynomial of degree $M$.

---
# Linear Regression Example

To plot the error, the _root mean squared error_
$\operatorname{RMSE}=\sqrt{\operatorname{MSE}}$ is frequently used.

![w=60%,f=right](../01/sin_errors.svgz)
~~~

The displayed error illustrates two main challenges in machine learning:
- _underfitting_
- _overfitting_

---
section: Regularization
class: section
# Regularization

---
# Model Capacity
We can control whether a model underfits or overfits by modifying its **capacity**.
~~~
- representational capacity
- effective capacity

~~~
![w=80%,h=center](generalization_error.svgz)

---
# Linear Regression Overfitting

Employing more data usually alleviates overfitting (the relative
capacity of the model is decreased).

![w=100%](sin_overfitting.svgz)

---
# Regularization

**Regularization** = any change that is designed to _reduce generalization error_ (but not necessarily
its training error) in a machine learning algorithm.

We already saw that **limiting model capacity** can work as regularization.

---
# L2 Regularization

$L^2$-regularization: one of the oldest regularization techniques; tries
to prefer â€œsimplerâ€ models by endorsing models with **smaller weights**.

~~~
**$\boldsymbol{L^2}$-regularization** (also called **weight decay**) penalizes
models with large weights by utilizing the following error function:

$$\frac{1}{2} âˆ‘_{i=1}^N \big(y(â†’x_i; â†’w) - t_i\big)^2 + \textcolor{red}{\frac{Î»}{2} \|â†’w\|^2}.$$

~~~
<hr />

The $L^2$-regularization is usually not applied to the _bias_, only to the
â€œproperâ€ weights, because we cannot really overfit via the bias.
~~~
Without penalizing the bias, linear regression with $L^2$-regularization is invariant
to shifts (i.e., adding a constant to all the targets results in the same solution, only with
the bias increased by that constant; if the bias were penalized, this would not
hold).

~~~
We will not explicitly exclude the bias from the
$L^2$-regularization penalty in the slides.

---
# L2 Regularization

![w=25%,f=right](l2_smoothness_data.png)

One way to look at $L^2$-regularization: it promotes smaller
changes of the model (the gradient of linear regression with respect to the
inputs are exactly the weights, i.e., $âˆ‡_{â†’x} y(â†’x; â†’w) = â†’w$).

~~~
Considering the data points on the right, we present mean squared errors
and $L^2$ norms of the weights for three linear regression models:

![w=70%,h=center](l2_smoothness.png)
![w=70%,h=center](l2_smoothness_equations.png)

---
# L2 Regularization

The effect of $L^2$-regularization can be seen as limiting the _effective
capacity_ of the model.

![w=66%,mh=70%,v=bottom](sin_regularization.svgz)
~~~
![w=32.5%,mh=70%,v=bottom](sin_regularization_ablation.svgz)

---
# Regularizing Linear Regression

In a matrix form, the regularized _sum of squares error_ for linear regression amounts
to
$$\tfrac{1}{2} \|â‡‰Xâ†’w - â†’t\|^2 + \tfrac{Î»}{2} \|â†’w\|^2.$$

When repeating the same calculation as in the unregularized case, we arrive at
$$(â‡‰X^Tâ‡‰X + Î»â‡‰I)â†’w = â‡‰X^Tâ†’t,$$
where $â‡‰I$ is an identity matrix.

~~~
<div class="algorithm">

**Input**: Dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$), constant $Î» âˆˆ â„^+$.<br>
**Output**: Weights $â†’w âˆˆ â„^D$ minimizing MSE of regularized linear regression.

- $â†’w â† (â‡‰X^Tâ‡‰X + Î»â‡‰I)^{-1}â‡‰X^Tâ†’t.$
</div>

~~~
The matrix $â‡‰X^Tâ‡‰X + Î»â‡‰I$ is always regular for $Î»>0$ <small>(you can show
that the matrix is positive definite)<small>.
---
section: Hyperparameters
class: section
# Hyperparameters

---
# Choosing Hyperparameters

**Hyperparameters** are not adapted by the learning algorithm itself.

~~~
A **validation set** or **development set** is used to
estimate the generalization error, allowing us to update hyperparameters accordingly.
If there is not enough data (well, there is **always** not enough data),
more sophisticated approaches can be used.

~~~
So far: two hyperparameters, $M$ and $Î»$.

~~~
![w=88%,mw=50%,h=center](../01/sin_errors.svgz)![w=87%,mw=50%,h=left](sin_regularization_ablation.svgz)


---
# What went wrong? (1/5)

![w=60%,h=center](wrong_too_weak.svgz)
~~~
<center>Too small model capacity.</center>

---
# What went wrong? (2/5)

![w=60%,h=center](wrong_poly_overfit.svgz)
~~~
<center>This is overfitting like crazy.

(We increased the capacity by adding polynomial features.)
</center>

---
# What went wrong? (3/5)

![w=60%,h=center](wrong_with_l2.svgz)
~~~
<center>Nothing, this looks pretty good.

(We added L2 regularization to the previous model.)
</center>

---
# What went wrong? (4/5)
![w=60%,h=center](wrong_outlier.svgz)
~~~
<center>The outlier steers the line away.</center>

---
# What went wrong? (5/5)
![w=60%,h=center](wrong_better_outlier.svgz)
~~~
<center>After adding L2 regularization, it is a little better.</center>

---
section: Random Variables
class: section
# Random Variables

---
# Random Variables
A random variable $â‡x$ is a result of a random process, and it can be either
discrete or continuous.

~~~
## Probability Distribution
A probability distribution describes how likely are the individual values that
a random variable can take.

The notation $â‡x âˆ¼ P$ stands for a random variable $â‡x$ having a distribution $P$.

~~~
For discrete variables, the probability that $â‡x$ takes a value $x$ is denoted as
$P(x)$ or explicitly as $P(â‡x = x)$. All probabilities are nonnegative, and the
sum of the probabilities of all possible values of $â‡x$ is $âˆ‘_x P(â‡x=x) = 1$.

~~~
For continuous variables, the probability that the value of $â‡x$ lies in the interval
$[a, b]$ is given by $âˆ«_a^b p(x)\d x$, where $p(x)$ is the _probability density
function_, which is always nonnegative and integrates to 1 over the range of
all values of $â‡x$.

---
style: .katex-display { margin: .8em 0 }
# Joint, Conditional, Marginal Probability

![w=44%,f=right](joint_probability.svgz)

For two random variables, a **joint probability distribution** is a distribution
of all possible pairs of outputs (and analogously for more than two):

$$P(â‡x = x_2, â‡y = y_1).$$

~~~
**Marginal distribution** is a distribution of one (orÂ aÂ subset) of the random
variables and can be obtained by summing over the other variable(s):
$$P(â‡x=x_2) = {\small âˆ‘\nolimits}_y P(â‡x = x_2, â‡y = y).$$

~~~
**Conditional distribution** is a distribution of one (or a subset) of the
random variables, given that another event has already occurred:
$$P(â‡x=x_2 | â‡y=y_1) = P(â‡x = x_2, â‡y = y_1) / P(â‡y = y_1).$$

~~~
If $P(â‡x, â‡y) = P(â‡x) â‹… P(â‡y)$ or $P(â‡x | â‡y) = P(â‡x)$, random variables $â‡x$ and $â‡y$ are **independent**.

---
# Random Variables

## Expectation
The expectation of a function $f(x)$ with respect to a discrete probability
distribution $P(â‡x)$ is defined as:
$$ğ”¼_{â‡x âˆ¼ P}[f(x)] â‰ âˆ‘_x P(x)f(x).$$

For continuous variables, the expectation is computed as:
$$ğ”¼_{â‡x âˆ¼ p}[f(x)] â‰ âˆ«_x p(x)f(x)\d x.$$

~~~
If the random variable is obvious from context, we can write only $ğ”¼_P[x]$,
$ğ”¼_{â‡x}[x]$, or even $ğ”¼[x]$.

~~~
Expectation is linear, i.e., for constants $Î±, Î² âˆˆ â„$:
$$ğ”¼_{â‡x} [Î±f(x) + Î²g(x)] = Î±ğ”¼_{â‡x} [f(x)] + Î²ğ”¼_{â‡x} [g(x)].$$

---
# Random Variables

## Variance
Variance measures how much the values of a random variable differ from its
mean $ğ”¼[x]$.

$$\begin{aligned}
  \Var(x) &â‰ ğ”¼\left[\big(x - ğ”¼[x]\big)^2\right]\textrm{, or more generally,} \\
  \Var_{â‡x âˆ¼ P}(f(x)) &â‰ ğ”¼\left[\big(f(x) - ğ”¼[f(x)]\big)^2\right].
\end{aligned}$$

~~~
It is easy to see that
$$\Var(x) = ğ”¼\left[x^2 - 2xâ‹…ğ”¼[x] + \big(ğ”¼[x]\big)^2\right] = ğ”¼\left[x^2\right] - \big(ğ”¼[x]\big)^2,$$
because $ğ”¼\big[2xâ‹…ğ”¼[x]\big] = 2(ğ”¼[x])^2$.

~~~
Variance is connected to $ğ”¼[x^2]$, the **second moment** of a random
variable â€“ it is in fact a **centered** second moment.

---
section: SGD
class: section
# Stochastic Gradient Descent

---
# Gradient Descent

Sometimes it is more practical to search for the best model weights
in an iterative/incremental/sequential fashion. Either because there is too much
data, or the direct optimization is not feasible.

~~~
![w=50%,f=right](gradient_descent.svgz)

Assuming we are minimizing an error function
$$\argmin_{â†’w} E(â†’w),$$
we may use _gradient descent_:
$$â†’w â† â†’w - Î±âˆ‡_{â†’w} E(â†’w)$$

~~~
The constant $Î±$ is called a **learning rate** and specifies the â€œlengthâ€
of a step we perform in every iteration of the gradient descent.

---
# Gradient Descent Variants

Let $â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$ be the training data, and denote
$pÌ‚_\textrm{data}(â†’x, t) â‰ \frac{|\{i: (â†’x, t) = (â†’x_i, t_i)\}|}{N}$.
~~~
Assume that the error function can be computed as an expectation over the dataset:
$$E(â†’w) = ğ”¼_{(â‡â†’x, â‡t)âˆ¼pÌ‚_\textrm{data}} L\big(y(â†’x; â†’w), t\big),\textrm{~~so that~~}
  âˆ‡_{â†’w} E(â†’w) = ğ”¼_{(â‡â†’x, â‡t)âˆ¼pÌ‚_\textrm{data}} âˆ‡_{â†’w} L\big(y(â†’x; â†’w), t\big).$$

~~~
- **(Standard/Batch) Gradient Descent**: We use all training data to compute $âˆ‡_{â†’w} E(â†’w)$.

~~~
- **Stochastic (or Online) Gradient Descent**: We estimate $âˆ‡_{â†’w} E(â†’w)$ using
  a single random example from the training data. Such an estimate is unbiased,
  but very noisy.

$$âˆ‡_{â†’w} E(â†’w) â‰ˆ âˆ‡_{â†’w} L\big(y(â†’x; â†’w), t\big)\textrm{~~for a randomly chosen~~}(â†’x, t)\textrm{~~from~~}pÌ‚_\textrm{data}.$$

~~~
- **Minibatch SGD**: Trade-off between gradient descent and SGD â€“ the
  expectation in $âˆ‡_{â†’w} E(â†’w)$ is estimated using $B$ random independent
  examples from the training data.

$$âˆ‡_{â†’w} E(â†’w) â‰ˆ \frac{1}{B} âˆ‘\nolimits_{i=1}^B âˆ‡_{â†’w} L\big(y(â†’x_i; â†’w), t_i\big)
               \textrm{~~for a randomly chosen~~}(â†’x_i, t_i)\textrm{~~from~~}pÌ‚_\textrm{data}.$$

---
# Gradient Descent Convergence

Assume that we perform a stochastic gradient descent, using a sequence
of learning rates $Î±_i$, and using a noisy estimate $J(â†’w)$ of the real
gradient $âˆ‡_{â†’w} E(â†’w)$:
$$â†’w_{i+1} â† â†’w_i - Î±_i J(â†’w_i).$$

~~~
It can be proven (under some reasonable conditions; see Robbins and Monro algorithm, 1951) that if
the loss function $L$ is convex and continuous, then SGD converges to the unique
optimum almost surely if the sequence of learning rates $Î±_i$ fulfills the
following conditions:
$$âˆ€i: Î±_i > 0,~~~âˆ‘_i Î±_i = âˆ,~~~âˆ‘_i Î±_i^2 < âˆ.$$

~~~
Note that the third condition implies that $Î±_i â†’ 0$.

~~~
For nonconvex loss functions, we can get guarantees of converging to a _local_
optimum only.

---
# Gradient Descent Convergence

Convex functions mentioned on the previous slide are such that for $â†’u, â†’v$
and real $0 â‰¤ t â‰¤ 1$,
$$f(tâ†’u + (1-t)â†’v) â‰¤ tf(â†’u) + (1-t)f(â†’v).$$

![w=88%,mw=50%,h=center](convex_2d.svgz)![w=66.5%,mw=50%,h=center](convex_3d.svgz)

~~~
A twice-differentiable function of a single variable is convex iff its second
derivative is always nonnegative. (For functions of multiple variables,
the Hessian must be positive semi-definite.)

~~~
A local minimum of a convex function is always the unique global minimum.

~~~
Well-known examples of convex functions are $x^2$, $e^x$, $-\log x$, and also
the _sum of squares_.

---
# Solving Linear Regression using SGD

To apply SGD on linear regression, we usually minimize one half of the mean squared error:
$$E(â†’w) = ğ”¼_{(â‡â†’x,t)âˆ¼pÌ‚_\textrm{data}}\big[\tfrac{1}{2} (y(â†’x; â†’w) - t)^2\big] = ğ”¼_{(â‡â†’x,â‡t)âˆ¼pÌ‚_\textrm{data}}\big[\tfrac{1}{2} (â†’x^T â†’w - t)^2\big].$$

~~~
If we also include $L^2$ regularization, we get
$$E(â†’w) = ğ”¼_{(â‡â†’x,â‡t)âˆ¼pÌ‚_\textrm{data}}\big[\tfrac{1}{2} (â†’x^T â†’w - t)^2\big] + \tfrac{Î»}{2}\|â†’w\|^2.$$

~~~
We then estimate the expectation by a minibatch of examples with indices $ğ”¹$ as
$$\frac{1}{|ğ”¹|} âˆ‘_{i âˆˆ ğ”¹} \Big(\tfrac{1}{2} (â†’x_i^T â†’w - t_i)^2\Big) + \tfrac{Î»}{2}\|â†’w\|^2,$$
~~~
which gives us an estimate of a gradient
$$âˆ‡_{â†’w} E(â†’w) â‰ˆ \frac{1}{|ğ”¹|} âˆ‘_{i âˆˆ ğ”¹} \Big((â†’x_i^T â†’w - t_i) â†’x_i\Big) + Î»â†’w.$$

---
# Solving Linear Regression using SGD

The computed gradient allows us to formulate the following algorithm for solving
linear regression with minibatch SGD.

~~~
<div class="algorithm">

**Input**: Dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$), learning rate $Î± âˆˆ â„^+$, $L^2$ strength $Î» âˆˆ â„$.<br>
**Output**: Weights $â†’w âˆˆ â„^D$ hopefully minimizing the regularized MSE of a linear regression model.

- $â†’w â† â†’0$ or we initialize $â†’w$ randomly
~~~
- repeat until convergence (or until our patience runs out):
  - sample a minibatch of examples with indices $ğ”¹$
~~~
    - either uniformly randomly,
~~~
    - or we may want to process all training instances before repeating them,
      which can be implemented by generating a random permutation and then
      splitting it into minibatch-sized chunks
      - the most common option; one pass through the data is called an **epoch**
~~~
  - $â†’w â† â†’w - Î± \frac{1}{|ğ”¹|} âˆ‘_{i âˆˆ ğ”¹} \big((â†’x_i^Tâ†’w-t_i)â†’x_i\big) - Î±Î»â†’w$
</div>

---
class: middle
# Training course

![w=80%,h=center](kaggle_anim.gif)

---
# What went wrong? (1/2)

![w=40%,h=center](underfitting.svgz)

~~~
<center>
The training did not converge yet.

We need to continue training.
</center>

---
# What went wrong? (2/2)

![w=40%,h=center](overfitting.svgz)

~~~
<center>The model is overfitting (zero training loss, increasing validation loss).

We should either regularize or take the best validation checkpoint (i.e.,
**early stopping**).

</center>

---
section: Features
class: section
# Features

---
# Features

Recall that the _input_ instance values are usually the raw observations and
are given. However, we might extend them suitably before running a ML
algorithm, especially if the algorithm cannot represent an arbitrary function
(e.g., is linear). Such instance representations are called _features_.

~~~
Example from the previous lecture: even if
our training examples were $x$ and $t$, we performed the linear regression
using features $(x^0, x^1, â€¦, x^M)$:
![w=40%,h=center](../01/sin_lr.svgz)


---
# Feature Types

Generally, it would be best if the ML algorithms would process
only the raw inputs. However, many algorithms can represent
only a limited set of functions (e.g., linear), and in that case,
**feature engineering** plays a major part in the final model performance.
Feature engineering is a process of constructing features from raw inputs.

Commonly used features are:
~~~
- **polynomial features** of degree $p$: Given features $(x_1, x_2, â€¦, x_D)$, we
  might consider _all_ products of $p$ input values. Therefore, polynomial
  features of degree 2 would consist of $x_i^2 \,âˆ€i$ and of $x_i x_j \,âˆ€iâ‰ j$.

~~~
- **categorical one-hot features**: Assume that a day in a week is
  represented in the input as an integer value of 1 to 7, or a breed of a dog is
  expressed as an integer value of 0 to 366.
~~~
  Using these integral values as an input to linear regression makes little sense
  â€“ instead, it might be better to learn weights for individual days in a week or
  for individual dog breeds.
~~~
  We might therefore represent input classes by binary indicators for every
  class, giving rise to a **one-hot** representation, where an input integral value
  $0 â‰¤ v < L$ is represented as $L$ binary values, which are all zero except for
  the $v^\mathrm{th}$ one, which is one.

---
# Feature Normalization

- Features in different scales would need different learning rates

- Common solution: normalize the features

  - Normalization: $x_{i,j}^\text{norm} = \frac{x_{i,j} - \min_k x_{k,j}}{ \max_k x_{k,j} - \min_k x_{k,j} }$ (`MinMaxScaler` in Scikit-learn)

  - Standardization: $x_{i,j}^\text{standard} = \frac{ x_{i,j} - \hat{\mu}_{j} }{\hat{\sigma{j}}}$ (`StandardScaler` in Scikit-learn)

---
class: summary
# Today's Lecture Objectives

After this lecture you should be able to

- Reason about **overfitting** in terms of **model capacity**.

- Use **$L^2$-regularization** to control model capacity.

- Explain what the difference between **parameters and hyperparameters** is.

- Tell what the **basic probability concepts** are (joint, marginal,
  conditional probability; expected value, mean, variance).

- Mathematically describe and implement the **stochastic gradient descent**
  algorithm.

- Use both **numerical and categorical features** in linear regression.

