title: NPFL129, Lecture 2
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Perceptron and Logistic Regression

## Milan Straka

### October 14, 2019

---
section: Regression
# Linear Regression

Given an input value $â†’x âˆˆ â„^d$, one of the simplest models to predict
a target real value is **linear regression**:
$$f(â†’x; â†’w, b) = x_1 w_1 + x_2 w_2 + â€¦ + x_D w_D + b = âˆ‘_{i=1}^d x_i w_i + b = â†’x^T â†’w + b.$$
The _bias_ $b$ can be considered one of the _weights_ $â†’w$ if convenient.

~~~
By computing derivatives of a sum of squares error function, we arrived
at the following equation for the optimum weights:
$$â‡‰X^Tâ‡‰Xâ†’w = â‡‰X^T â†’t.$$

~~~
If $â‡‰X^T â‡‰X$ is regular, we can invert it and compute the weights as $â†’w = (â‡‰X^Tâ‡‰X)^{-1}â‡‰X^Tâ†’t$.

~~~
Matrix $â‡‰X^T â‡‰X$ is regular if and only if $â‡‰X$ has rank $d$, which is
equivalent to the columns of $â‡‰X$ being linearly independent.

---
class: dbend
# SVD Solution of Linear Regression

Now consider the case that $â‡‰X^T â‡‰X$ is singular. We will show that
$â‡‰X^Tâ‡‰Xâ†’w = â‡‰X^T â†’t$ is still solvable, but it does not have a unique
solution. Our goal in this case will be to find the smallest $â†’w$ fulfilling the
equation.

~~~
We now consider _singular value decomposition (SVD)_ of X, writing $â‡‰X = â‡‰U â‡‰Î£ â‡‰V^T$,
where
- $â‡‰U âˆˆ â„^{NÃ—N}$ is an orthogonal matrix, i.e., $â†’u_i^T â†’u_j = [i=j]$,
- $â‡‰Î£ âˆˆ â„^{NÃ—D}$ is a diagonal matrix,
- $â‡‰V âˆˆ â„^{DÃ—D}$ is again an orthogonal matrix.

~~~
Assuming the diagonal matrix $â‡‰Î£$ has rank $r$, we can write it as
$$â‡‰Î£ = \begin{bmatrix} â‡‰Î£_r & â‡‰0 \\ â‡‰0 & â‡‰0 \end{bmatrix},$$
where $â‡‰Î£_râˆˆâ„^{dÃ—d}$ is a regular diagonal matrix.
~~~
Denoting $â‡‰U_r$ and $â‡‰V_r$ the matrix of first $r$ columns of $â‡‰U$ and $â‡‰V$, respectively,
we can write $â‡‰X = â‡‰U_r â‡‰Î£_r â‡‰V_r^T$.

---
class: dbend
# SVD Solution of Linear Regression

Using the decomposition $â‡‰X = â‡‰U_r â‡‰Î£_r â‡‰V_r^T$, we can rewrite the goal equation
as
$$â‡‰V_r â‡‰Î£_r^T â‡‰U_r^T â‡‰U_r â‡‰Î£_r â‡‰V_r^T â†’w = â‡‰V_r â‡‰Î£_r^T â‡‰U_r^T â†’t.$$

~~~
A transposition of an orthogonal matrix is its inverse. Therefore, our
submatrix $â‡‰U_r$ fulfils that $â‡‰U_r^T â‡‰U_r = â‡‰I$, because $â‡‰U_r^T â‡‰U_r$
is a top left submatrix of $â‡‰U^T â‡‰U$. Analogously, $â‡‰V_r^T â‡‰V_r = â‡‰I$.

~~~
We therefore simplify the goal equation to
$$â‡‰Î£_r â‡‰Î£_r â‡‰V_r^T â†’w = â‡‰Î£_r â‡‰U_r^T â†’t$$

~~~
Because the diagonal matrix $â‡‰Î£_r$ is regular, we can divide by it and obtain
$$â‡‰V_r^T â†’w = â‡‰Î£_r^{-1} â‡‰U_r^T â†’t.$$

---
class: dbend
# SVD Solution of Linear Regression

We have $â‡‰V_r^T â†’w = â‡‰Î£_r^{-1} â‡‰U_r^T â†’t$. If he original matrix $â‡‰X^T â‡‰X$ was
regular, then $r=d$ and $â‡‰V_r$ is a square regular orthogonal matrix, in which case
$$â†’w = â‡‰V_r â‡‰Î£_r^{-1} â‡‰U_r^T â†’t.$$

~~~
If we denote $â‡‰Î£^+ âˆˆ â„^{DÃ—N}$ the diagonal matrix with $Î£_{i,i}^{-1}$ on
diagonal, we can rewrite to
$$â†’w = â‡‰V â‡‰Î£^+ â‡‰U^T â†’t.$$

~~~
Now if $r < d$, $â‡‰V_r^T â†’w = â†’y$ is undetermined and has infinitely many
solutions. To find the one with smallest norm $||â†’w||$, consider the full
product $â‡‰V^Tâ†’w$. Because $â‡‰V$ is orthogonal, $||â‡‰V^Tâ†’w||=||â†’w||$, and it is
sufficient to find $â†’w$ with smallest $||â‡‰V^Tâ†’w||$.
~~~
We know that the first $r$ elements of $||â‡‰V^Tâ†’w||$ are fixed by the above equation
â€“ the smallest $||â‡‰V^Tâ†’w||$ can be therefore obtained by setting the last $d-r$
elements to zero.
~~~
Finally, we note that $â‡‰Î£^+ â‡‰U^T â†’t$ is exactly $â‡‰Î£_r^{-1} â‡‰U_r^T â†’t$ padded
with $d-r$ zeros, obtaining the same solution $â†’w = â‡‰V â‡‰Î£^+ â‡‰U^T â†’t$.

---
class: dbend
# SVD Solution of Linear Regression and Pseudoinverses

The solution to a linear regression with sum of squares error function is
tightly connected to matrix pseudoinverses. If a matrix $â‡‰X$ is singular or
rectangular, it does not have an exact inverse, and $â‡‰Xâ†’w=â†’b$ does not
have an exact solution. 

~~~
However, we can consider the so-called _Moore-Penrose pseudoinverse_
$$â‡‰X^+ â‰ â‡‰V â‡‰Î£^+ â‡‰U^T$$
to be the closest approximation to an inverse, in the sense that we can find
the best solution (with smallest MSE) to the equation $â‡‰Xâ†’w=â†’b$ by setting $â†’w=â‡‰X^+ â†’b$.

~~~
Alternatively, we can define the pseudoinverse as
$$â‡‰X^+ = \argmin_{â‡‰Yâˆˆâ„^{DÃ—N}} ||â‡‰X â‡‰Y - â‡‰I_N||_F = \argmin_{â‡‰Yâˆˆâ„^{NÃ—D}} ||â‡‰Y â‡‰X - â‡‰I_D||_F$$
which can be verified to be the same as our SVD formula.

---
section: Random Variables
# Random Variables
A random variable $â‡x$ is a result of a random process. It can be discrete or
continuous.

~~~
## Probability Distribution
A probability distribution describes how likely are individual values a random
variable can take.

The notation $â‡x âˆ¼ P$ stands for a random variable $â‡x$ having a distribution $P$.

~~~
For discrete variables, the probability that $â‡x$ takes a value $x$ is denoted as
$P(x)$ or explicitly as $P(â‡x = x)$. All probabilities are non-negative and sum
of probabilities of all possible values of $â‡x$ is $âˆ‘_x P(â‡x=x) = 1$.

~~~
For continuous variables, the probability that the value of $â‡x$ lies in the interval
$[a, b]$ is given by $âˆ«_a^b p(x)\d x$.

---
# Random Variables

## Expectation
The expectation of a function $f(x)$ with respect to discrete probability
distribution $P(x)$ is defined as:
$$ğ”¼_{â‡x âˆ¼ P}[f(x)] â‰ âˆ‘_x P(x)f(x)$$

For continuous variables it is computed as:
$$ğ”¼_{â‡x âˆ¼ p}[f(x)] â‰ âˆ«_x p(x)f(x)\d x$$

~~~
If the random variable is obvious from context, we can write only $ğ”¼_P[x]$
of even $ğ”¼[x]$.

~~~
Expectation is linear, i.e.,
$$ğ”¼_â‡x [Î±f(x) + Î²g(x)] = Î±ğ”¼_â‡x [f(x)] + Î²ğ”¼_â‡x [g(x)]$$

---
# Random Variables

## Variance
Variance measures how much the values of a random variable differ from its
mean $Î¼ = ğ”¼[x]$.

$$\begin{aligned}
  \Var(x) &â‰ ğ”¼\left[\big(x - ğ”¼[x]\big)^2\right]\textrm{, or more generally} \\
  \Var(f(x)) &â‰ ğ”¼\left[\big(f(x) - ğ”¼[f(x)]\big)^2\right]
\end{aligned}$$

~~~
It is easy to see that
$$\Var(x) = ğ”¼\left[x^2 - 2xğ”¼[x] + \big(ğ”¼[x]\big)^2\right] = ğ”¼\left[x^2\right] - \big(ğ”¼[x]\big)^2,$$
because $ğ”¼\big[2xğ”¼[x]\big] = 2(ğ”¼[x])^2$.

~~~
Variance is connected to $E[x^2]$, a _second moment_ of a random
variable â€“ it is in fact a _centered_ second moment.

---
# Estimators and Bias

An _estimator_ is a rule for computing an estimate of a given value, often an
expectation of some random value(s).

~~~
For example, we might estimate _mean_ of random variable by sampling a value
according to its probability distribution.

~~~
_Bias_ of an estimator is the difference of the expected value of the estimator
and the true value being estimated:
$$\textrm{bias} = ğ”¼[\textrm{estimate}] - \textrm{true estimated value}.$$

~~~
If the bias is zero, we call the estimator _unbiased_, otherwise we call it
_biased_.

---
# Estimators and Bias

If we have a sequence of estimates, it also might happen that the bias converges
to zero. Consider the well known sample estimate of variance. Given $â‡x_1,
\ldots, â‡x_n$ independent and identically distributed random variables, we might
estimate mean and variance as
$$Î¼Ì‚ = \frac{1}{n} âˆ‘\nolimits_i x_i,~~~ÏƒÌ‚_2 = \frac{1}{n} âˆ‘\nolimits_i (x_i - Î¼Ì‚)^2.$$
~~~
Such estimate is biased, because $ğ”¼[ÏƒÌ‚^2] = (1 - \frac{1}{n})Ïƒ^2$, but the bias
converges to zero with increasing $n$.

~~~
Also, an unbiased estimator does not necessarily have small variance â€“ in some
cases it can have large variance, so a biased estimator with smaller variance
might be preferred.

---
section: SGD
# Gradient Descent

Sometimes it is more practical to search for the best model weights
in an iterative/incremental/sequential fashion. Either because there is too much
data, or the direct optimization is not feasible.

~~~
![w=50%,f=right](gradient_descent.pdf)

Assuming we are minimizing an error function
$$\argmin_â†’w E(â†’w),$$
we may use _gradient descent_:
$$â†’w â† â†’w - Î±âˆ‡_â†’wE(â†’w)$$

~~~
The constant $Î±$ is called a _learning rate_ and specifies the â€œlengthâ€
of a step we perform in every iteration of the gradient descent.

---
# Gradient Descent Variants

Consider an error function computed as an expectation over the dataset:
$$âˆ‡_â†’w E(â†’w) = âˆ‡_â†’w ğ”¼_{(â†’x, t)âˆ¼pÌ‚_\textrm{data}} L\big(f(â†’x; â†’w), t\big).$$

~~~
- **(Regular) Gradient Descent**: We use all training data to compute $âˆ‡_â†’w E(â†’w)$
  exactly.

~~~
- **Online (or Stochastic) Gradient Descent**: We estimate $âˆ‡_â†’w E(â†’w)$ using
  a single random example from the training data. Such an estimate is unbiased,
  but very noisy.

$$âˆ‡_â†’w E(â†’w) â‰ˆ âˆ‡_â†’w L\big(f(â†’x; â†’w), t\big)\textrm{~~for randomly chosen~~}(â†’x, t)\textrm{~~from~~}pÌ‚_\textrm{data}.$$

~~~
- **Minibatch SGD**: The minibatch SGD is a trade-off between gradient descent
  and SGD â€“ the expectation in $âˆ‡_â†’w E(â†’w)$ is estimated using $m$ random independent
  examples from the training data.

$$âˆ‡_â†’w E(â†’w) â‰ˆ \frac{1}{m} âˆ‘_{i=1}^m âˆ‡_â†’w L\big(f(â†’x_i; â†’w), t_i\big)
               \textrm{~~for randomly chosen~~}(â†’x_i, t_i)\textrm{~~from~~}pÌ‚_\textrm{data}.$$

---
# Gradient Descent Convergence

Assume that we perform a stochastic gradient descent, using a sequence
of learning rates $Î±_i$, and using a noisy estimate $J(â†’w)$ of the real
gradient $âˆ‡_â†’w E(â†’w)$:
$$â†’w_{i+1} â† â†’w_i - Î±_i J(â†’w_i).$$

~~~
It can be proven (under some reasonable conditions; see Robbins and Monro algorithm, 1951) that if
the loss function $L$ is convex and continuous, then SGD converges to the unique
optimum almost surely if the sequence of learning rates $Î±_i$ fulfills the
following conditions:
$$Î±_i \rightarrow 0,~~~âˆ‘_i Î±_i = âˆ,~~~âˆ‘_i Î±_i^2 < âˆ.$$

~~~
For non-convex loss functions, we can get guarantees of converging to a _local_
optimum only. However, note that finding a global minimum of an arbitrary
function is _at least NP-hard_.

---
# Gradient Descent Convergence

Convex functions mentioned on a previous slide are such that for $x_1, x_2$
and real $0 â‰¤ t â‰¤ 1$,
$$f(tx_1 + (1-t)x_2) â‰¤ tf(x_1) + (1-t)f(x_2).$$

![w=90%,mw=50%,h=center](convex_2d.pdf)![w=68%,mw=50%,h=center](convex_3d.pdf)

~~~
A twice-differentiable function is convex iff its second derivative is always
non-negative.

~~~
A local minimum of a convex function is always the unique global minimum.

~~~
Well-known examples of convex functions are $x^2$, $e^x$ and $-\log x$.

---
# Gradient Descent of Linear Regression

For linear regression and sum of squares, using online gradient descent we can
update the weights as
$$â†’w â† â†’w - Î±âˆ‡_â†’wE(â†’w) = â†’w - Î±(â†’x^Tâ†’w-t)â†’x.$$

~~~
<div class="algorithm">

**Input**: Dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$), learning rate $Î± âˆˆ â„^+$.<br>
**Output**: Weights $â†’w âˆˆ â„^D$ which hopefully minimize MSE of linear regression.

- $â†’w â† 0$
- repeat until convergence:
  - for $i = 1, \ldots, n$:
    - $â†’w â† â†’w - Î±(â†’x_i^Tâ†’w-t_i)â†’x_i.$
</div>

---
section: Features
# Features

Note that until now, we did not explicitly distinguished _input_ instance values and
instance _features_.

~~~
The _input_ instance values are usually the raw observations and are given.
However, we might extend them suitably before running a machine learning
algorithm, especially if the algorithm is linear or otherwise limited and
cannot represent arbitrary function.

~~~
We already saw this in the example from the previous lecture, where even if
our training examples were $x$ and $t$, we performed the linear regression
using features $(x^0, x^1, â€¦, x^M)$:
![w=35%,h=center](../01/sin_lr.pdf)

---
# Features

Generally, it would be best if we have machine learning algorithms processing
only the raw inputs. However, many algorithms are capable of representing
only a limited set of functions (for example linear ones), and in that case,
_feature engineering_ plays a major part in the final model performance.
Feature engineering is a process of constructing features from raw inputs.

Commonly used features are:
~~~
- **polynomial features** of degree $p$: Given features $(x_1, x_2, â€¦, x_D)$, we
  might consider _all_ products of $p$ input values. Therefore, polynomial
  features of degree 2 would consist of $x_i^2 âˆ€i$ and of $x_i x_j âˆ€iâ‰ j$.

~~~
- **categorical one-hot features**: Assume for example that a day in a week is
  represented on the input as an integer value of 1 to 7, or a breed of a dog is
  expressed as an integer value of 0 to 366. Using these integral values as
  input to linear regression makes little sense â€“ instead it might be better
  to learn weights for individual days in a week or for individual dog breeds.
  We might therefore represent input classes by binary indicators for every
  class, giving rise to **one-hot** representation, where input integral
  value $1 â‰¤ v â‰¤ L$ is represented as $L$ binary values, which are all
  zero except for the $v$-th one, which is one.

---
section: CV
# Cross-Validation

We already talked about a **train set** and a **test set**. Given that the main
goal of machine learning is to perform well on unseen data, the test set must
not be used during training nor hyperparameter selection. Ideally, it is hidden
to us altogether.

~~~
Therefore, to evaluate a machine learning model (for example to select model
architecture, input features, or hyperparameter value), we normally need the
**validation** or a **development** set.

~~~
However, using a single development set might give us noisy results. To obtain
less noisy results (i.e., with smaller variance), we can use
**cross-validation**.

~~~
![w=48%,f=right](k-fold_cross_validation.pdf)

In cross-validation, we choose multiple validation sets from the training data,
and for every one, we train a model on the rest of the training data and
evaluate on the chosen validation sets. A commonly used strategy to choose
the validation sets is called **k-fold cross-validation**. Here the training set is partitioned
into $k$ subsets of approximately the same size, and each subset takes turn
to play a role of a validation set.

---
section: Perceptron
# Binary Classification

Binary classification is a classification in two classes.

~~~
To extend linear regression to binary classification, we might seek
a _threshold_ and the classify an input as negative/positive
depending whether $â†’x^Tâ†’w$ is smaller/larger than a given threshold.

~~~
Zero value is usually used as the threshold, both because it
is symmetric and also because the _bias_ parameter acts as a trainable threshold
anyway.

---
# Perceptron

The perceptron algorithm is probably the oldest one for training
weights of a binary classification. Assuming the target value
$t âˆˆ \{-1, +1\}$, the goal is to find weights $â†’w$ such that
for all train data
$$\operatorname{sign}(â†’w^T â†’x_i) = t_i,$$
or equivalently
$$t_i â†’w^T â†’x_i > 0.$$

~~~
Note that a set is called **linearly separable**, if there exist
a weight vector $â†’w$ such that the above equation holds.

---
# Perceptron

The perceptron algorithm was invented by Rosenblat in 1958.

<div class="algorithm">

**Input**: Linearly separable dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{-1, +1\}$).<br>
**Output**: Weights $â†’w âˆˆ â„^D$ such that $t_i â†’x_i^Tâ†’w > 0$ for all $i$.

- $â†’w â† 0$
- until all examples are classified correctly, process example $i$:
  - $y â† â†’w^Tâ†’x_i$
  - if $t_i y < 0$ (incorrectly classified example):
    - $â†’w â† â†’w + t_i â†’x_i$
</div>

~~~
We will prove that the algorithm always arrives at some correct set of
weights $â†’w$ if the training set is linearly separable.

---
# Perceptron as SGD

Consider the main part of the perceptron algorithm:

<div class="algorithm">

  - $y â† â†’w^Tâ†’x_i$
  - if $t_i y < 0$ (incorrectly classified example):
    - $â†’w â† â†’w + t_i â†’x_i$
</div>

~~~
We can derive the algorithm using on-line gradient descent, using
the following loss function
$$L(f(â†’x; â†’w), t) â‰ \begin{cases} -t â†’x^T â†’w & \textrm{if~}t â†’x^T â†’w < 0 \\ 0 & \textrm{otherwise}\end{cases}
  = \max(0, -tâ†’x^T â†’w) = \ReLU(-tâ†’x^T â†’w).$$

~~~
In this specific case, the value of the learning rate does not actually matter,
because multiplying $â†’w$ by a constant does not change a prediction.

---
# Proof of Perceptron Convergence

Let $â†’w_*$ be some weights separating the training data and let $â†’w_k$ be the
weights after $t$ non-trivial updates of the perceptron algorithm, with $â†’w_0$
being 0.

~~~
We will prove that the angle $Î±$ between $â†’w_*$ and $â†’w_k$ decreases at each step.
Note that
$$\cos(Î±) = \frac{â†’w_*^T â†’w_k}{||â†’w_*||â‹…||â†’w_k||}.$$

---
# Proof of Perceptron Convergence

Assume that the maximum norm of any training example $||â†’x||$ is bounded by $R$,
and that $Î³$ is the minimum margin of $â†’w_*$, so $t â†’w_* â†’x â‰¥ Î³.$

~~~
First consider the dot product of $â†’w_*$ and $â†’w_k$:
$$â†’w_*^T â†’w_k = â†’w_*^T (â†’w_{k-1} + t_k â†’x_k) â‰¥ â†’w_*^T â†’w_{k-1} + Î³.$$
By iteratively applying this equation, we get
$$â†’w_*^T â†’w_k â‰¥ kÎ³.$$

~~~
Now consider the length of $â†’w_k$:
$$\begin{aligned}
||â†’w_k||^2 &= ||â†’w_{k-1} + t_kâ†’x_k||^2 = ||â†’w_{k-1}||^2 + 2tâ†’w_{k-1}^Tâ†’x_k + ||â†’x_k||^2
\end{aligned}$$

~~~
Because $â†’x_k$ was misclassified, we know that $t â†’w_{k-1}^T â†’x_k < 0$, so
$||â†’w_k||^2 â‰¤ ||â†’w_{k-1}||^2 + R^2.$

---
# Proof of Perceptron Convergence

Putting everything together, we get
$$\cos(Î±) = \frac{â†’w_*^T â†’w_k}{||â†’w_*||â‹…||â†’w_k||} â‰¥ \frac{kÎ³}{\sqrt{kR^2}||â†’w_*||}.$$

~~~
Therefore, the $\cos(Î±)$ increases during every update. Because the value of
$\cos(Î±)$ is at most one, we can compute the upper bound on the number
of steps when the algorithm converges as
$$1 â‰¤ \frac{kÎ³}{\sqrt{kR^2}||â†’w_*||}\textrm{~or~}k â‰¥ \frac{R^2||â†’w_*||^2}{Î³^2}.$$

---
# Perceptron Issues

Perceptron has several drawbacks:
- If the input set is not linearly separable, the algorithm never finishes.
~~~
- The algorithm cannot be easily extended to classification into more than two
  classes.
~~~
- The algorithm performs only prediction, it is not able to return the
  probabilities of predictions.

---
section: Logistic Regression
# Logistic Regression

An extension of perceptron, which models the conditional probabilities
of $p(C_0|â†’x)$ and of $p(C_1|â†’x)$. Logistic regression can in fact
handle also more than two classes, but we will stay in binary classification
settings for now.

~~~
Logistic regression employs the following parametrization of the conditional
class probabilities:
$$\begin{aligned}
  P(C_1 | â†’x) &= Ïƒ(â†’x^t â†’w + â†’b) \\
  P(C_0 | â†’x) &= 1 - P(C_1 | â†’x),
\end{aligned}$$
where $Ïƒ$ is a _sigmoid function_
$$Ïƒ(x) = \frac{1}{1+e^{-x}}.$$

~~~
Can be trained using an SGD algorithm.

---
# Logistic Regression

To give some meaning to the sigmoid function, starting with
$$P(C_1 | â†’x) = Ïƒ(f(â†’x; â†’w))$$
we can arrive at
$$f(â†’x; â†’w) = \log\left(\frac{P(C_1 | â†’x)}{P(C_0 | â†’x)}\right),$$
where the prediction of the model $f(â†’x; â†’w)$ is called a _logit_
and it is a logarithm of odds of the two classes probabilities.

---
section: MLE
# Maximum Likelihood Estimation

Let $ğ• = \{(â†’x_1, t_1), (â†’x_2, t_2), â€¦, (â†’x_N, t_N)\}$ be training data drawn
independently from the data-generating distribution $p_\textrm{data}$. We denote
the empirical data distribution as $pÌ‚_\textrm{data}$.

~~~
Let $p_\textrm{model}(t | â†’x; â†’w)$ be a family of distributions.

~~~
The *maximum likelihood estimation* of $â†’w$ is:

$$\begin{aligned}
â†’w_\mathrm{ML} &= \argmax_â†’w p_\textrm{model}(\mathbb X; â†’w) \\
               &= \argmax_â†’w âˆ\nolimits_{i=1}^N p_\textrm{model}(t_i | â†’x_i; â†’w) \\
               &= \argmin_â†’w âˆ‘\nolimits_{i=1}^N -\log p_\textrm{model}(t_i | â†’x_i; â†’w) \\
               &= \argmin_â†’w ğ”¼_{â‡â†’x âˆ¼ pÌ‚_\textrm{data}} [-\log p_\textrm{model}(t | â†’x; â†’w)] \\
\end{aligned}$$

---
# Properties of Maximum Likelihood Estimation

Assume that the true data generating distribution $p_\textrm{data}$ lies within the model
family $p_\textrm{model}(â‹…; â†’w)$, and assume there exists a unique
$â†’w_{p_\textrm{data}}$ such that $p_\textrm{data} = p_\textrm{model}(â‹…; â†’w_{p_\textrm{data}})$.

~~~
- MLE is a _consistent_ estimator. If we denote $â†’w_m$ to be the parameters
  found by MLE for a training set with $m$ examples generated by the data
  generating distribution, then $â†’w_m$ converges in probability to
  $â†’w_{p_\textrm{data}}$.

  Formally, for any $Îµ > 0$, $P(||â†’w_m - â†’w_{p_\textrm{data}}|| > Îµ) â†’ 0$
  as $m â†’ âˆ$.

~~~
- MLE is in a sense most _statistic efficient_. For any consistent estimator, we
  might consider the average distance of $â†’w_m$ and $â†’w_{p_\textrm{data}}$,
  formally $ğ”¼_{â‡â†’x_1, \ldots, â‡â†’x_m âˆ¼ p_\textrm{data}} [||â†’w_m - â†’w_{p_\textrm{data}}||_2^2]$.
  It can be shown (Rao 1945, CramÃ©r 1946) that no consistent estimator has
  lower mean squared error than the maximum likelihood estimator.

~~~
Therefore, for reasons of consistency and efficiency, maximum likelihood is
often considered the preferred estimator for machine learning.

---
section: LR
# Logistic Regression
<div class="algorithm">

**Input**: Input dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{0, +1\}$), learning rate $Î± âˆˆ â„^+$.<br>

- $â†’w â† 0$
- until convergence (or until patience is over), process example $i$:
  - $â†’w â† â†’w + Î±âˆ‡_â†’w \log p(t_i|â†’x_i; â†’w)$
</div>

---
# Common Probability Distributions
## Bernoulli Distribution
The Bernoulli distribution is a distribution over a binary random variable.
It has a single parameter $Ï† âˆˆ [0, 1]$, which specifies the probability of the random
variable being equal to 1.

~~~
$$\begin{aligned}
  P(x) &= Ï†^x (1-Ï†)^{1-x} \\
  ğ”¼[x] &= Ï† \\
  \Var(x) &= Ï†(1-Ï†)
\end{aligned}$$

~~~
## Categorical Distribution
Extension of the Bernoulli distribution to random variables taking one of $k$ different
discrete outcomes. It is parametrized by $â†’p âˆˆ [0, 1]^k$ such that $âˆ‘_{i=1}^{k} p_{i} = 1$.
$$\begin{aligned}
  P(â†’x) &= âˆ\nolimits_i^k p_i^{x_i} \\
  ğ”¼[x_i] &= p_i, \Var(x_i) = p_i(1-p_i) \\
\end{aligned}$$

---
# Information Theory

## Self Information

Amount of _surprise_ when a random variable is sampled.
~~~
- Should be zero for events with probability 1.
~~~
- Less likely events are more surprising.
~~~
- Independent events should have _additive_ information.

~~~
$$I(x) â‰ -\log P(x) = \log \frac{1}{P(x)}$$

~~~
## Entropy

Amount of _surprise_ in the whole distribution.
$$H(P) â‰ ğ”¼_{â‡xâˆ¼P}[I(x)] = -ğ”¼_{â‡xâˆ¼P}[\log P(x)]$$

~~~
- for discrete $P$: $H(P) = -âˆ‘_x P(x) \log P(x)$
- for continuous $P$: $H(P) = -âˆ« P(x) \log P(x)\,\mathrm dx$

---
# Information Theory

## Cross-Entropy

$$H(P, Q) â‰ -ğ”¼_{â‡xâˆ¼P}[\log Q(x)]$$

~~~
- Gibbs inequality
    - $H(P, Q) â‰¥ H(P)$
    - $H(P) = H(P, Q) â‡” P = Q$
~~~
    - Proof: Using Jensen's inequality, we get
      $$âˆ‘_x P(x) \log \frac{Q(x)}{P(x)} â‰¤ \log âˆ‘_x P(x) \frac{Q(x)}{P(x)} = \log âˆ‘_x Q(x) = 0.$$
~~~
    - Corollary: For a categorical distribution with $n$ outcomes, $H(P) â‰¤ \log n$,
    because for $Q(x) = 1/n$ we get $H(P) â‰¤ H(P, Q) = -âˆ‘_x P(x) \log Q(x) = \log n.$
~~~
- generally $H(P, Q) â‰  H(Q, P)$

---
# Information Theory

## Kullback-Leibler Divergence (KL Divergence)

Sometimes also called _relative entropy_.

$$D_\textrm{KL}(P || Q) â‰ H(P, Q) - H(P) = ğ”¼_{â‡xâˆ¼P}[\log P(x) - \log Q(x)]$$

~~~
- consequence of Gibbs inequality: $D_\textrm{KL}(P || Q) â‰¥ 0$
- generally $D_\textrm{KL}(P || Q) â‰  D_\textrm{KL}(Q || P)$

---
# Nonsymmetry of KL Divergence

![w=100%,v=middle](kl_nonsymmetry.pdf)

---
# Common Probability Distributions
## Normal (or Gaussian) Distribution
Distribution over real numbers, parametrized by a mean $Î¼$ and variance $Ïƒ^2$:
$$ğ“(x; Î¼, Ïƒ^2) = \sqrt{\frac{1}{2Ï€Ïƒ^2}} \exp \left(-\frac{(x - Î¼)^2}{2Ïƒ^2}\right)$$

For standard values $Î¼=0$ and $Ïƒ^2=1$ we get $ğ“(x; 0, 1) = \sqrt{\frac{1}{2Ï€}} e^{-\frac{x^2}{2}}$.

![w=45%,h=center](normal_distribution.pdf)

---
# Why Normal Distribution

## Central Limit Theorem
The sum of independent identically distributed random variables
with finite variance converges to normal distribution.

~~~
## Principle of Maximum Entropy
Given a set of constraints, a distribution with maximal entropy fulfilling the
constraints can be considered the most general one, containing as little
additional assumptions as possible.

~~~
Considering distributions with a given mean and variance, it can be proven
(using variational inference) that such a distribution with _maximal entropy_
is exactly the normal distribution.
