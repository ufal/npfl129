title: NPFL129, Lecture 2
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Linear Regression II, SGD

## Milan Straka

### October 10, 2022

---
section: Refresh
# Linear Regression

Given an input value $→x ∈ ℝ^D$, **linear regression** computes predictions as:
$$y(→x; →w, b) = x_1 w_1 + x_2 w_2 + … + x_D w_D + b = ∑_{i=1}^D x_i w_i + b = →x^T →w + b.$$
The _bias_ $b$ can be considered one of the _weights_ $→w$ if convenient.

~~~
We train the weights by minimizing an **error function** between the real target
values and their predictions, notably _sum of squares_:
$$\frac{1}{2} ∑_{i=1}^N \big(y(→x_i; →w) - t_i\big)^2$$

~~~
There are various approaches to minimize it, but for linear regression an explicit solution exists:
$$→w = (⇉X^T⇉X)^{-1}⇉X^T→t.$$

---
# Linear Regression Example

Assume our input vectors comprise of $→x = (x^0, x^1, …, x^M)$, for $M ≥ 0$.

![w=60%,h=center](../01/sin_lr.svgz)

---
# Linear Regression Example

To plot the error, the _root mean squared error_
$\operatorname{RMSE}=\sqrt{\operatorname{MSE}}$ is frequently used.

![w=60%,f=right](../01/sin_errors.svgz)
~~~

The displayed error nicely illustrates two main challenges in machine learning:
- _underfitting_
- _overfitting_

---
section: Regularization
# Model Capacity
We can control whether a model underfits or overfits by modifying its **capacity**.
~~~
- representational capacity
- effective capacity

~~~
![w=80%,h=center](generalization_error.svgz)

---
# Linear Regression Overfitting

Note that employing more data usually alleviates overfitting (the relative
capacity of the model is decreased).

![w=100%](sin_overfitting.svgz)

---
# Regularization

**Regularization**, in a broad sense, is any change that is designed to _reduce generalization error_ (but not necessarily
its training error) in a machine learning
algorithm.

~~~
We already saw that **limiting model capacity** can work as regularization.

![w=38%,h=center](classification_overfitting.svgz)

---
# L2 Regularization

Being one of the oldest regularization techniques, it tries to prefer “simpler” models
by endorsing models with **smaller weights**.

~~~
Concretely, **$\boldsymbol{L^2}$-regularization** (also called **weight decay**) penalizes
models with large weights by utilizing the following error function:

$$\frac{1}{2} ∑_{i=1}^N \big(y(→x_i; →w) - t_i\big)^2 + \textcolor{red}{\frac{λ}{2} \|→w\|^2}.$$

~~~
Note that the $L^2$-regularization is usually not applied to the _bias_, only to the
“proper” weights, because we cannot really overfit via the bias.
~~~
Also, without penalizing the bias, $L^2$-regularization is invariant to shifts
(i.e., adding a constant to all the targets would result in the same solution, only with
the bias increased by that constant; if the bias were to be penalized, this would not
be true).

~~~
For simplicity, we will not explicitly exclude the bias from the
$L^2$-regularization penalty in the slides (several textbooks also take the
same approach).

---
# L2 Regularization

![w=25%,f=right](l2_smoothness_data.png)

One way to look at $L^2$-regularization is as a promoter of smaller
changes of the model (the gradient of linear regression with respect to the
inputs are exactly the weights, i.e., $∇_{→x} y(→x; →w) = →w$).

~~~
Considering the data points on the right, we present mean squared errors
and $L^2$ norms of the weights for three linear regression models:

![w=70%,h=center](l2_smoothness.png)
![w=70%,h=center](l2_smoothness_equations.png)

---
# L2 Regularization

The effect of $L^2$-regularization can be seen as limiting the _effective
capacity_ of the model.

![w=66%,mh=70%,v=bottom](sin_regularization.svgz)
~~~
![w=32.5%,mh=70%,v=bottom](sin_regularization_ablation.svgz)

---
# Regularizing Linear Regression

In a matrix form, the regularized "sum of squares error" for linear regression amounts
to
$$\tfrac{1}{2} \|⇉X→w - →t\|^2 + \tfrac{λ}{2} \|→w\|^2.$$

When repeating the same calculation as in the unregularized case, we arrive at
$$(⇉X^T⇉X + λ⇉I)→w = ⇉X^T→t,$$
where $⇉I$ is an identity matrix.

~~~
<div class="algorithm">

**Input**: Dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ ℝ^N$), constant $λ ∈ ℝ^+$.<br>
**Output**: Weights $→w ∈ ℝ^D$ minimizing MSE of regularized linear regression.

- $→w ← (⇉X^T⇉X + λ⇉I)^{-1}⇉X^T→t.$
</div>

~~~
Note that the matrix $⇉X^T⇉X + λ⇉I$ is always regular for $λ>0$ (you can show that the matrix is positive definite). So, another effect of $L^2$-regularization is that the
inverse always exists.

---
section: Hyperparameters
# Choosing Hyperparameters

**Hyperparameters** are not adapted by the learning algorithm itself.

~~~
Usually a **validation set** or **development set** is used to
estimate the generalization error, allowing us to update hyperparameters accordingly.
If there is not enough data (well, there is **always** not enough data),
more sophisticated approaches can be used.

~~~
So far, we have seen two hyperparameters, $M$ and $λ$.

~~~
![w=88%,mw=50%,h=center](../01/sin_errors.svgz)![w=87%,mw=50%,h=left](sin_regularization_ablation.svgz)


---
section: SVD Solution
# Linear Regression

When training a linear regression model, we minimized the "sum of squares error
function" by computing its gradient (partial derivatives with respect to all weights). We found a solution when the gradient is equal to zero, arriving at the following equation
for optimal weights:
$$⇉X^T⇉X→w = ⇉X^T →t.$$

~~~
If $⇉X^T ⇉X$ is regular, we can invert it and compute the weights as $→w = (⇉X^T⇉X)^{-1}⇉X^T→t$.

~~~
It can be proven (see next slide) that $\operatorname{rank}(⇉X) = \operatorname{rank}(⇉X^T →X)$.
So, the matrix $⇉X^T ⇉X ∈ ℝ^{D×D}$ is regular if and only if $⇉X$ has rank $D$, which is
equivalent to the columns of $⇉X$ being linearly independent.

---
class: dbend
# Linear Regression Solution Always Exists

We now show that the solution of $⇉X^T⇉X→w = ⇉X^T →t$ always exists.

~~~
Recall that the rank-nullity theorem states that for a matrix $⇉A ∈ ℝ^{V×W}$,
$$\operatorname{rank}(⇉A) + \operatorname{nullity}(⇉A) ≝ \dim(\operatorname{im}(⇉A)) + \dim(\operatorname{ker}(⇉A)) = W.$$

~~~
Our goal is to show that $\operatorname{im}(⇉X^T ⇉X) = \operatorname{im}(⇉X^T)$.
This would imply that the solution will always exist because for any $→t$, $⇉X^T →t ∈ \operatorname{im}(⇉X^T ⇉X)$.
~~~
- We first show that $\operatorname{ker}(⇉X^T ⇉X) = \operatorname{ker}(⇉X)$.
~~~
  - If $⇉X →t = 0$, then also $⇉X^T ⇉X →t = 0$, so $\operatorname{ker}(⇉X^T ⇉X) ⊇ \operatorname{ker}(⇉X)$.
~~~
  - If $⇉X^T ⇉X →t = 0$, then also $→t^T ⇉X^T ⇉X →t = 0$. Therefore $(⇉X →t)^T (⇉X →t) = 0$, which implies
    $⇉X →t = 0$, resulting in $\operatorname{ker}(⇉X^T ⇉X) ⊆ \operatorname{ker}(⇉X)$.
~~~
- Therefore, the rank-nullity theorem implies that
  $\operatorname{rank}(⇉X^T ⇉X) = \operatorname{rank}(⇉X) = \operatorname{rank}(⇉X^T)$.
~~~
- Finally, it is easy to see that $\operatorname{im}(⇉X^T ⇉X) ⊆ \operatorname{im}(⇉X^T)$. This,
  combined with the equality of ranks above, proves the required equation $\operatorname{im}(⇉X^T ⇉X) = \operatorname{im}(⇉X^T)$.

---
class: dbend
# SVD Solution of Linear Regression

Now consider the case that $⇉X^T ⇉X$ is singular. We already know that $⇉X^T⇉X→w
= ⇉X^T →t$ is solvable, but it does not have a unique solution (it has many
solutions). Our goal in this case will be to find the $→w$ with the minimum
$\|→w\|$ fulfilling the equation.

~~~
We now consider _singular value decomposition (SVD)_ of $⇉X$, writing $⇉X = ⇉U ⇉Σ ⇉V^T$,
where
- $⇉U ∈ ℝ^{N×N}$ is an orthogonal matrix, i.e., $→u_i^T →u_j = [i=j] ⇔ ⇉U^T ⇉U = ⇉I ⇔ ⇉U^{-1} = ⇉U^T$,
- $⇉Σ ∈ ℝ^{N×D}$ is a diagonal matrix,
- $⇉V ∈ ℝ^{D×D}$ is again an orthogonal matrix.

~~~
Assuming the diagonal matrix $⇉Σ$ has a rank $r$, we can write the matrix as
$$⇉Σ = \begin{bmatrix} ⇉Σ_r & ⇉0 \\ ⇉0 & ⇉0 \end{bmatrix},$$
where $⇉Σ_r∈ℝ^{r×r}$ is a regular diagonal matrix.
~~~
Denoting the matrices of the first $r$ columns of $⇉U$ and $⇉V$ as $⇉U_r$ and $⇉V_r$, respectively,
we can write $⇉X = ⇉U_r ⇉Σ_r ⇉V_r^T$.

---
class: dbend
# SVD Solution of Linear Regression

Using the decomposition $⇉X = ⇉U_r ⇉Σ_r ⇉V_r^T$, we can rewrite the goal equation
as
$$\big(⇉V_r ⇉Σ_r^T ⇉U_r^T\big) \big(⇉U_r ⇉Σ_r ⇉V_r^T\big) →w = \big(⇉V_r ⇉Σ_r^T ⇉U_r^T\big) →t.$$

~~~
The transposition of an orthogonal matrix is its inverse. Therefore, our
submatrix $⇉U_r$ fulfills $\textcolor{darkred}{⇉U_r^T ⇉U_r} = ⇉I$, because
$⇉U_r^T ⇉U_r$ is the top left submatrix of $⇉U^T ⇉U$. Analogously,
$\textcolor{blue}{⇉V_r^T ⇉V_r} = ⇉I$.

~~~
We therefore simplify the goal equation to
$$\textcolor{blue}{⇉V_r^T ⇉V_r} ⇉Σ_r^T \textcolor{darkred}{⇉U_r^T ⇉U_r} ⇉Σ_r ⇉V_r^T →w = \textcolor{blue}{⇉V_r^T ⇉V_r} ⇉Σ_r^T ⇉U_r^T →t.$$
~~~
$$⇉Σ_r^T ⇉Σ_r ⇉V_r^T →w = ⇉Σ_r ⇉U_r^T →t$$

~~~
Because the diagonal matrix $⇉Σ_r=⇉Σ_r^T$ is regular, we can divide by it and obtain
$$⇉V_r^T →w = ⇉Σ_r^{-1} ⇉U_r^T →t.$$

---
class: dbend
# SVD Solution of Linear Regression

We have $⇉V_r^T →w = ⇉Σ_r^{-1} ⇉U_r^T →t$. If the original matrix $⇉X^T ⇉X$ was
regular, then $r=D$ and $⇉V_r$ is a square regular orthogonal matrix, in which case
$$→w = ⇉V_r ⇉Σ_r^{-1} ⇉U_r^T →t.$$

~~~
If we denote $⇉Σ_r^{-1} ∈ ℝ^{D×N}$, the diagonal matrix with $Σ_{i,i}^{-1}$ on its
diagonal, as $⇉Σ^+$, we can rewrite $→w$ to
$$→w = ⇉V ⇉Σ^+ ⇉U^T →t.$$

~~~
Now if $r < D$, $⇉V_r^T →w = →y$ is undetermined and has infinitely many
solutions. To find the one with the smallest norm $\|→w\|$, consider the full
product $⇉V^T→w$. Because $⇉V$ is orthogonal, $\|⇉V^T→w\|=\|→w\|$, and it is
sufficient to find $→w$ with the smallest $\|⇉V^T→w\|$.
~~~
We know that the first $r$ elements of $⇉V^T→w$ are fixed by the above equation
– the smallest $\|⇉V^T→w\|$ can be therefore obtained by setting the last $D-r$
elements to zero.
~~~
Finally, note that $⇉Σ^+ ⇉U^T →t$ is exactly $⇉Σ_r^{-1} ⇉U_r^T →t$ padded
with $D-r$ zeros, obtaining the same solution $→w = ⇉V ⇉Σ^+ ⇉U^T →t$.

---
class: dbend
# SVD Solution of Linear Regression and Pseudoinverses

The solution to a linear regression problem with a "sum of squares error function" is
tightly connected to matrix pseudoinverses. If a matrix $⇉X$ is singular or
rectangular, it does not have an exact inverse, and $⇉X→w=→b$ does not
have an exact solution.

~~~
However, we can consider the so-called _Moore-Penrose pseudoinverse_
$$⇉X^+ ≝ ⇉V ⇉Σ^+ ⇉U^T$$
to be the closest approximation to an inverse, in the sense that we can find
the best solution (with smallest MSE) to the equation $⇉X→w=→b$ by setting $→w=⇉X^+ →b$.

~~~
Alternatively, we can define the pseudoinverse of a matrix $⇉X$ as
$$⇉X^+ = \argmin_{⇉Y∈ℝ^{D×N}} \big\|⇉X ⇉Y - ⇉I_N\big\|_F = \argmin_{⇉Y∈ℝ^{D×N}} \big\|⇉Y ⇉X - ⇉I_D\big\|_F$$
which can be verified to be the same as our SVD formula.

---
section: Random Variables
# Random Variables
A random variable $⁇x$ is a result of a random process. The random variable can be discrete or
continuous.

~~~
## Probability Distribution
A probability distribution describes how likely are the individual values that a random
variable can take.

The notation $⁇x ∼ P$ stands for a random variable $⁇x$ having a distribution $P$.

~~~
For discrete variables, the probability that $⁇x$ takes a value $x$ is denoted as
$P(x)$ or explicitly as $P(⁇x = x)$. All probabilities are non-negative, and the sum
of the probabilities of all possible values of $⁇x$ is $∑_x P(⁇x=x) = 1$.

~~~
For continuous variables, the probability that the value of $⁇x$ lies in the interval
$[a, b]$ is given by $∫_a^b p(x)\d x$, where $p(x)$ is the _probability density
function_, which is always non-negative and integrates to 1 over the range of
all values of $⁇x$.

---
style: .katex-display { margin: 0.8em 0 }
# Joint, Conditional, Marginal Probability

![w=44%,f=right](joint_probability.svgz)

For two random variables, a **joint probability distribution** is a distribution
of all possible pairs of outputs (and analogously for more than two):

$$P(⁇x = x_2, ⁇y = y_1).$$

~~~
**Marginal distribution** is a distribution of one (or a subset) of the random
variables and can be obtained by summing over the other variable(s):
$$P(⁇x=x_2) = {\small ∑\nolimits}_y P(⁇x = x_2, ⁇y = y).$$

~~~
**Conditional distribution** is a distribution of one (or a subset) of the
random variables, given that another event has already occurred:
$$P(⁇x=x_2 | ⁇y=y_1) = P(⁇x = x_2, ⁇y = y_1) / P(⁇y = y_1).$$

~~~
If $P(⁇x, ⁇y) = P(⁇x) ⋅ P(⁇y)$ or $P(⁇x | ⁇y) = P(⁇x)$, random variables $⁇x$ and $⁇y$ are **independent**.

---
# Random Variables

## Expectation
The expectation of a function $f(x)$ with respect to a discrete probability
distribution $P(x)$ is defined as:
$$𝔼_{⁇x ∼ P}[f(x)] ≝ ∑_x P(x)f(x).$$

For continuous variables, it is computed as:
$$𝔼_{⁇x ∼ p}[f(x)] ≝ ∫_x p(x)f(x)\d x.$$

~~~
If the random variable is obvious from context, we can write only $𝔼_P[x]$
or even $𝔼[x]$.

~~~
Expectation is linear, i.e.,
$$𝔼_⁇x [αf(x) + βg(x)] = α𝔼_⁇x [f(x)] + β𝔼_⁇x [g(x)].$$

---
# Random Variables

## Variance
Variance measures how much the values of a random variable differ from its
mean $μ = 𝔼[x]$.

$$\begin{aligned}
  \Var(x) &≝ 𝔼\left[\big(x - 𝔼[x]\big)^2\right]\textrm{, or more generally,} \\
  \Var(f(x)) &≝ 𝔼\left[\big(f(x) - 𝔼[f(x)]\big)^2\right].
\end{aligned}$$

~~~
It is easy to see that
$$\Var(x) = 𝔼\left[x^2 - 2x𝔼[x] + \big(𝔼[x]\big)^2\right] = 𝔼\left[x^2\right] - \big(𝔼[x]\big)^2,$$
because $𝔼\big[2x𝔼[x]\big] = 2(𝔼[x])^2$.

~~~
Variance is connected to $E[x^2]$, the **second moment** of a random
variable – it is in fact a **centered** second moment.

---
# Estimators and Bias

An **estimator** is a rule for computing an estimate of a given value, often an
expectation of some random value(s).

~~~
For example, we might estimate _mean_ of a random variable by sampling a value
according to its probability distribution.

~~~
**Bias** of an estimator is the difference of the expected value of the estimator
and the true value being estimated:
$$\textrm{bias} = 𝔼[\textrm{estimate}] - \textrm{true estimated value}.$$

~~~
If the bias is zero, we call the estimator **unbiased**; otherwise, we call it
**biased**.

~~~
As an example, consider estimating $𝔼_P [f(x)]$ by generating a single sample
$x$ from $P$ and returning $f(x)$. Such an estimate is unbiased, because
$𝔼[\textrm{estimate}] = 𝔼_P [f(x)]$, which is exactly the true estimated value.

---
# Estimators and Bias

If we have a sequence of estimates, it might also happen that the bias converges
to zero. Consider the well-known sample estimate of variance. Given $⁇x_1,
\ldots, ⁇x_n$—independent and identically distributed random variables—we might
estimate the mean and variance as
$$μ̂ = \frac{1}{n} ∑\nolimits_i x_i,~~~σ̂^2 = \frac{1}{n} ∑\nolimits_i (x_i - μ̂)^2.$$
~~~
Such an estimate is biased, because $𝔼[σ̂^2] = (1 - \frac{1}{n})σ^2$, but the bias
converges to zero with increasing $n$.

~~~
Also, an unbiased estimator does not necessarily have a small variance – in some
cases, it can have a large variance. So, a biased estimator with a smaller variance
might be preferred.

---
section: SGD
# Gradient Descent

Sometimes it is more practical to search for the best model weights
in an iterative/incremental/sequential fashion. Either because there is too much
data, or the direct optimization is not feasible.

~~~
![w=50%,f=right](gradient_descent.svgz)

Assuming we are minimizing an error function
$$\argmin_{→w} E(→w),$$
we may use _gradient descent_:
$$→w ← →w - α∇_{→w} E(→w)$$

~~~
The constant $α$ is called a **learning rate** and specifies the “length”
of a step we perform in every iteration of the gradient descent.

---
# Gradient Descent Variants

Let $⇉X ∈ ℝ^{N×D}$, $→t ∈ ℝ^N$ be the training data, and denote
$p̂_\textrm{data}(→x, t) ≝ \frac{|\{i: (→x, t) = (→x_i, t_i)\}|}{N}$.
~~~
Assume that the error function can be computed as an expectation over the dataset:
$$E(→w) = 𝔼_{(→x, t)∼p̂_\textrm{data}} L\big(y(→x; →w), t\big),\textrm{~~so that~~}
  ∇_{→w} E(→w) = 𝔼_{(→x, t)∼p̂_\textrm{data}} ∇_{→w} L\big(y(→x; →w), t\big).$$

~~~
- **(Standard/Batch) Gradient Descent**: We use all training data to compute $∇_{→w} E(→w)$.

~~~
- **Stochastic (or Online) Gradient Descent**: We estimate $∇_{→w} E(→w)$ using
  a single random example from the training data. Such an estimate is unbiased,
  but very noisy.

$$∇_{→w} E(→w) ≈ ∇_{→w} L\big(y(→x; →w), t\big)\textrm{~~for randomly chosen~~}(→x, t)\textrm{~~from~~}p̂_\textrm{data}.$$

~~~
- **Minibatch SGD**: Trade-off between gradient descent and SGD – the
  expectation in $∇_{→w} E(→w)$ is estimated using $B$ random independent
  examples from the training data.

$$∇_{→w} E(→w) ≈ \frac{1}{B} ∑\nolimits_{i=1}^B ∇_{→w} L\big(y(→x_i; →w), t_i\big)
               \textrm{~~for randomly chosen~~}(→x_i, t_i)\textrm{~~from~~}p̂_\textrm{data}.$$

---
# Gradient Descent Convergence

Assume that we perform a stochastic gradient descent, using a sequence
of learning rates $α_i$, and using a noisy estimate $J(→w)$ of the real
gradient $∇_{→w} E(→w)$:
$$→w_{i+1} ← →w_i - α_i J(→w_i).$$

~~~
It can be proven (under some reasonable conditions; see Robbins and Monro algorithm, 1951) that if
the loss function $L$ is convex and continuous, then SGD converges to the unique
optimum almost surely if the sequence of learning rates $α_i$ fulfills the
following conditions:
$$∀i: α_i > 0,~~~∑_i α_i = ∞,~~~∑_i α_i^2 < ∞.$$

~~~
Note that the third condition implies that $α_i → 0$.

~~~
For non-convex loss functions, we can get guarantees of converging to a _local_
optimum only. However, note that finding the global minimum of an arbitrary
function is _at least NP-hard_.

---
# Gradient Descent Convergence

Convex functions mentioned on the previous slide are such that for $→u, →v$
and real $0 ≤ t ≤ 1$,
$$f(t→u + (1-t)→v) ≤ tf(→u) + (1-t)f(→v).$$

![w=88%,mw=50%,h=center](convex_2d.svgz)![w=66.5%,mw=50%,h=center](convex_3d.svgz)

~~~
A twice-differentiable function of a single variable is convex iff its second
derivative is always non-negative. (For functions of multiple variables,
the Hessian must be positive semi-definite.)

~~~
A local minimum of a convex function is always the unique global minimum.

~~~
Well-known examples of convex functions are $x^2$, $e^x$, $-\log x$, and also
the _sum of squares_.

---
section: LR-SGD
# Solving Linear Regression using SGD

To apply SGD on linear regression, we usually minimize one half of the mean squared error:
$$E(→w) = 𝔼_{(→x,t)∼p̂_\textrm{data}}\big[\tfrac{1}{2} (y(→x; →w) - t)^2\big] = 𝔼_{(→x,t)∼p̂_\textrm{data}}\big[\tfrac{1}{2} (→x^T →w - t)^2\big].$$

~~~
If we also include $L^2$ regularization, we get
$$E(→w) = 𝔼_{(→x,t)∼p̂_\textrm{data}}\big[\tfrac{1}{2} (→x^T →w - t)^2\big] + \tfrac{λ}{2}\|→w\|^2.$$

~~~
We then estimate the expectation by a minibatch of examples with indices $→b$ as
$$∑_{i ∈ →b} \frac{1}{|→b|} \Big(\tfrac{1}{2} (→x_i^T →w - t_i)^2\Big) + \tfrac{λ}{2}\|→w\|^2,$$
~~~
which gives us an estimate of a gradient
$$∇_{→w} E(→w) ≈ ∑_{i ∈ →b} \frac{1}{|→b|} \Big((→x_i^T →w - t_i) →x_i\Big) + λ→w.$$

---
# Solving Linear Regression using SGD

The computed gradient allows us to formulate the following algorithm for solving
linear regression with minibatch SGD.

~~~
<div class="algorithm">

**Input**: Dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ ℝ^N$), learning rate $α ∈ ℝ^+$, $L^2$ strength $λ ∈ ℝ$.<br>
**Output**: Weights $→w ∈ ℝ^D$ which hopefully minimize the regularized MSE of linear regression.

- $→w ← 0$ or we initialize $→w$ randomly
~~~
- repeat until convergence (or until our patience runs out):
  - sample a minibatch of examples with indices $→b$
~~~
    - either uniformly randomly,
~~~
    - or we may want to process all training instances before repeating them,
      which can be implemented by generating a random permutation and then
      splitting it into minibatch-sizes chunks
      - the most common option; one pass through the data is called an **epoch**
~~~
  - $→w ← →w - α ∑_{i ∈ →b} \tfrac{1}{|→b|} \big((→x_i^T→w-t_i)→x_i\big) - αλ→w$
</div>

---
section: Features
# Features

Recall that the _input_ instance values are usually the raw observations and are
given. However, we might extend them suitably before running a machine learning
algorithm, especially if the algorithm is linear or otherwise limited and cannot
represent an arbitrary function. Such instance representations are called _features_.

~~~
We already saw this in the example from the previous lecture, where even if
our training examples were $x$ and $t$, we performed the linear regression
using features $(x^0, x^1, …, x^M)$:
![w=40%,h=center](../01/sin_lr.svgz)

---
# Features

Generally, it would be best if we have machine learning algorithms processing
only the raw inputs. However, many algorithms are capable of representing
only a limited set of functions (for eg, linear ones), and in that case,
_feature engineering_ plays a major part in the final model performance.
Feature engineering is the process of constructing features from raw inputs.

Commonly used features are:
~~~
- **polynomial features** of degree $p$: Given features $(x_1, x_2, …, x_D)$, we
  might consider _all_ products of $p$ input values. Therefore, polynomial
  features of degree 2 would consist of $x_i^2 \,∀i$ and of $x_i x_j \,∀i≠j$.

~~~
- **categorical one-hot features**: Assume, for example, that a day in a week is
  represented in the input as an integer value of 1 to 7, or a breed of a dog is
  expressed as an integer value of 0 to 366.
~~~
  Using these integral values as an input to linear regression makes little sense
  – instead, it might be better to learn weights for individual days in a week or
  for individual dog breeds.
~~~
  We might therefore represent input classes by binary indicators for every
  class, giving rise to a **one-hot** representation, where an input integral value
  $1 ≤ v ≤ L$ is represented as $L$ binary values, which are all zero except for
  the $v$-th one, which is one.

