title: NPFL129, Lecture 9
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Model Combination, Decision Trees, Random Forests

## Milan Straka

### November 30, 2020

---
section: Covariance
# Covariance

Given a collection of random variables $â‡x_1, â€¦, â‡x_N$, we know that
$$ð”¼\left[âˆ‘\nolimits_i â‡x_i\right] = âˆ‘_i ð”¼ \big[â‡x_i\big].$$

~~~
But how about $\Var(âˆ‘_i â‡x_i)$?

~~~
$$\begin{aligned}
  \Var\left(âˆ‘\nolimits_i â‡x_i\right)
    &= ð”¼\left[\left(âˆ‘\nolimits_i â‡x_i - âˆ‘\nolimits_i ð”¼[â‡x_i]\right)^2\right] \\
    &= ð”¼\left[\left(âˆ‘\nolimits_i \big(â‡x_i - ð”¼[â‡x_i]\big)\right)^2\right] \\
    &= ð”¼\left[âˆ‘\nolimits_i âˆ‘\nolimits_j \big(â‡x_i - ð”¼[â‡x_i]\big) \big(â‡x_j - ð”¼[â‡x_j]\big)\right] \\
    &= âˆ‘_i âˆ‘_j ð”¼\left[\big(â‡x_i - ð”¼[â‡x_i]\big) \big(â‡x_j - ð”¼[â‡x_j]\big)\right].
\end{aligned}$$

---
# Covariance

We define **covariance** of two random variables $â‡x, â‡y$ as
$$\cov(â‡x, â‡y) = ð”¼\Big[\big(â‡x - ð”¼[â‡x]\big) \big(â‡y - ð”¼[â‡y]\big)\Big].$$

~~~
Then,
$$\Var\left(âˆ‘\nolimits_i â‡x_i\right) = âˆ‘_i âˆ‘_j \cov(â‡x_i, â‡x_j).$$

~~~
Note that $\cov(â‡x, â‡x) = \Var(â‡x)$ and that we can write covariance as
$$\begin{aligned}
  \cov(â‡x, â‡y)
   &= ð”¼\Big[\big(â‡x - ð”¼[â‡x]\big) \big(â‡y - ð”¼[â‡y]\big)\Big] \\
   &= ð”¼\big[â‡x â‡y - â‡x ð”¼[â‡y] - ð”¼[â‡x] â‡y + ð”¼[â‡x] ð”¼[â‡y]\big] \\
   &= ð”¼\big[â‡x â‡y\big] - ð”¼\big[â‡x\big] ð”¼\big[â‡y\big].
\end{aligned}$$

---
section: Correlation
# Corellation

Two random variables $â‡x, â‡y$ are **uncorrelated**, if $\cov(â‡x, â‡y) = 0$;
otherwise, they are **correlated**.

~~~
Note that two _independent_ random variables are uncorrelated, because
$$\begin{aligned}
  \cov(â‡x, â‡y)
   &= ð”¼\Big[\big(â‡x - ð”¼[â‡x]\big) \big(â‡y - ð”¼[â‡y]\big)\Big] \\
   &= âˆ‘_{x,y} P(x, y) \big(x - ð”¼[x]\big) \big(y - ð”¼[y]\big) \\
   &= âˆ‘_{x,y} P(x) \big(x - ð”¼[x]\big) P(y) \big(y - ð”¼[y]\big) \\
   &= âˆ‘_x P(x) \big(x - ð”¼[x]\big) âˆ‘_y P(y) \big(y - ð”¼[y]\big) \\
   &= ð”¼_â‡x \big[â‡x - ð”¼[â‡x]\big] ð”¼_â‡y \big[â‡y - ð”¼[â‡y]\big] = 0.
\end{aligned}$$

~~~
However, dependent random variables can be uncorrelated â€“ random
uniform $â‡x$ on $[-1, 1]$ and $â‡y = |â‡x|$ are not independent ($â‡y$ is
completely determined by $â‡x$), but they are uncorrelated.

---
# Pearson correlation coefficient

There are several ways to measure correlation of random variables $â‡x, â‡y$.

**Pearson correlation coefficient**, denoted as $Ï$ or $r$, is defined as
$$\begin{aligned}
  Ï &â‰ \frac{\cov(â‡x, â‡y)}{\sqrt{\Var(â‡x)} \sqrt{\Var(â‡y)}} \\
  r &â‰ \frac{âˆ‘_i (x_i - xÌ„) (y_i - È³)}{\sqrt{âˆ‘_i (x_i - xÌ„)^2} \sqrt{âˆ‘_i (y_i - È³)^2}},
\end{aligned}$$
where:
~~~
- $Ï$ is used when the full expectation is computed (population Pearson
  correlation coefficient);
~~~
- $r$ is used when estimating the coefficient from data (sample Pearson
  correlation coefficient).
  - $xÌ„$ and $È³$ are sample estimates of mean

---
# Pearson correlation coefficient

Pearson correlation coefficient is in fact normalized covariance, because
applying Cauchy-Schwarz inequality $\langle u, v\rangle â‰¤ \sqrt{\langle u, u\rangle} \sqrt{\langle v, v\rangle}$
on $\langle x, y\rangle â‰ ð”¼[xy]$ yields
$$-1 â‰¤ Ï, r â‰¤ 1.$$

~~~
Pearson correlation coefficient quantifies **linear dependence** of the two
random variables.

![w=44%](correlation_coefficient.png)![w=56%](correlation_examples.svgz)

---
# Pearson correlation coefficient

![w=70%,f=right](ancombes_quartet.svgz)

The four displayed variables have the same mean 7.5, variance 4.12,
Pearson correlation coefficient 0.816 and regression line $3 + \frac{1}{2}x$:

---
# Nonlinear Correlation

To measure also non-linear correlation, the most common coefficients are:

~~~
- **Spearman's rank correlation coefficient** $Ï$, which is Pearson correlation
  coefficient measured on **ranks** of the original data;

  ![w=100%](spearman.svgz)

~~~
- **Kendall rank correlation coefficient** $Ï„$, measuring the amount of
  pairs where $y$ inceases when $x$ does and pairs when $y$ decreases when $x$
  does:
  $$Ï„ â‰ \frac{|\{\mathrm{pairs~such~that}~x_j > x_i, y_j > y_i\}| - |\{\mathrm{pairs~such~that}~x_j > x_i, y_j < y_i\}|}{\binom{n}{2}}.$$

---
section: Model Combination
# Model Combination aka Ensembling

Ensembling is combining several models with a goal of reaching higher
performance.

~~~
The simplest approach is to train several independent models and then combine
their outputs by averaging or voting.
~~~
- The terminology varies, but for classification, voting (or hard voting)
  usually means predicting majority, while averaging (or soft voting) means
  predicting the class with highest probability.

~~~
The main idea behind ensembling is that if models have uncorrelated
errors, then by averaging model outputs the errors will cancel out.

~~~
Concretely, if $â‡x_1, â€¦, â‡x_N$ are uncorrelated identically distributed random variables, we
get that
$$\Var\left(\frac{1}{N} âˆ‘\nolimits_i â‡x_i\right)
  = âˆ‘_i \Var\left(\frac{1}{N} â‡x_i\right)
  = âˆ‘_i \frac{1}{N^2} \Var\left(â‡x_i\right)
  = \frac{1}{N} \Var\left(â‡x\right).$$

---
# Bagging

For neural network models, training models with independent initialization is
usually enough, given that the loss has many local minima, so the models tend to
be quite independent just when using different initialization.

~~~
However, algorithms with a convex loss functions usually converge to the same
optimum independent on randomization.

~~~
In these cases, we can use **bagging**, which stands for **bootstrap
aggregation**.

~~~
![w=50%,f=right](bagging.svgz)

In bagging, we construct a different dataset for every model to be trained.
We construct it using **bootstrapping** â€“ we sample as many training instances
as the original dataset has, but **with replacement**.

Such dataset is sampled using the same empirical data distribution and has the
same size, but is not identical.

---
section: DecisionTree
# Decision Trees

The idea of decision trees is to partition the input space into usually cuboid
regions and solving each region with a simpler model.

~~~
We focus on **Classification and Regression Trees** (CART; Breiman et al.,
1984), but there are additional variants like ID3, C4.5, â€¦

~~~
![w=80%,mw=49%,h=center](tree_partitioning.svgz)
~~~
![w=90%,mw=49%,h=center](tree_representation.svgz)

---
# Regression Decision Trees

Assume we have an input dataset $â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$. At the beginning,
the decision tree is just a single node and all input examples belong to this
node. We denote $I_ð“£$ the set of training example indices belonging to a leaf
node $ð“£$.

~~~
For each leaf, our model will predict the average of the training examples
belonging to that leaf, $tÌ‚_ð“£ = \frac{1}{|I_ð“£|} âˆ‘_{i âˆˆ I_ð“£} t_i$.

~~~
We will use a **criterion** $c_ð“£$ telling us how _uniform_ or _homogeneous_ are the
training examples belonging to a leaf node $ð“£$ â€“ for regression, we will
employ the sum of squares error between the examples belonging to the node and the predicted
value in that node; this is proportional to the variance of the training examples belonging
to the leaf node $ð“£$, multiplied by the number of the examples. Note that even
if it not _mean_ squared error, it is sometimes denoted as MSE.
$$c_\textrm{SE}(ð“£) â‰ âˆ‘_{i âˆˆ I_ð“£} (t_i - tÌ‚_ð“£)^2\textrm{, where } tÌ‚_ð“£=\frac{1}{|I_ð“£|} âˆ‘_{i âˆˆ I_ð“£} t_i.$$

---
# Tree Construction

To split a node, the goal is to find a feature and its value such that when
splitting a node $ð“£$ into $ð“£_L$ and $ð“£_R$, the resulting regions decrease the
overall criterion value the most, i.e., the difference $c_{ð“£_L} + c_{ð“£_R} - c_ð“£$
is the lowest.

~~~
Usually we have several constraints, we mention on the most common ones:
- **maximum tree depth**: we do not split nodes with this depth;
~~~
- **minimum examples to split**: we only split nodes with this many training
  examples;
~~~
- **maximum number of leaf nodes**: we split until we reach the given number of
  leaves.

~~~
The tree is usually built in one of two ways:
- if the number of leaf nodes is unlimited, we usually build the tree in
  a depth-first manner, recursively splitting every leaf until some
  of the above constraint is invalidated;
~~~
- if the maximum number of leaf nodes is given, we usually split such leaf $ð“£$
  where the criterion difference $c_{ð“£_L} + c_{ð“£_R} - c_ð“£$ is the lowest.

---
# Pruning

To control overfitting, the mentioned constraints can be used.

~~~
Additionally, **pruning** can also be used. After training, we might decide that
some subtrees are not necessary and _prune_ them (replacing them by a leaf).
Pruning can be used both as a regularization and model compression.

~~~
There are many heuristics to prune a decision tree; Scikit-learn implements
**minimal cost-complexity pruning**:
~~~
- we extend the criterion to _cost-complexity criterion_ as

  - for a leaf, $c_Î±(Ï„) = c(Ï„) + Î±$,
~~~
  - for a subtree $T_t$ with a root $t$, $c_Î±(T_t) = âˆ‘_\mathrm{leaves} c_Î±(Ï„) =  âˆ‘_\mathrm{leaves} c(Ï„) + Î±|\mathrm{leaves}|$;
~~~
- generally a criterion in a node $t$ is greater or equal to the sum of
  criteria of its leaves;
~~~
- $Î±_\mathrm{eff}$ is the value of $Î±$ such that the above two cost-complexity
  quantities are equal
  - $Î±_\mathrm{eff} = \big(c(Ï„) - c(T_t)\big) / \big(|\mathrm{leaves}| - 1\big)$;
~~~
- we then prune the nodes in the order of increasing $Î±_\mathrm{eff}$.

---
# Classification Decision Trees

For multi-class classification, we predict such class most frequent
in the training examples belonging to a leaf $ð“£$.

~~~
To define the criterions, let us denote the average probability for class $k$ in
a region $ð“£$ as $p_{ð“£}(k)$.

~~~
For classification trees, one of the following two criterions is usually used:

- **Gini index**:
  $$c_\textrm{Gini}(ð“£) â‰ |I_ð“£| âˆ‘_k p_ð“£(k) \big(1 - p_ð“£(k)\big)$$

~~~
- **Entropy Criterion**
  $$c_\textrm{entropy}(ð“£) â‰ |I_ð“£| H(p_ð“£) = - |I_ð“£| âˆ‘_{\substack{k\\p_ð“£(k) â‰  0}} p_ð“£(k) \log p_ð“£(k)$$

---
section: Gini and Entropy Losses
# Binary Gini as (M)SE Loss

Recall that $I_ð“£$ denotes the set of training example indices belonging to a leaf node $ð“£$,
let $n_ð“£(0)$ be the number of examples with target value 0, $n_ð“£(1)$ be the
number of examples with target value 1, and let $p_ð“£ = \frac{1}{|I_ð“£|} âˆ‘_{i âˆˆ I_ð“£} t_i$.

~~~
Consider sum of squares loss $ð“›(p) = âˆ‘_{i âˆˆ I_ð“£} (p - t_i)^2$.

~~~
By setting the derivative of the loss to zero, we get that the $p$ minimizing
the loss fulfils $|I_ð“£|p = âˆ‘_{i âˆˆ I_ð“£} t_i$, i.e., $p = p_ð“£$.

~~~
The value of the loss is then
$$\begin{aligned}
  ð“›(p_ð“£) &= âˆ‘_{i âˆˆ I_ð“£} (p_ð“£ - t_i)^2 = n_ð“£(0) (p_ð“£ - 0)^2 + n_ð“£(1) (p_ð“£ - 1)^2 \\
         &= \frac{n_ð“£(0) n_ð“£(1)^2}{\big(n_ð“£(0) + n_ð“£(1)\big)^2} + \frac{n_ð“£(1) n_ð“£(0)^2}{\big(n_ð“£(0) + n_ð“£(1)\big)^2}
          = \frac{n_ð“£(0) n_ð“£(1)}{n_ð“£(0) + n_ð“£(1)} \\
         &= \big(n_ð“£(0) + n_ð“£(1)\big) (1-p_ð“£) p_ð“£ = |I_ð“£| p_ð“£ (1-p_ð“£)
\end{aligned}$$

---
# Entropy as NLL Loss

Again let $I_ð“£$ denote the set of training example indices belonging to a leaf node $ð“£$,
let $n_ð“£(c)$ be the number of examples with target value $c$, and let
$p_ð“£(c) = \frac{n_ð“£(c)}{|I_ð“£|} = \frac{1}{|I_ð“£|} âˆ‘_{i âˆˆ I_ð“£} [t_i = c]$.

~~~
Consider a distribution $â†’p$ on $K$ classes and non-averaged NLL loss $ð“›(â†’p) = âˆ‘_{i âˆˆ I_ð“£} - \log p_{t_i}$.

~~~
By setting the derivative of the loss with respect to $p_c$ to zero (using
a Lagrangian with constraint $âˆ‘_c p_c = 1$),
we get that the $â†’p$ minimizing the loss fulfils $p_c = p_ð“£(c)$.

~~~
The value of the loss with respect to $p_ð“£$ is then
$$\begin{aligned}
  ð“›(p_ð“£) &= âˆ‘_{i âˆˆ I_ð“£} - \log p_{t_i} \\
         &= - âˆ‘_{\substack{c\\p_ð“£(c)â‰ 0}} n_ð“£(c) \log p_ð“£(c) \\
         &= - |I_ð“£| âˆ‘_{\substack{c\\p_ð“£(c)â‰ 0}}  p_ð“£(c) \log p_ð“£(c) = |I_ð“£| H(p_ð“£).
\end{aligned}$$

---
section: RandomForests
# Random Forests

Bagging of data combined with random subset of features (sometimes
called _feature bagging_).

![w=80%,h=center](random_forest.svgz)

---
# Random Forests

## Bagging

Every decision tree is trained using bagging (on a bootstrapped dataset).

~~~
## Random Subset of Features

During each node split, only a random subset of features is considered, when
finding the best split. A fresh random subset is used for every node.

~~~
## Extra Trees

The so-called extra trees are even more randomized, not finding the best
possible feature value when choosing a split, but considering uniformly
random samples from a feature's empirical range (minimum and maximum in the
training data).


---
# Demos

https://cs.stanford.edu/~karpathy/svmjs/demo/demoforest.html
