title: NPFL129, Lecture 9
class: title, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Decision Trees, Random Forests

## JindÅ™ich LibovickÃ½ <small>(reusing materials by Milan Straka)</small>

### November 28, 2023

---
class: middle
# Today's Lecture Objectives

After this lecture you should be able to

- Implement Decision Trees and Random Forests for classification and regression

- Explain how the splitting criterion depend on optimized loss function

- Tell how Random Forests differ from Gradient Boosted Decision Trees

---
section: DecisionTree
# Decision Trees

The idea of decision trees is to partition the input space into regions and
solving each region with a simpler model.

~~~
We focus on **Classification and Regression Trees** (CART; Breiman et al.,
1984), but there are additional variants like ID3, C4.5, â€¦

~~~
![w=80%,mw=49%,h=center](tree_partitioning.svgz)
~~~
![w=90%,mw=49%,h=center](tree_representation.svgz)

---
# Inference and Training

## Inference

![w=30%,f=right](tree_meme.jpg)

- Just follow the branching rules until you reach a leaf.

- Output a prediction (real value/distribution/predicted class) based on the leaf.

~~~
## Training

- Training data is stored in tree leaves -- the leaf prediction is based on what is data items are in the leaf.

- At the beginning the tree is a single leaf node.

- Adding a node = leaf $\rightarrow$ decision node + 2 leafs

- The goal of training = finding the most consistent leafs for the prediction

Later, we will show that the consistency measures follow from the loss function, we are optimizing.

---
# Regression Decision Trees

Assume we have an input dataset $â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$.
~~~
At the beginning, the decision tree is just a single node and all input examples
belong to this node.
~~~
We denote $I_ğ“£$ the set of training example indices belonging to a node $ğ“£$.

~~~
For each leaf (a node without children), our model predicts the average of the
training examples belonging to that leaf, $tÌ‚_ğ“£ = \frac{1}{|I_ğ“£|} âˆ‘_{i âˆˆ I_ğ“£} t_i$.

~~~
We use a **criterion** $c_ğ“£$ telling us how _uniform_ or _homogeneous_ the
training examples of a node $ğ“£$ are
~~~
â€“ for regression, we employ the sum of squares error between the examples
belonging to the node and the predicted value in that node; this is proportional
to the variance of the training examples belonging to the node $ğ“£$, multiplied
by the number of the examples. Note that even if it is not _mean_ squared error,
it is sometimes denoted as MSE.
$$c_\textrm{SE}(ğ“£) â‰ âˆ‘_{i âˆˆ I_ğ“£} (t_i - tÌ‚_ğ“£)^2\textrm{, ~where~ } tÌ‚_ğ“£=\frac{1}{|I_ğ“£|} âˆ‘_{i âˆˆ I_ğ“£} t_i.$$

---
class: middle
# Tree Construction

To split a node, the goal is to find

1. A feature and <small>(i.e., a for loop over all features)</small>

2. Its value <small>(i.e., a for loop over all unique feature values)</small>

such that when splitting a node $ğ“£$ into $ğ“£_L$ and $ğ“£_R$, the resulting regions
decrease the overall criterion value the most, i.e., the difference $c_{ğ“£_L} +
c_{ğ“£_R} - c_ğ“£$ is the lowest.


---
# Tree Construction: Heuristics
We usually employ several constraints, the most common ones are:
- **maximum tree depth**: we do not split nodes with this depth;

~~~
- **minimum examples to split**: we only split nodes with this many training
  examples;
~~~
- **maximum number of leaf nodes**: we split until we reach the given number of
  leaves.

~~~
The tree is usually built in one of two ways:
- if the number of leaf nodes is unlimited, we usually build the tree in
  a depth-first manner, recursively splitting every leaf until one
  of the above constraints is invalidated;
~~~
- if the maximum number of leaf nodes is given, we usually split such leaf $ğ“£$
  where the criterion difference $c_{ğ“£_L} + c_{ğ“£_R} - c_ğ“£$ is the lowest.

~~~
<br />

_Terminological note:_ Decision tree with unlimited size can be considered a
non-parametric model: it is a way of building an index. With a limited size,
it has a fixed number of parameters to be learned and it can be considered a
parametric model.

---
# Classification Decision Trees

For multi-class classification, we predict the class which is the most frequent
in the training examples belonging to a leaf $ğ“£$.

~~~
To define the criteria, let us denote the average probability for class $k$ in
a region $ğ“£$ as $p_{ğ“£}(k)$.

~~~
For classification trees, one of the following two criteria is usually used:

- **Gini index**, also called **Gini impurity**, measuring how often a randomly
  chosen element would be incorrectly labeled if it was randomly labeled
  according to $â†’p_ğ“£$:
  $$c_\textrm{Gini}(ğ“£) â‰ |I_ğ“£| âˆ‘_k p_ğ“£(k) \big(1 - p_ğ“£(k)\big),$$

~~~
- **Entropy Criterion**
  $$c_\textrm{entropy}(ğ“£) â‰ |I_ğ“£| â‹… H(â†’p_ğ“£) = - |I_ğ“£| âˆ‘_{\substack{k\\p_ğ“£(k) â‰  0}} p_ğ“£(k) \log p_ğ“£(k).$$

---
# From Loss Function to Splitting Criterion

~~~
- Training GLMs and MLPs is formulated as optimizing a loss function.

~~~
- For an already constructed decision tree, we can do it the same way. For each
  leaf, do the optimization and find the best parameter.

~~~
- So far, we were always interested in $\arg\min$, i.e., parameters that
  minimize the loss.

~~~
- If we plug the $\arg\min$ value in the loss function, we get the minimum
  reachable loss for the given tree structure.

~~~
- By splitting a leaf, we want to decrease the minimum reachable loss
  $\Rightarrow$ the **minimum node loss is the splitting criterion**.

---
section: Gini and Entropy Losses
# Binary Gini as (M)SE Loss

Recall that $I_ğ“£$ denotes the set of training example indices belonging to a leaf node $ğ“£$,
let $n_ğ“£(0)$ be the number of examples with target value 0, $n_ğ“£(1)$ be the
number of examples with target value 1, and let $p_ğ“£ = \frac{1}{|I_ğ“£|} âˆ‘_{i âˆˆ I_ğ“£} t_i = \frac{n_ğ“£(1)}{n_ğ“£(0) + n_ğ“£(1)}$.

~~~
Consider sum of squares loss $L(p) = âˆ‘_{i âˆˆ I_ğ“£} (p - t_i)^2$.

~~~
By setting the derivative of the loss to zero, we get that the $p$ minimizing
the loss fulfills $|I_ğ“£|p = âˆ‘_{i âˆˆ I_ğ“£} t_i$, i.e., $p = p_ğ“£$.

~~~
The value of the loss is then

$\displaystyle L(p_ğ“£) = âˆ‘_{i âˆˆ I_ğ“£} (p_ğ“£ - t_i)^2$
~~~
$= n_ğ“£(0) \textcolor{blue}{(p_ğ“£ - 0)^2} + n_ğ“£(1) \textcolor{magenta}{(p_ğ“£ - 1)^2}$

~~~
$\displaystyle \phantom{L(p_ğ“£)}
  = \frac{n_ğ“£(0) \textcolor{blue}{n_ğ“£(1)^2}}{\textcolor{blue}{\big(n_ğ“£(0) + n_ğ“£(1)\big)^2}}
    + \frac{n_ğ“£(1) \textcolor{magenta}{n_ğ“£(0)^2}}{\textcolor{magenta}{\big(n_ğ“£(0) + n_ğ“£(1)\big)^2}}$
~~~
$\displaystyle = \mathrlap{
  \frac{(n_ğ“£(1) + n_ğ“£(0)) \textcolor{green}{n_ğ“£(0)} \textcolor{red}{n_ğ“£(1)}}
       {\textcolor{green}{\big(n_ğ“£(0) + n_ğ“£(1)\big)} \textcolor{red}{\big(n_ğ“£(0) + n_ğ“£(1)\big)}}}$

~~~
$\displaystyle \phantom{L(p_ğ“£)} = \big(n_ğ“£(0) + n_ğ“£(1)\big) \textcolor{green}{(1-p_ğ“£)} \textcolor{red}{p_ğ“£} = |I_ğ“£| â‹… p_ğ“£ (1-p_ğ“£).$

---
# Entropy as NLL Loss

Again let $I_ğ“£$ denote the set of training example indices belonging to a leaf node $ğ“£$,
let $n_ğ“£(k)$ be the number of examples with target value $k$, and let
$p_ğ“£(k) = \frac{1}{|I_ğ“£|} âˆ‘_{i âˆˆ I_ğ“£} [t_i = k] = \frac{n_ğ“£(k)}{|I_ğ“£|}$.

~~~
Consider a distribution $â†’p$ on $K$ classes and non-averaged NLL loss $L(â†’p) = âˆ‘_{i âˆˆ I_ğ“£} - \log p_{t_i}$.

~~~
By setting the derivative of the loss with respect to $p_k$ to zero (using
a Lagrangian with constraint $âˆ‘_k p_k = 1$), we get that the $â†’p$ minimizing the
loss fulfills $p_k = p_ğ“£(k)$.

~~~
The value of the loss with respect to $â†’p_ğ“£$ is then

$\displaystyle \kern10em\mathllap{L(â†’p_ğ“£)} = âˆ‘_{i âˆˆ I_ğ“£} - \log p_{t_i}$

~~~
$\displaystyle \kern10em{} = - âˆ‘_{\substack{k\\p_ğ“£(k)â‰ 0}} n_ğ“£(k) \log p_ğ“£(k)$

~~~
$\displaystyle \kern10em{} = - |I_ğ“£| âˆ‘_{\substack{k\\p_ğ“£(k)â‰ 0}}  p_ğ“£(k) \log p_ğ“£(k) = |I_ğ“£| â‹… H(â†’p_ğ“£).$


---
section: RandomForests
# Random Forests

Bagging of data combined with a random subset of features (sometimes
called _feature bagging_).

![w=80%,h=center](random_forest.svgz)

---
# Random Forests

## Bagging

Every decision tree is trained using bagging (on a bootstrapped dataset).

~~~
## Random Subset of Features

During each node split, only a random subset of features is considered when
finding the best split. A fresh random subset is used for every node.

~~~
## Extra Trees

The so-called extra trees are even more randomized, not finding the best
possible feature value when choosing a split, but considering uniformly
random samples from a feature's empirical range (minimum and maximum in the
training data).

~~~
## Demo

https://cs.stanford.edu/~karpathy/svmjs/demo/demoforest.html

---
section: Gradient Boosting
# Gradient Boosting Decision Trees: Overview

~~~
- A collection of decision trees: each tree fixes the error of the previous trees

~~~
- Decreasing error = step in the direction of the error/loss gradient

~~~
- GBDT repeat SGD computation at the inference time (start with a stupid
  estimate and do gradient steps that improve the estimate)

~~~
- One tree is one gradient descent step

## Theoretical steps to be taken

~~~
- Derive what exactly we want to the intermediate steps to predict, so we do
  more clever steps than SGD that just moves with gradient

~~~
- Based on that, express the per-node loss and splitting criterion as the
  minimum reachable loss value

<center>For regression, binary classification and multiclass classification.</center>

---
# Gradient Boosting Decision Trees

GBDT are trained sequentially to correct the errors of the previous trees.

<br />

~~~
![w=70%,f=right,mh=80%,v=middle](gbt_example.svgz)

If we denote $y_t$ as the prediction function of the $t^\mathrm{th}$
tree, the prediction of the whole collection is then
$$y(â†’x_i) = âˆ‘_{t=1}^T y_t(â†’x_i; â†’w_t),$$
where $â†’w_t$ is a vector of parameters (leaf values, to be concrete) of the
$t^\mathrm{th}$ tree.

---
# Gradient Boosting for Regression

Considering a regression task first, we define the overall loss as

$$E(â†’w) = âˆ‘_i \ell\big(t_i, y(â†’x_i; â†’w)\big) + âˆ‘_{t=1}^T \frac{1}{2} Î» \big\|â†’w_t\big\|^2,$$
where
~~~
- $â†’w = (â†’w_1, â€¦, â†’w_T)$ are the parameters (leaf values) of the trees;

~~~
- $\ell\big(t_i, y(â†’x_i; â†’w)\big)$ is an per-example loss, $(t_i - y(â†’x_i;
  â†’w))^2$ for regression;
~~~
- the $Î»$ is the usual $L^2$-regularization strength.

---
# Gradient Boosting for Regression

To construct the trees sequentially, we extend the definition to

$$E^{(t)}(â†’w_t; â†’w_{1..t-1}) = âˆ‘_i \Big[\ell\big(t_i, y^{(t-1)}(â†’x_i; â†’w_{1..t-1}) + y_t(â†’x_i; â†’w_t)\big)\Big] + \frac{1}{2} Î» \big\|â†’w_t\big\|^2.$$

~~~
In the following text, we drop the parameters of $y^{(t-1)}$ and $y_t$ for
brevity.

~~~
The original idea of gradient boosting was to set
$$y_t(â†’x_i) â† -\frac{âˆ‚\ell\big(t_i, y^{(t-1)}(â†’x_i)\big)}{âˆ‚y^{(t-1)}(â†’x_i)}
  \textcolor{gray}{=-\frac{âˆ‚\ell\big(t_i, y\big)}{âˆ‚y}\bigg|_{y=y^{(t-1)}(â†’x_i)}}$$
as a direction minimizing the residual loss
~~~
and then finding a suitable constant $Î³_t$, which would minimize the loss
$$âˆ‘_i \Big[\ell\big(t_i, y^{(t-1)}(â†’x_i) + Î³_t y_t(â†’x_i)\big)\Big] + \frac{1}{2} Î» \big\|â†’w_t\big\|^2.$$

---
class: middle
# Today's Lecture Objectives

After this lecture you should be able to

- Implement Decision Trees and Random Forests for classification and regression

- Explain how the splitting criterion depend on optimized loss function

- Tell how Random Forests differ from Gradient Boosted Decision Trees
