title: NPFL129, Lecture 9
class: title, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Decision Trees, Random Forests

## JindÅ™ich LibovickÃ½ <small>(reusing materials by Milan Straka)</small>

### November 28, 2024

---
class: middle
# Today's Lecture Objectives

After this lecture you should be able to

- Implement Decision Trees and Random Forests for classification and regression

- Explain how the splitting criterion depends on the optimized loss function

---
section: Decision Trees
class: section
# Decision Trees

---
# Decision Trees

The idea of decision trees is to partition the input space into regions and
solve each region with a simpler model.

~~~
We focus on **Classification and Regression Trees** (CART; Breiman et al.,
1984), but there are additional variants like ID3, C4.5, â€¦

~~~
![w=80%,mw=49%,h=center](tree_partitioning.svgz)
~~~
![w=90%,mw=49%,h=center](tree_representation.svgz)

---
# Inference and Training

## Inference

![w=30%,f=right](tree_meme.jpg)

- Just follow the branching rules until you reach a leaf.

- Output a prediction (real value/distribution/predicted class) based on the leaf.

~~~
## Training

- Training data is stored in tree leaves â€“ the leaf prediction is based on what data items are in the leaf.

- At the beginning the tree is a single leaf node.

- Adding a node = leaf $\rightarrow$ decision node + 2 leaves.

- The goal of training = to find the most consistent leaves for the prediction.

Later, we will show that the consistency measures follow from the loss function we are optimizing.

---
# Regression Decision Trees

Assume we have an input dataset $â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$.
~~~
At the beginning, the decision tree is just a single node and all input examples
belong to this node.
~~~
We denote $I_ğ“£$ the set of training example indices belonging to a node $ğ“£$.

~~~
For each leaf (a node without children), our model predicts the average of the
training examples belonging to that leaf, $tÌ‚_ğ“£ = \frac{1}{|I_ğ“£|} âˆ‘_{i âˆˆ I_ğ“£} t_i$.

~~~
We use a **criterion** $c_ğ“£$ telling us how _uniform_ or _homogeneous_ the
training examples of a node $ğ“£$ are
~~~
â€“ for regression, we employ the sum of squares error between the examples
belonging to the node and the predicted value in that node; this is proportional
to the variance of the training examples belonging to the node $ğ“£$, multiplied
by the number of the examples. Note that even if it is not _mean_ squared error,
it is sometimes denoted as MSE.
$$c_\textrm{SE}(ğ“£) â‰ âˆ‘_{i âˆˆ I_ğ“£} (t_i - tÌ‚_ğ“£)^2\textrm{, ~where~ } tÌ‚_ğ“£=\frac{1}{|I_ğ“£|} âˆ‘_{i âˆˆ I_ğ“£} t_i.$$

---
class: middle
# Tree Construction

To split a node, the goal is to find

1. a feature and <small>(i.e., a for-loop over all features)</small>

2. its value <small>(i.e., a for-loop over all unique feature values)</small>

such that when splitting a node $ğ“£$ into $ğ“£_L$ and $ğ“£_R$, the resulting regions
decrease the overall criterion value the most, i.e., the difference $c_{ğ“£_L} +
c_{ğ“£_R} - c_ğ“£$ is the lowest.


---
# Tree Construction: Heuristics
We usually employ several constraints, the most common ones are:
- **maximum tree depth**: we do not split nodes with this depth;

~~~
- **minimum examples to split**: we do not split nodes with this many training
  examples;
~~~
- **maximum number of leaf nodes**: we split until we reach the given number of
  leaves.

~~~
The tree is usually built in one of two ways:
- if the number of leaf nodes is unlimited, we usually build the tree in
  a depth-first manner, recursively splitting every leaf until one
  of the above constraints is met;
~~~
- if the maximum number of leaf nodes is given, we usually split such leaf $ğ“£$
  where the criterion difference $c_{ğ“£_L} + c_{ğ“£_R} - c_ğ“£$ is the lowest.

~~~
<br />

_Terminological note:_ Decision tree with unlimited size can be considered a
non-parametric model: it is a way of building an index. With a limited size,
it has a fixed number of parameters to be learned and it can be considered a
parametric model.

---
# Classification Decision Trees

For multi-class classification, we predict the class which is the most frequent
in the training examples belonging to a leaf $ğ“£$.

~~~
To define the criteria, let us denote the average probability for class $k$ in
a region $ğ“£$ as $p_{ğ“£}(k)$.

~~~
For classification trees, one of the following two criteria is usually used:

- **Gini index**, also called **Gini impurity**, measuring how often a randomly
  chosen element would be incorrectly labeled if it was randomly labeled
  according to $â†’p_ğ“£$:
  $$c_\textrm{Gini}(ğ“£) â‰ |I_ğ“£| âˆ‘_k p_ğ“£(k) \big(1 - p_ğ“£(k)\big);$$

~~~
- **entropy criterion**
  $$c_\textrm{entropy}(ğ“£) â‰ |I_ğ“£| â‹… H(â†’p_ğ“£) = - |I_ğ“£| âˆ‘_{\substack{k\\p_ğ“£(k) â‰  0}} p_ğ“£(k) \log p_ğ“£(k).$$

---
# From Loss Function to Splitting Criterion

~~~
- Training GLMs and MLPs is formulated as optimizing a loss function.

~~~
- For an already constructed decision tree, we can do it the same way. For each
  leaf, do the optimization and find the best parameter.

~~~
- So far, we were always interested in $\arg\min$, i.e., parameters that
  minimize the loss.

~~~
- If we plug the $\arg\min$ value in the loss function, we get the minimum
  reachable loss for the given tree structure.

~~~
- By splitting a leaf, we want to decrease the minimum reachable loss
  $\Rightarrow$ the **minimum node loss is the splitting criterion**.

---
section: Gini and Entropy Losses
class: section
# Gini and Entropy Losses

---
# Binary Gini as (M)SE Loss

Recall that $I_ğ“£$ denotes the set of training example indices belonging to a leaf node $ğ“£$.
Let $n_ğ“£(0)$ be the number of examples with target value 0, $n_ğ“£(1)$ be the
number of examples with target value 1, and let $p_ğ“£ = \frac{1}{|I_ğ“£|} âˆ‘_{i âˆˆ I_ğ“£} t_i = \frac{n_ğ“£(1)}{n_ğ“£(0) + n_ğ“£(1)}$.

~~~
Consider the sum of squares loss $L(p) = âˆ‘_{i âˆˆ I_ğ“£} (p - t_i)^2$.

~~~
By setting the derivative of the loss to zero, we get that the $p$ minimizing
the loss fulfills $|I_ğ“£|p = âˆ‘_{i âˆˆ I_ğ“£} t_i$, i.e., $p = p_ğ“£$.

~~~
The value of the loss is then

$\displaystyle L(p_ğ“£) = âˆ‘_{i âˆˆ I_ğ“£} (p_ğ“£ - t_i)^2$
~~~
$= n_ğ“£(0) \textcolor{blue}{(p_ğ“£ - 0)^2} + n_ğ“£(1) \textcolor{magenta}{(p_ğ“£ - 1)^2}$

~~~
$\displaystyle \phantom{L(p_ğ“£)}
  = \frac{n_ğ“£(0) \textcolor{blue}{n_ğ“£(1)^2}}{\textcolor{blue}{\big(n_ğ“£(0) + n_ğ“£(1)\big)^2}}
    + \frac{n_ğ“£(1) \textcolor{magenta}{n_ğ“£(0)^2}}{\textcolor{magenta}{\big(n_ğ“£(0) + n_ğ“£(1)\big)^2}}$
~~~
$\displaystyle = \mathrlap{
  \frac{(n_ğ“£(1) + n_ğ“£(0)) \textcolor{green}{n_ğ“£(0)} \textcolor{red}{n_ğ“£(1)}}
       {\textcolor{green}{\big(n_ğ“£(0) + n_ğ“£(1)\big)} \textcolor{red}{\big(n_ğ“£(0) + n_ğ“£(1)\big)}}}$

~~~
$\displaystyle \phantom{L(p_ğ“£)} = \big(n_ğ“£(0) + n_ğ“£(1)\big) \textcolor{green}{(1-p_ğ“£)} \textcolor{red}{p_ğ“£} = |I_ğ“£| â‹… p_ğ“£ (1-p_ğ“£).$

---
# Entropy as NLL Loss

Again, let $I_ğ“£$ denote the set of training example indices belonging to a leaf node $ğ“£$,
let $n_ğ“£(k)$ be the number of examples with target value $k$, and let
$p_ğ“£(k) = \frac{1}{|I_ğ“£|} âˆ‘_{i âˆˆ I_ğ“£} [t_i = k] = \frac{n_ğ“£(k)}{|I_ğ“£|}$.

~~~
Consider a distribution $â†’p$ on $K$ classes and a non-averaged NLL loss $L(â†’p) = âˆ‘_{i âˆˆ I_ğ“£} - \log p_{t_i}$.

~~~
By setting the derivative of the loss with respect to $p_k$ to zero (using
a Lagrangian with the constraint $âˆ‘_k p_k = 1$), we get that the $â†’p$ minimizing
the loss fulfills $p_k = p_ğ“£(k)$.

~~~
The value of the loss with respect to $â†’p_ğ“£$ is then

$\displaystyle \kern10em\mathllap{L(â†’p_ğ“£)} = âˆ‘_{i âˆˆ I_ğ“£} - \log p_{t_i}$

~~~
$\displaystyle \kern10em{} = - âˆ‘_{\substack{k\\p_ğ“£(k)â‰ 0}} n_ğ“£(k) \log p_ğ“£(k)$

~~~
$\displaystyle \kern10em{} = - |I_ğ“£| âˆ‘_{\substack{k\\p_ğ“£(k)â‰ 0}}  p_ğ“£(k) \log p_ğ“£(k) = |I_ğ“£| â‹… H(â†’p_ğ“£).$


---
section: Random Forests
class: section
# Random Forests

---
# Random Forests

Bagging of data combined with a random subset of features (sometimes
called _feature bagging_).

![w=80%,h=center](random_forest.svgz)

---
# Random Forests

## Bagging

Every decision tree is trained using bagging (i.e., on a bootstrapped dataset).

~~~
## Random Subset of Features

During each node split, only a random subset of features is considered when
finding the best split. A fresh random subset is used for every node.

~~~
## Extra Trees

The so-called extra trees are even more randomized, not finding the best
possible feature value when choosing a split, but considering uniformly
random samples from a feature's empirical range (minimum and maximum in the
training data).

~~~
## Demo

https://cs.stanford.edu/~karpathy/svmjs/demo/demoforest.html

---
class: summary
# Today's Lecture Objectives

After this lecture you should be able to

- Implement Decision Trees and Random Forests for classification and regression

- Explain how the splitting criterion depends on optimized loss function
