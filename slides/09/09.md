title: NPFL129, Lecture 9
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Naive Bayes, K-Means, Gaussian Mixture

## Milan Straka

### December 16, 2019

---
section: SVR
# SVM For Regression

![w=25%,f=right](svr_loss.pdf)

The idea of SVM for regression is to use an $Îµ$-insensitive error function
$$ğ“›_Îµ\big(t, y(â†’x)\big) = \max\big(0, |y(â†’x) - t| - Îµ\big).$$

~~~
The primary formulation of the loss is then
$$C âˆ‘_i ğ“›_Îµ\big(t, y(â†’x)\big) + \frac{1}{2} ||â†’w||^2.$$

~~~
![w=25%,f=right](svr.pdf)

In the dual formulation, we ideally require every example to be withing $Îµ$ of
its target, but introduce two slack variables $â†’Î¾^-$, $â†’Î¾^+$ to allow outliers. We therefore
minimize the loss
$$C âˆ‘_i (Î¾_i^- + Î¾_i^+) + \tfrac{1}{2} ||â†’w||^2$$
while requiring for every example $t_i - Îµ - Î¾_i^- â‰¤ y(â†’x) â‰¤ t_i + Îµ + Î¾_i^+$ for $Î¾_i^- â‰¥ 0, Î¾_i^+ â‰¥ 0$.

---
# SVM For Regression

The Langrangian after substituting for $â†’w$, $b$, $â†’Î¾^-$ and $â†’Î¾^+$ we get
that we want to minimize
$$L = âˆ‘_i (a_i^+ - a_i^-) t_i - Îµ âˆ‘_i (a_i^+ + a_i^-)
      - \frac{1}{2} âˆ‘_i âˆ‘_j (a_i^+ - a_i^-) (a_j^+ - a_j^-) K(â†’x_i, â†’x_j)$$

![w=40%,f=right](svr_example.pdf)

subject to
$$0 â‰¤ a_i^+, a_i^- â‰¤ C.$$

~~~
The prediction is then given by
$$y(â†’z) = âˆ‘_i (a_i^+ - a_j^-) K(â†’z, â†’x_i) + b.$$

---
section: TF-IDF
# Term Frequency â€“ Inverse Document Frequency

To represent a document, we might consider it a **bag of words**, and create
a feature space with a dimension of every word. We might represent the words as:

- **binary indicators**: 1/0 depending on whether a word is present in
  a document or not;
~~~
- **term frequency TF**: relative frequency of the term in the document;
  $$\mathit{TF}(t) = \frac{\textrm{number of occurrences of $t$ in the document}}{\textrm{number of terms in the document}}$$
~~~
- **inverse document frequency IDF**: we might represent the term using its
  self-information, where terms with lower probability have higher weights;
  $$\mathit{IDF}(t) = \log \frac{\textrm{number of documents}}{\textrm{number of documents containing term $t$}\big[\textrm{optionally} + 1]}$$
~~~
- **TF-IDF**: product of $\mathit{TF}(t)$ and $\mathit{IDF}(t)$.

---
section: NaiveBayes
# Naive Bayes Classifier

Consider a discriminative classifier modelling probabilities
$$p(C_k|â†’x) = p(C_k | x_1, x_2, â€¦, x_D).$$

~~~
We might use Bayes theorem and rewrite it to
$$p(C_k|â†’x) = \frac{p(C_k) p(â†’x | C_k)}{p(â†’x)}.$$

~~~
The so-called **Naive Bayes** classifier assumes all $x_i$
are independent given $C_k$, so we can write
$$p(â†’x | C_k) = p(x_1 | C_k) p(x_2 | C_k, x_1) p(x_3 | C_k, x_1, x_2) â‹¯ p(x_D | C_k, x_1, â€¦)$$
as
$$p(C_k | â†’x) âˆ p(C_k) âˆ_i p(x_i | C_k).$$

---
# Naive Bayes Classifier

There are several used naive Bayes classifiers, depending on the distribution
$p(x_i | C_k)$:
~~~
- **Gaussian NB**: the probability $p(x_i | C_k)$ is modelled as a normal
  distribution $ğ“(Î¼_{i, k}, Ïƒ_{i, k}^2)$;
~~~
- **Multinomial NB**: the probability $p(x_i | C_k)$ is proportional to
  $p_{i, k}^{x_i}$, so the
  $$\log p(C_k, â†’x) = \log p(C_k) + âˆ‘_i\log p_{i, k}^{x_i} = \log p(C_k) + âˆ‘_i x_i \log p_{i, k} = b + â†’x^T â†’w$$
  is a linear model in the log space with $b = \log p(C_k)$ and $w_i = \log p_{i, k}$.
~~~
  Denoting $n_{i, k}$ as the sum of features $x_i$ for a class $C_k$, the
  probabilities $p_{i, k}$ are usually estimated as
  $$p_{i, k} = \frac{n_{i, k} + Î±}{âˆ‘_j n_{j, k} + Î±D}$$
  where $Î±$ is a _smoothing_ parameter accounting for terms not appearing in any
  document of class $C_k$.

---
# Naive Bayes Classifier

- **Bernoulli NB**: when the input features are binary, the $p(x_i | C_k)$ might
  also be a Bernoulli distribution
  $$p(x_i | C_k) = p_{i, k}^{x_i} â‹… (1 - p_{i, k})^{(1-x_i)}.$$
~~~
  Similarly to multinomial NB, the probabilities are usually estimated as
  $$p_{i, k} = \frac{\textrm{number of documents of class $k$ with nonzero feature $i$} + Î±}{\textrm{number of documents of class $k$} + 2Î±}.$$
~~~
  The difference with respect to Multinomial NB is that Bernoulli NB explicitly
  models also an _absence of terms_.

~~~
Given that a Multinomial/Bernoulli NB fits $p(C_k, â†’x)$ as a linear model and
a logistic regression fits $p(C_k | â†’x)$ as a log-linear model, naive Bayes and 
logistic regression form a so-called _generative-discriminative_ pair, where
the naive Bayes is a _generative_ model, while logistic regression is
a _discriminative_ model.

---
section: MultivariateGaussian
# Multivariate Gaussian Distribution

Recall that
$$ğ“(x; Î¼, Ïƒ^2) = \sqrt{\frac{1}{2Ï€Ïƒ^2}} \exp \left(-\frac{(x - Î¼)^2}{2Ïƒ^2}\right).$$

~~~
For $D$-dimensional vector $â†’x$, the multivariate Gaussian distribution takes
the form
$$ğ“(â†’x | â†’Î¼, â‡‰Î£) â‰ \frac{1}{\sqrt{(2Ï€)^D |Î£|}} \exp \left(-\frac{1}{2}(â†’x-â†’Î¼)^T â‡‰Î£^{-1} (â†’x-â†’Î¼) \right).$$

---
# Multivariate Gaussian Distribution

The $â‡‰Î£$ is a _covariance_ matrix, and it is symmetrical. If we represent it
using its _eigenvectors_ $â†’u_i$ and _eigenvalues $Î»_i$, we get
$$â‡‰Î£^{-1} = âˆ‘_i \frac{1}{Î»_i} â†’u_i â†’u_i^T,$$
~~~
from which we can see that the constant surfaces of the multivariate Gaussian
distribution are ellipsoids centered at $â†’Î¼$, with axes oriented at $â†’u_i$
with scaling factors $Î»_i ^{1/2}$.

![w=30%](multivariate_gaussian_elipsoids.pdf)![w=60%](multivariate_gaussian_covariance.pdf)

---
section: Clustering
# Clustering

Clustering is an unsupervised machine learning technique, which given input
data tries to divide them into some number of groups, or _clusters_.

~~~
The number of clusters might be given in advance, or should also be inferred.

~~~
When clustering documents, we usually use TF-IDF normalized so that each
feature vector has length 1 (i.e., L2 normalization).

---
section: KMeans
# K-Means Clustering

Let $â†’x_1, â†’x_2, â€¦, â†’x_N$ be a collection of $N$ input examples, each being
a $D$-dimensional vector $â†’x_i âˆˆ â„^D$. Let $K$, the number of target clusters,
be given.

~~~
Let each cluster be specified by a point $â†’Î¼_1, â€¦, â†’Î¼_K$.
~~~
Further, let $z_{i, k} âˆˆ \{0, 1\}$ be a binary indicator variables describing whether input
example $â†’x_i$ is assigned to cluster $k$, and let each cluster be specified by
a point $â†’Î¼_1, â€¦, â†’Î¼_K$, usually called the cluster _center_.

~~~
Our objective function $J$ which we aim to minimize is
$$J = âˆ‘_{i=1}^N âˆ‘_{k=1}^K z_{i, k} ||â†’x_i - â†’Î¼_k||^2.$$

---
# K-Means Clustering

To find out the cluster centers $â†’Î¼_i$ and input example assignments $z_{i, k}$,
we use the following iterative algorithm (which could be considered a coordinate
descent):
~~~
1. compute the best possible $z_{i, k}$. It is easy to see that the smallest $J$
   is achieved by
   $$z_{i,k} = \begin{cases} 1 & \textrm{~~if~}k = \argmin_j ||â†’x_i - â†’Î¼_j||^2 \\
                             0 & \textrm{~~otherwise}.\end{cases}$$

2. compute best possible $â†’Î¼_k = \argmin\nolimits_â†’Î¼ âˆ‘_i z_{i,k} ||â†’x_i-â†’Î¼||^2$.
~~~
   By computing a derivative with respect to $â†’Î¼$, we get
   $$â†’Î¼_k = \frac{âˆ‘_i z_{i,k} â†’x_i}{âˆ‘_i z_{i,k}}.$$

---
# K-Means Clustering

![w=55%,h=center](kmeans_example.pdf)

---
# K-Means Clustering

![w=60%,f=right](kmeans_convergence.pdf)

It is easy to see that:
- updating the cluster assignment $z_{i, k}$ decreases the loss $J$ or keeps it the same;
~~~
- updating the cluster centers again decreases the loss $J$ or keeps it the
  same.

~~~
K-Means clustering therefore converges to a local optimum. However, it
is quite sensitive to the starting initialization:
~~~
- It is common practise to run K-Means algorithm multiple times with different
  initialization and use the result with lowest $J$ (scikit-learn uses
  `n_init=10` by default).
~~~
- There exist better initialization schemes, a frequently used one is
  `k-means++`, where the first cluster center is chosen randomly and others are
  chosen proportionally to the square of their distance to the nearest cluster
  center.

---
# K-Means Clustering

![w=75%,h=center](kmeans_color_reduction.pdf)

---
section: GaussianMixture
# Gaussian Mixture

Let $â†’x_1, â†’x_2, â€¦, â†’x_N$ be a collection of $N$ input examples, each being
a $D$-dimensional vector $â†’x_i âˆˆ â„^D$. Let $K$, the number of target clusters,
be given.

~~~
Our goal is to represent the data as a Gaussian mixture, which is a combination
of $K$ Gaussian in the form
$$p(â†’x) = âˆ‘_{k=1}^K Ï€_k ğ“(â†’x | â†’Î¼_k, â‡‰Î£_k).$$
Therefore, each cluster is parametrized as $ğ“(â†’x | â†’Î¼_k, â‡‰Î£_k)$.

~~~
Let $â†’z âˆˆ \{0, 1\}^K$ be a $K$-dimensional random variable, such that exactly
one $z_k$ is 1, denoting to which cluster a training example belongs.
~~~
Let the marginal distribution of $z_k$ be
$$p(z_k = 1) = Ï€_k.$$

~~~
Therefore, $p(â†’z) = âˆ_k Ï€_k^{z_k}$.

---
# Gaussian Mixture

We can write
$$p(â†’x) = âˆ‘_{â†’z} p(â†’z) p(â†’x | â†’z) = âˆ‘_{k=1}^K Ï€_k ğ“(â†’x | â†’Î¼_k, â‡‰Î£_k)$$
~~~
and the probability of the whole clustering is therefore
$$\log p(â‡‰X | â†’Ï€, â†’Î¼, â†’Î£) = âˆ‘_{i=1}^N \log \left(âˆ‘_{k=1}^K Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)\right).$$

~~~
To fit a Gaussian mixture model, we start with maximum likelihood estimation and
minimize
$$ğ“›(â‡‰X) = - âˆ‘_i \log âˆ‘_{k=1}^K Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)$$

---
# Gaussian Mixture

The derivative of the loss with respect to $â†’Î¼_k$ gives
$$\frac{âˆ‚ğ“›(â‡‰X)}{âˆ‚â†’Î¼_k} = - âˆ‘_i \frac{Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i | â†’Î¼_l, â‡‰Î£_l)} â‡‰Î£_k^{-1} \big(â†’x_i - â†’Î¼_k\big)$$

~~~
Denoting $r(z_{i,k}) = \frac{Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i | â†’Î¼_l, â‡‰Î£_l)}$,
setting the derivative equal to zero and multiplying by $â‡‰Î£_k^{-1}$, we get
$$â†’Î¼_k = \frac{âˆ‘_i r(z_{i,k}) â†’x_i}{âˆ‘_i r(z_{i,k})}.$$

~~~
The $r(z_{i,k})$ are usually called **responsibilities** and denote the
probability $p(z_k = 1 | â†’x_i)$. Note that the responsibilities depend on
$â†’Î¼_k$, so the above equation is not an analytical solution for $â†’Î¼_k$, but
can be used as an _iterative_ algorithm for converting to local optimum.

---
# Gaussian Mixture

For $â‡‰Î£_k$, we again compute the derivative of the loss, which is technically
complicated (we need to compute derivative with respect a matrix, and also we
need to differentiate matrix determinant) and results in an analogous equation
$$â†’Î£_k = \frac{âˆ‘_i r(z_{i,k}) (â†’x_i - â†’Î¼_k) (â†’x_i - â†’Î¼_k)^T}{âˆ‘_i r(z_{i,k})}.$$

~~~
To minimize the loss with respect to $â†’Ï€$, we need to include the constraint
$âˆ‘_k Ï€_k = 1$, so we form a Lagrangian $ğ“›(â‡‰X) + Î»\left(âˆ‘\nolimits_k Ï€_k - 1\right)$,
and get
$$0 = âˆ‘_i \frac{ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i | â†’Î¼_l, â‡‰Î£_l)} + Î»,$$
~~~
from which we get $Ï€_k âˆ âˆ‘_i r(z_{i,k})$ and therefore
$$Ï€_k = 1/N â‹… âˆ‘\nolimits_i r(z_{i,k}).$$

---
# Gaussian Mixture

![w=40%,h=center](mog_algorithm1.pdf)
![w=40%,h=center](mog_algorithm2.pdf)

---
# Gaussian Mixture

![w=75%,h=center](mog_data.pdf)
![w=75%,h=center](mog_illustration.pdf)

---
# Gaussian Mixture

![w=75%,h=center](mog_example.pdf)
