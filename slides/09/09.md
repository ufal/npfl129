title: NPFL129, Lecture 9
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Correlation, Model Combination, Decision Trees, Random Forests

## Milan Straka

### November 29, 2021

---
section: Covariance
# Covariance

Given a collection of random variables $â‡x_1, â€¦, â‡x_N$, we know that
$$ð”¼\left[âˆ‘\nolimits_i â‡x_i\right] = âˆ‘_i ð”¼ \big[â‡x_i\big].$$

~~~
But how about $\Var\big(âˆ‘_i â‡x_i\big)$?

~~~
$\displaystyle \kern5em\Var\left(âˆ‘\nolimits_i â‡x_i\right)
 = ð”¼\left[\left(âˆ‘\nolimits_i â‡x_i - âˆ‘\nolimits_i ð”¼[â‡x_i]\right)^2\right]$

~~~
$\displaystyle \phantom{\kern5em\Var\left(âˆ‘\nolimits_i â‡x_i\right)}
 = ð”¼\left[\left(âˆ‘\nolimits_i \big(â‡x_i - ð”¼[â‡x_i]\big)\right)^2\right]$

~~~
$\displaystyle \phantom{\kern5em\Var\left(âˆ‘\nolimits_i â‡x_i\right)}
 = ð”¼\left[âˆ‘\nolimits_i âˆ‘\nolimits_j \big(â‡x_i - ð”¼[â‡x_i]\big) \big(â‡x_j - ð”¼[â‡x_j]\big)\right]$

~~~
$\displaystyle \phantom{\kern5em\Var\left(âˆ‘\nolimits_i â‡x_i\right)}
 = âˆ‘_i âˆ‘_j ð”¼\left[\big(â‡x_i - ð”¼[â‡x_i]\big) \big(â‡x_j - ð”¼[â‡x_j]\big)\right].$

---
# Covariance

We define **covariance** of two random variables $â‡x, â‡y$ as
$$\cov(â‡x, â‡y) = ð”¼\Big[\big(â‡x - ð”¼[â‡x]\big) \big(â‡y - ð”¼[â‡y]\big)\Big].$$

~~~
Then,
$$\Var\left(âˆ‘\nolimits_i â‡x_i\right) = âˆ‘_i âˆ‘_j \cov(â‡x_i, â‡x_j).$$

~~~
Note that $\cov(â‡x, â‡x) = \Var(â‡x)$ and that we can write covariance as
$$\begin{aligned}
  \cov(â‡x, â‡y)
   &= ð”¼\Big[\big(â‡x - ð”¼[â‡x]\big) \big(â‡y - ð”¼[â‡y]\big)\Big] \\
   &= ð”¼\big[â‡x â‡y - â‡x ð”¼[â‡y] - ð”¼[â‡x] â‡y + ð”¼[â‡x] ð”¼[â‡y]\big] \\
   &= ð”¼\big[â‡x â‡y\big] - ð”¼\big[â‡x\big] ð”¼\big[â‡y\big].
\end{aligned}$$

---
section: Correlation
# Correlation

Two random variables $â‡x, â‡y$ are **uncorrelated** if $\cov(â‡x, â‡y) = 0$;
otherwise, they are **correlated**.

~~~
Note that two _independent_ random variables are uncorrelated, because

$\displaystyle \kern10em\mathllap{\cov(â‡x, â‡y)} = ð”¼\Big[\big(â‡x - ð”¼[â‡x]\big) \big(â‡y - ð”¼[â‡y]\big)\Big]$

~~~
$\displaystyle \kern10em{} = âˆ‘_{x,y} P(x, y) \big(x - ð”¼[x]\big) \big(y - ð”¼[y]\big)$

~~~
$\displaystyle \kern10em{} = âˆ‘_{x,y} P(x) \big(x - ð”¼[x]\big) P(y) \big(y - ð”¼[y]\big)$

~~~
$\displaystyle \kern10em{} = âˆ‘_x P(x) \big(x - ð”¼[x]\big) âˆ‘_y P(y) \big(y - ð”¼[y]\big)$

~~~
$\displaystyle \kern10em{} = ð”¼_â‡x \big[â‡x - ð”¼[â‡x]\big] ð”¼_â‡y \big[â‡y - ð”¼[â‡y]\big] = 0.$

~~~
However, dependent random variables can be uncorrelated â€“ random
uniform $â‡x$ on $[-1, 1]$ and $â‡y = |â‡x|$ are not independent ($â‡y$ is
completely determined by $â‡x$), but they are uncorrelated.

---
# Pearson correlation coefficient

There are several ways to measure correlation of random variables $â‡x, â‡y$.

**Pearson correlation coefficient**, denoted as $Ï$ or $r$, is defined as
$$\begin{aligned}
  Ï &â‰ \frac{\cov(â‡x, â‡y)}{\sqrt{\Var(â‡x)} \sqrt{\Var(â‡y)}} \\
  r &â‰ \frac{âˆ‘_i (x_i - xÌ„) (y_i - yÌ„)}{\sqrt{âˆ‘_i (x_i - xÌ„)^2} \sqrt{âˆ‘_i (y_i - yÌ„)^2}},
\end{aligned}$$
where:
~~~
- $Ï$ is used when the full expectation is computed (population Pearson
  correlation coefficient);
~~~
- $r$ is used when estimating the coefficient from data (sample Pearson
  correlation coefficient);
  - $xÌ„$ and $yÌ„$ are sample estimates of mean.

---
class: dbend
# Pearson correlation coefficient

The value of Pearson correlation coefficient is in fact normalized covariance,
because its value is always bounded by $-1 â‰¤ Ï â‰¤ 1$ (and the same holds for $r$).

~~~
The bound can be derived from

$\displaystyle \kern5em\mathllap{0} â‰¤ ð”¼\bigg[\bigg(\frac{(â‡x - ð”¼[â‡x])}{\sqrt{\Var(â‡x)}} - Ï\frac{(â‡y - ð”¼[â‡y])}{\sqrt{\Var(â‡y)}}\bigg)^2\bigg]$

~~~
$\displaystyle \kern5em{} = ð”¼\bigg[\frac{(â‡x - ð”¼[â‡x])^2}{\Var(â‡x)}\bigg]
                            - 2Ïð”¼\bigg[\frac{(â‡x - ð”¼[â‡x])}{\sqrt{\Var(â‡x)}}\frac{(â‡y - ð”¼[â‡y])}{\sqrt{\Var(â‡y)}}\bigg]
                            + Ï^2 ð”¼\bigg[\frac{(â‡y - ð”¼[â‡y])^2}{\Var(â‡y)}\bigg]$

~~~
$\displaystyle \kern5em{} = \frac{\Var(â‡x)}{\Var(â‡x)} - 2Ïâ‹…Ï + Ï^2 \frac{\Var(â‡y)}{\Var(â‡y)} = 1 - Ï^2,$

~~~
which yields $Ï^2 â‰¤ 1$.

~~~
Alternatively, the desired inequality can be obtained by applying the
Cauchy-Schwarz inequality
$\langle u, v\rangle â‰¤ \sqrt{\langle u, u\rangle} \sqrt{\langle v, v\rangle}$
on $\langle x, y\rangle â‰ ð”¼[xy]$.

---
# Pearson correlation coefficient

Pearson correlation coefficient quantifies **linear dependence** of the two
random variables.

![w=84%,h=center](correlation_coefficient.png)

---
# Pearson correlation coefficient

Pearson correlation coefficient quantifies **linear dependence** of the two
random variables.

![w=100%,h=center](correlation_examples.svgz)

---
# Pearson correlation coefficient

The four displayed variables have the same mean 7.5, variance 4.12,
Pearson correlation coefficient 0.816 and regression line $3 + \frac{1}{2}x$.

![w=60%,h=center](ancombes_quartet.svgz)

---
# Nonlinear Correlation â€“ Spearman's $Ï$

To measure also non-linear correlation, two common coefficients are used.

### Spearman's rank correlation coefficient $Ï$
Spearman's $Ï$ is Pearson correlation coefficient measured on **ranks** of the
original data, where a rank of an element is its index in sorted ascending
order.

![w=100%](spearman.svgz)

---
# Nonlinear Correlation â€“ Kendall's $Ï„$

### Kendall rank correlation coefficient $Ï„$
Kendall's $Ï„$ measures the amount of _concordant pairs_ (pairs where $y$
increases/decreases when $x$ does), minus the _discordant pairs_
(where $y$ increases/decreases when $x$ does the opposite):

$$\begin{aligned}
  Ï„ &â‰ \frac{|\{\mathrm{pairs}~i â‰  j: x_j > x_i, y_j > y_i\}| - |\{\mathrm{pairs}~i â‰  j: x_j > x_i, y_j < y_i\}|}{\binom{n}{2}} \\
    &= \frac{âˆ‘_{i < j} \sign(x_j - x_i) \sign(y_j - y_i)}{\binom{n}{2}}.
\end{aligned}$$

~~~
There is no clear consensus whether to use Spearman's $Ï$ or Kendall's $Ï„$,
but I believe Kendall's $Ï„$ is a bit more preferred. First, $\frac{1+Ï„}{2}$ can be
interpreted as a probability of a concordant pair, and Kendall's $Ï„$
converges to a normal distribution faster.

~~~
As defined, the range of Kendall's $Ï„ âˆˆ [-1, 1]$. However, if there are ties,
its range is smaller â€“ therefore, several corrections (not discussed here) exist
to adjust its value in case of ties.

---
# Usage of Correlation in Machine Learning

In the machine learning area, correlation is commonly used to measure
â€œagreementâ€ between:

~~~
- several human annotations;

~~~
- automatic metric and gold annotation;
![w=70%,h=center](correlation_scores.svgz)

~~~
- automatic metric and human evaluation.
![w=24%,h=center](correlation_metrics.svgz)

---
section: Model Combination
# Model Combination aka Ensembling

Ensembling is combining several models with a goal of reaching higher
performance.

~~~
The simplest approach is to train several independent models and then combine
their outputs by averaging or voting.

~~~
The terminology varies, but for classification:
- voting (or hard voting) usually means predicting the class predicted most
  often by the individual models,
~~~
- averaging (or soft voting) denotes averaging the returned model distributions
  and predicting the class with the highest probability.

~~~
The main idea behind ensembling is that if models have uncorrelated
errors, then by averaging model outputs the errors will cancel out.

---
# Model Combination aka Ensembling

If we denote the prediction of a model $y_i$ on a training example $(â†’x, t)$ as
$y_i(â†’x) = t + Îµ_i(â†’x)$, so that $Îµ_i(â†’x)$ is the model error on example $â†’x$,
the mean square error of the model is
$$ð”¼\big[(y_i(â†’x) - t)^2\big] = ð”¼\big[Îµ_i^2(â†’x)\big].$$

~~~
Considering $M$ models, we analogously get that the mean square error
of the ensemble is
$$ð”¼\bigg[\Big(\frac{1}{M} âˆ‘\nolimits_i Îµ_i(â†’x)\Big)^2\bigg].$$

~~~
Finally, assuming that the individual errors $Îµ_i$ have zero mean and are _uncorrelated_,
we get that $ð”¼\big[Îµ_i(â†’x) Îµ_j(â†’x)\big] = 0$ for $i â‰  j$, and therefore,
~~~
$$ð”¼\Big[\Big(\frac{1}{M} âˆ‘\nolimits_i Îµ_i(â†’x)\Big)^2\Big]
= ð”¼\Big[\frac{1}{M^2} âˆ‘\nolimits_{i,j} Îµ_i(â†’x) Îµ_j(â†’x)\Big]
= \frac{1}{M} ð”¼\Big[\frac{1}{M} âˆ‘\nolimits_i Îµ_i^2(â†’x)\Big],$$
~~~
so the average error of the ensemble is $\frac{1}{M}$ times the average error
of the individual models.

---
# Bagging â€“ Bootstrap Aggregation

For neural network models, training models with independent initialization is
usually enough, given that the loss has many local minima, so the models tend to
be quite independent just when using different initialization.

~~~
However, algorithms with a convex loss functions usually converge to the same
optimum independent of randomization.

~~~
In these cases, we can use **bagging**, which stands for **bootstrap
aggregation**.

~~~
![w=50%,f=right](bagging.svgz)

In bagging, we construct a different dataset for every model to be trained.
We construct it using **bootstrapping** â€“ we sample as many training instances
as the original dataset has, but **with replacement**.

Such dataset is sampled using the same empirical data distribution and has the
same size, but is not identical.

---
section: DecisionTree
# Decision Trees

The idea of decision trees is to partition the input space into usually cuboid
regions and solving each region with a simpler model.

~~~
We focus on **Classification and Regression Trees** (CART; Breiman et al.,
1984), but there are additional variants like ID3, C4.5, â€¦

~~~
![w=80%,mw=49%,h=center](tree_partitioning.svgz)
~~~
![w=90%,mw=49%,h=center](tree_representation.svgz)

---
# Regression Decision Trees

Assume we have an input dataset $â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$. At the beginning,
the decision tree is just a single node and all input examples belong to this
node. We denote $I_ð“£$ the set of training example indices belonging to a node
$ð“£$.

~~~
For each leaf, our model predict the average of the training examples belonging
to that leaf, $tÌ‚_ð“£ = \frac{1}{|I_ð“£|} âˆ‘_{i âˆˆ I_ð“£} t_i$.

~~~
We use a **criterion** $c_ð“£$ telling us how _uniform_ or _homogeneous_ are the
training examples belonging to a node $ð“£$
~~~
â€“ for regression, we employ the sum of squares error between the examples
belonging to the node and the predicted value in that node; this is proportional
to the variance of the training examples belonging to the node $ð“£$, multiplied
by the number of the examples. Note that even if it is not _mean_ squared error,
it is sometimes denoted as MSE.
$$c_\textrm{SE}(ð“£) â‰ âˆ‘_{i âˆˆ I_ð“£} (t_i - tÌ‚_ð“£)^2\textrm{, ~where~ } tÌ‚_ð“£=\frac{1}{|I_ð“£|} âˆ‘_{i âˆˆ I_ð“£} t_i.$$

---
# Tree Construction

To split a node, the goal is to find a feature and its value such that when
splitting a node $ð“£$ into $ð“£_L$ and $ð“£_R$, the resulting regions decrease the
overall criterion value the most, i.e., the difference $c_{ð“£_L} + c_{ð“£_R} - c_ð“£$
is the lowest.

~~~
Usually we have several constraints, we mention on the most common ones:
- **maximum tree depth**: we do not split nodes with this depth;
~~~
- **minimum examples to split**: we only split nodes with this many training
  examples;
~~~
- **maximum number of leaf nodes**: we split until we reach the given number of
  leaves.

~~~
The tree is usually built in one of two ways:
- if the number of leaf nodes is unlimited, we usually build the tree in
  a depth-first manner, recursively splitting every leaf until some
  of the above constraint is invalidated;
~~~
- if the maximum number of leaf nodes is given, we usually split such leaf $ð“£$,
  where the criterion difference $c_{ð“£_L} + c_{ð“£_R} - c_ð“£$ is the lowest.

---
class: dbend
# Pruning

To control overfitting, the mentioned constraints can be used.

~~~
Additionally, **pruning** can also be used. After training, we might decide that
some subtrees are not necessary and _prune_ them (replacing them by a leaf).
Pruning can be used both as a regularization or model compression.

~~~
There are many heuristics to prune a decision tree; Scikit-learn implements
**minimal cost-complexity pruning**:
~~~
- we extend the criterion to _cost-complexity criterion_ as

  - for a leaf, $c_Î±(ð“£) = c(ð“£) + Î±$,
~~~
  - for a subtree with a root $t$, $c_Î±(t) = âˆ‘_{ð“£ âˆˆ \mathrm{leaves}} c_Î±(ð“£) =  âˆ‘_{ð“£ âˆˆ \mathrm{leaves}} c(ð“£) + Î±|\mathrm{leaves}|$;
~~~
- generally a criterion in a node $t$ is greater or equal to the sum of
  criteria of its leaves;
~~~
- $Î±_\mathrm{eff}$ for a node $t$ is the value of $Î±$ such that the two
  mentioned cost-complexity quantities ($c_Î±(ð“£)$ computed as if $t$ is a leaf,
  and $c_Î±(t)$) are equal
  - $Î±_\mathrm{eff} = \big(c(ð“£) - c(t)\big) / \big(|\mathrm{leaves}| - 1\big)$;
~~~
- we then prune the nodes in the order of increasing $Î±_\mathrm{eff}$.

---
# Classification Decision Trees

For multi-class classification, we predict such class most frequent
in the training examples belonging to a leaf $ð“£$.

~~~
To define the criterions, let us denote the average probability for class $k$ in
a region $ð“£$ as $p_{ð“£}(k)$.

~~~
For classification trees, one of the following two criterions is usually used:

- **Gini index**, also called **Gini impurity**, measuring how often a randomly
  chosen element would be incorrectly labeled if it was randomly labeled
  according to $â†’p_ð“£$:
  $$c_\textrm{Gini}(ð“£) â‰ |I_ð“£| âˆ‘_k p_ð“£(k) \big(1 - p_ð“£(k)\big),$$

~~~
- **Entropy Criterion**
  $$c_\textrm{entropy}(ð“£) â‰ |I_ð“£| â‹… H(â†’p_ð“£) = - |I_ð“£| âˆ‘_{\substack{k\\p_ð“£(k) â‰  0}} p_ð“£(k) \log p_ð“£(k).$$

---
section: Gini and Entropy Losses
# Binary Gini as (M)SE Loss

Recall that $I_ð“£$ denotes the set of training example indices belonging to a leaf node $ð“£$,
let $n_ð“£(0)$ be the number of examples with target value 0, $n_ð“£(1)$ be the
number of examples with target value 1, and let $p_ð“£ = \frac{1}{|I_ð“£|} âˆ‘_{i âˆˆ I_ð“£} t_i = \frac{n_ð“£(1)}{n_ð“£(0) + n_ð“£(1)}$.

~~~
Consider sum of squares loss $L(p) = âˆ‘_{i âˆˆ I_ð“£} (p - t_i)^2$.

~~~
By setting the derivative of the loss to zero, we get that the $p$ minimizing
the loss fulfills $|I_ð“£|p = âˆ‘_{i âˆˆ I_ð“£} t_i$, i.e., $p = p_ð“£$.

~~~
The value of the loss is then

$\displaystyle L(p_ð“£) = âˆ‘_{i âˆˆ I_ð“£} (p_ð“£ - t_i)^2$
~~~
$= n_ð“£(0) \textcolor{blue}{(p_ð“£ - 0)^2} + n_ð“£(1) \textcolor{magenta}{(p_ð“£ - 1)^2}$

~~~
$\displaystyle \phantom{L(p_ð“£)}
  = \frac{n_ð“£(0) \textcolor{blue}{n_ð“£(1)^2}}{\textcolor{blue}{\big(n_ð“£(0) + n_ð“£(1)\big)^2}}
    + \frac{n_ð“£(1) \textcolor{magenta}{n_ð“£(0)^2}}{\textcolor{magenta}{\big(n_ð“£(0) + n_ð“£(1)\big)^2}}$
~~~
$\displaystyle = \mathrlap{
  \frac{(n_ð“£(1) + n_ð“£(0)) \textcolor{green}{n_ð“£(0)} \textcolor{red}{n_ð“£(1)}}
       {\textcolor{green}{\big(n_ð“£(0) + n_ð“£(1)\big)} \textcolor{red}{\big(n_ð“£(0) + n_ð“£(1)\big)}}}$

~~~
$\displaystyle \phantom{L(p_ð“£)} = \big(n_ð“£(0) + n_ð“£(1)\big) \textcolor{green}{(1-p_ð“£)} \textcolor{red}{p_ð“£} = |I_ð“£| p_ð“£ (1-p_ð“£).$

---
# Entropy as NLL Loss

Again let $I_ð“£$ denote the set of training example indices belonging to a leaf node $ð“£$,
let $n_ð“£(k)$ be the number of examples with target value $k$, and let
$p_ð“£(k) = \frac{1}{|I_ð“£|} âˆ‘_{i âˆˆ I_ð“£} [t_i = k] = \frac{n_ð“£(k)}{|I_ð“£|}$.

~~~
Consider a distribution $â†’p$ on $K$ classes and non-averaged NLL loss $L(â†’p) = âˆ‘_{i âˆˆ I_ð“£} - \log p_{t_i}$.

~~~
By setting the derivative of the loss with respect to $p_k$ to zero (using
a Lagrangian with constraint $âˆ‘_k p_k = 1$), we get that the $â†’p$ minimizing the
loss fulfills $p_k = p_ð“£(k)$.

~~~
The value of the loss with respect to $â†’p_ð“£$ is then

$\displaystyle \kern10em\mathllap{L(â†’p_ð“£)} = âˆ‘_{i âˆˆ I_ð“£} - \log p_{t_i}$

~~~
$\displaystyle \kern10em{} = - âˆ‘_{\substack{k\\p_ð“£(k)â‰ 0}} n_ð“£(k) \log p_ð“£(k)$

~~~
$\displaystyle \kern10em{} = - |I_ð“£| âˆ‘_{\substack{k\\p_ð“£(k)â‰ 0}}  p_ð“£(k) \log p_ð“£(k) = |I_ð“£| â‹… H(â†’p_ð“£).$

---
section: RandomForests
# Random Forests

Bagging of data combined with random subset of features (sometimes
called _feature bagging_).

![w=80%,h=center](random_forest.svgz)

---
# Random Forests

## Bagging

Every decision tree is trained using bagging (on a bootstrapped dataset).

~~~
## Random Subset of Features

During each node split, only a random subset of features is considered, when
finding the best split. A fresh random subset is used for every node.

~~~
## Extra Trees

The so-called extra trees are even more randomized, not finding the best
possible feature value when choosing a split, but considering uniformly
random samples from a feature's empirical range (minimum and maximum in the
training data).


---
# Demos

https://cs.stanford.edu/~karpathy/svmjs/demo/demoforest.html
