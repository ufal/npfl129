title: NPFL129, Lecture 1
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Introduction to Machine Learning

## Milan Straka

### October 03, 2022

---
# Machine Learning

![w=69%](charles_translator.svgz)
~~~
![w=29%](dialogue_system.svgz)

~~~
<div style="width: 40%; float: left">
![w=100%,v=middle](object_detection.svgz)
![w=100%,v=middle](image_segmentation.svgz)
![w=100%,v=middle](human_pose_estimation.svgz)
</div>
~~~
![w=80%,mw=25%,h=center](chronicle.png)
~~~
![w=34%](omr.png)

---
# Machine Learning

![w=80%,h=center](lrw_showcase.svgz)

~~~
![w=35%](a0_results.svgz)
~~~
![w=32%](alphastar.png)
~~~
![w=31%](self_driving_car.jpg)

---
# Machine Learning

![w=79%,h=center](mad_scientist.jpg)

---
section: Organization

# Organization

**Course Website:** https://ufal.mff.cuni.cz/courses/npfl129
~~~
  - Slides, recordings, assignments, exam questions
~~~

**Course Repository:** https://github.com/ufal/npfl129
- Templates for the assignments, slide sources.

~~~

## Piazza

- Piazza will be used as a communication platform.

  You can post questions or notes,
  - privately to the instructors, or
~~~
  - to everyone (signed or anonymously).
~~~

  Students can answer other student's questions too, which allows you to get
  faster response. However, please do not send even parts of your solutions to
  other students.

~~~
- Please use Piazza for **all communication** with the instructors.
~~~
- You will get the invite link after the first lecture.

---
# ReCodEx

https://recodex.mff.cuni.cz

- The assignments will be evaluated automatically in ReCodEx.
~~~
- If you have a MFF SIS account, you should be able to create an account
  using your CAS credentials and should automatically see the right group.
~~~
- Otherwise, there will be **instructions** on **Piazza** how to get
  ReCodEx account (generally you will need to send me a message with several
  pieces of information and I will send it to ReCodEx administrators in
  batches).

---
# Course Requirements

## Practicals
~~~

- There will be 1-3 assignments a week, each with a 2-week deadline.
~~~
  - There is also another week-long second deadline, but for less points.
~~~
- After solving the assignment, you get non-bonus points, and sometimes also
  bonus points.
~~~
- To pass the practicals, you need to get 80 non-bonus points. There will be
  assignments for at least 120 non-bonus points.
~~~
- If you get more than 80 points (be it bonus or non-bonus), they will be
  transferred to the exam (but at most 40 points are transferred).

~~~
## Lecture

You need to pass a written exam.
~~~
- All questions are publicly listed on the course website.
~~~
- There are questions for 100 points in every exam, plus at most 40 surplus
  points from the practicals and plus at most 10 surplus points for **community
  work** (improving slides, â€¦).
~~~
- You need 60/75/90 points to pass with grade 3/2/1.

---
section: Machine Learning
# Machine Learning

A possible definition of learning from Mitchell (1997):
>  A computer program is said to learn from experience E with respect to some
>  class of tasks T and performance measure P, if its performance at tasks in
>  T, as measured by P, improves with experience E.

~~~
- Task T
    - _classification_: assigning one of $k$ categories to a given input
    - _regression_: producing a number $xâˆˆâ„$ for a given input
    - _structured prediction_, _denoising_, _density estimation_, â€¦
~~~
- Measure P
    - _accuracy_, _error rate_, _F-score_, â€¦
~~~
- Experience E
    - _supervised_: usually a dataset with desired outcomes (_labels_ or
      _targets_)
    - _unsupervised_: usually data without any annotation (raw text, raw images, â€¦)
    - _reinforcement learning_, _semi-supervised learning_, â€¦

---
# Supervised Machine Learning

![w=60%,h=center](imagenet_recognition.jpg)
---
# Supervised Machine Learning

![w=100%,h=center](life-expectancy-at-birth-total-years.svgz)

---
# Unsupervised Machine Learning â€“ Clustering

![w=70%,h=center](clustering_1965.png)

---
# Unsupervised Machine Learning â€“ Clustering

![w=65%,h=center](clustering_2017.png)

---
class: wide
# Introduction to Machine Learning History

![w=99%,h=center](figure1_ANN_history.jpg)

---
section: TL;DR
# Basic Machine Learning Settings

Assume we have an input of $â†’x âˆˆ â„^D$. The two basic ML tasks are:
1. **regression**: The goal of a regression is to predict real-valued target
   variable $t âˆˆ â„$ for the given input.

~~~
2. **classification**: Assuming we have a fixed set of $K$ labels, the goal
   of a classification is to choose a corresponding label/class for a given
   input.
~~~
   - We can predict the class only.
~~~
   - We can predict the whole distribution of all classes probabilities.

~~~
We usually have a **training set**, which is assumed to consist of examples
of $(â†’x, t)$ generated independently from a **data generating distribution**.

~~~
The goal of _optimization_ is to match the training set as well as possible.

~~~
However, the goal of _machine learning_ is to perform well on _previously
unseen_ data, to achieve lowest **generalization error** or **test error**. We
typically estimate it using a **test set** of examples independent of the
training set, but generated by the same data generating distribution.

---
# Notation

- $a$, $â†’a$, $â‡‰A$, $â‡¶A$: scalar (integer or real), vector, matrix, tensor

~~~
  - all vectors are always **column** vectors
~~~
  - transposition changes a column vector into a row vector, so $â†’a^T$ is a row vector
~~~
  - we denote **scalar product** between vectors $â†’a$ and $â†’b$ as $â†’a^T â†’b$
    - we understand it as matrix multiplication
~~~
  - the $\|â†’a\|_2$ or just $\|a\|$ is the Euclidean (or $L^2$) norm
    - $\|â†’a\|_2 = \sqrt{\sum_i a_i^2}$
~~~

- $â‡a$, $â‡â†’a$, $â‡â‡‰A$: scalar, vector, matrix random variable

~~~
- $\frac{df}{dx}$: derivative of $f$ with respect to $x$

- $\frac{âˆ‚f}{âˆ‚x}$: partial derivative of $f$ with respect to $x$

~~~
- $âˆ‡_{â†’x} f(â†’x)$: gradient of $f$ with respect to $â†’x$, i.e.,
  $\left(\frac{âˆ‚f(â†’x)}{âˆ‚x_1}, \frac{âˆ‚f(â†’x)}{âˆ‚x_2}, \ldots, \frac{âˆ‚f(â†’x)}{âˆ‚x_n}\right)$

---
section: Input Data
# Example Dataset

Assume we have the following data, generated from an underlying curve
by adding a small amount of noise.

![w=57%,h=center](sin_data.svgz)

---
# Input Data

Usually, our machine learning algorithms will be trained using
the **train set** $â‡‰X âˆˆ â„^{NÃ—D}$, which is a collection of $N$ instances,
each represented by $D$ real numbers.

~~~
In supervised learning, we also have a **target** $â†’t$ for every instance,
- a real number for regression, $â†’t âˆˆ â„^N$;
- a class for classification, $â†’t âˆˆ \{0, 1, â€¦, K-1\}^N$.

~~~

The input to machine learning algorithms is frequently preprocessed, i.e., the
algorithms do not always work directly on the input $â‡‰X$, but on some
modification of it. These preprocessed input values are called **features**.

~~~
In literature, the collection of the processed inputs is called a **design
matrix** $â‡‰Î¦âˆˆ â„^{NÃ—M}$. However, we will denote the inputs to algorithms
always as $â‡‰X$, be it the original training data or processed features.

---
section: Linear Regression
# Linear Regression

Given an input value $â†’x âˆˆ â„^D$, one of the simplest models to predict
a target real value is **linear regression**:
$$y(â†’x; â†’w, b) = x_1 w_1 + x_2 w_2 + â€¦ + x_D w_D + b = âˆ‘_{i=1}^D x_i w_i + b = â†’x^T â†’w + b.$$
The $â†’w$ are usually called _weights_ and $b$ is called _bias_.

~~~
Sometimes it is convenient not to deal with the bias separately. Instead,
we might enlarge the input vector $â†’x$ by padding a value 1, and consider only
$â†’x^Tâ†’w$, where the role of a bias is accomplished by the last weight.
Therefore, when we say â€œweightsâ€, we usually mean both weights and biases.

---
# Separate Bias vs. Padding $â‡‰X$ with Ones

Using an explicit bias term in the form of $y(x) = â†’x^T â†’w + b$.
$$
\begin{bmatrix}
x_{11} \quad x_{12} \\
x_{21} \quad x_{22} \\
\vdots \\
x_{n1} \quad x_{n2} \\
\end{bmatrix} \cdot
\begin{bmatrix}
w_1 \\ w_2
\end{bmatrix} + b
=
\begin{bmatrix}
w_1 x_{11} + w_2 x_{12} + b \\
w_1 x_{21} + w_2 x_{22} + b \\
\vdots \\
w_1 x_{n1} + w_2 x_{n2} + b
\end{bmatrix}
$$

~~~
With extra $1$ padding in $â‡‰X$ and an additional $b$ weight representing the
bias.
$$
\begin{bmatrix}
x_{11} & x_{12} & 1 \\
x_{21} & x_{22} & 1 \\
& \vdots & \\
x_{n1} & x_{n2} & 1 \\
\end{bmatrix} \cdot
\begin{bmatrix}
w_1 \\ w_2 \\ b
\end{bmatrix}
=
\begin{bmatrix}
w_1 x_{11} + w_2 x_{12} + b \\
w_1 x_{21} + w_2 x_{22} + b \\
\vdots \\
w_1 x_{n1} + w_2 x_{n2} + b
\end{bmatrix}
$$

---
# Linear Regression

Assume we have a dataset of $N$ input values $â†’x_1, â€¦, â†’x_N$ and targets
$t_1, â€¦, t_N$.

To find the values of weights, we usually minimize an **error function**
between the real target values and their predictions.

~~~
A popular and simple error function is _mean squared error_:
![w=40%,f=right](mse.svgz)

$$\operatorname{MSE}(â†’w) = \frac{1}{N} âˆ‘_{i=1}^N \big(y(â†’x_i; â†’w) - t_i\big)^2.$$

~~~
Often, _sum of squares_
$$\frac{1}{2} âˆ‘_{i=1}^N \big(y(â†’x_i; â†’w) - t_i\big)^2$$
is used instead, because minimizing it is equal to minimizing MSE, but the math
comes out nicer.

---
# Linear Regression

There are several ways how to minimize the error function, but in the case of
linear regression and sum of squares error, there exists an explicit solution.

Our goal is to minimize the following quantity:
$$\tfrac{1}{2}âˆ‘_i^N (â†’x_i^Tâ†’w - t_i)^2.$$

~~~
If we denote $â‡‰X âˆˆ â„^{NÃ—D}$ the matrix of input values with $â†’x_i$ on a row $i$
and $â†’t âˆˆ â„^N$ the vector of target values, we can rewrite the minimized
quantity as
$$\tfrac{1}{2}\|â‡‰Xâ†’w - â†’t\|^2.$$

---
# Minimization â€“ Unconstrained, Single Real Variable

Assume we have a function and we want to find its minimum.

![w=30%,h=center](extremas.svgz)

~~~
We usually use the Fermat's theorem (interior extremum theorem):

Let $f : â„ â†’ â„$ be a function. If it has minimum (or maximum) in $x$
and if it has derivative in $x$, then
$$\frac{âˆ‚f}{âˆ‚x} = 0.$$

---
# Minimization â€“ Unconstrained, Multiple Real Variables

The previous theorem can be generalized to the multivariate case:

Let $f : â„^D â†’ â„$ be a function. If it has minimum (or maximum) in
$â†’x = (x_1, x_2, â€¦, x_D)$ and if it has derivative in $â†’x$, then
for all $i$, $\frac{âˆ‚f}{âˆ‚x_i} = 0$. In other words, $âˆ‡_{â†’x} f(â†’x) = 0$.

~~~
![w=50%](multivariate_minimum.svgz)![w=50%](multivariate_saddle.svgz)

---
# Linear Regression

In order to find a minimum of $\tfrac{1}{2}âˆ‘_i^N (â†’x_i^Tâ†’w - t_i)^2$,
we can inspect values where the derivative of the
error function is zero, with respect to all weights $w_j$.

~~~
$$\frac{âˆ‚}{âˆ‚w_j} \frac{1}{2}âˆ‘_i^N (â†’x_i^Tâ†’w - t_i)^2 = \frac{1}{2} âˆ‘_i^N \left(2(â†’x_i^Tâ†’w - t_i) x_{ij}\right) = âˆ‘_i^N x_{ij}(â†’x_i^Tâ†’w - t_i)$$

~~~
Therefore, we want for all $j$ that $âˆ‘_i^N x_{ij}(â†’x_i^Tâ†’w - t_i) = 0$. We can
write all the equations together using matrix notation as $â‡‰X^T(â‡‰Xâ†’w - â†’t) = 0$
and rewrite to
$$â‡‰X^Tâ‡‰Xâ†’w = â‡‰X^T â†’t.$$

~~~
The matrix $â‡‰X^Tâ‡‰X$ is of size $DÃ—D$. If it is regular, we can compute its
inverse and therefore
$$â†’w = (â‡‰X^Tâ‡‰X)^{-1}â‡‰X^Tâ†’t.$$

---
# Linear Regression
<div class="algorithm">

**Input**: Dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$).<br>
**Output**: Weights $â†’w âˆˆ â„^D$ minimizing MSE of linear regression.

- $â†’w â† (â‡‰X^Tâ‡‰X)^{-1}â‡‰X^Tâ†’t.$
</div>

~~~
The algorithm has complexity $ğ“(ND^2)$, assuming $Nâ‰¥D$.

~~~
When the matrix $â‡‰X^Tâ‡‰X$ is singular, we can solve $â‡‰X^Tâ‡‰Xâ†’w = â‡‰X^T â†’t$ using
SVD, which will be demonstrated on the next lecture.

---
# Linear Regression Example

Assume our input vectors comprise of $â†’x = (x^0, x^1, â€¦, x^M)$, for $M â‰¥ 0$.

![w=60%,h=center](sin_lr.svgz)

---
# Linear Regression Example

To plot the error, the _root mean squared error_
$\operatorname{RMSE}=\sqrt{\operatorname{MSE}}$ is frequently used.

![w=60%,f=right](sin_errors.svgz)
~~~

The displayed error nicely illustrates two main challenges in machine learning:
- _underfitting_
- _overfitting_

---
section: Regularization
# Model Capacity
We can control whether a model underfits or overfits by modifying its **capacity**.
~~~
- representational capacity
- effective capacity

~~~
![w=80%,h=center](generalization_error.svgz)

---
# Linear Regression Overfitting

Note that employing more data usually alleviates overfitting (the relative
capacity of the model is decreased).

![w=100%](sin_overfitting.svgz)

---
# Regularization

**Regularization** in a broad sense is any change in a machine learning
algorithm that is designed to _reduce generalization error_ (but not necessarily
its training error).

~~~
We already saw that **limiting model capacity** can work as regularization.

![w=35%,h=center](classification_overfitting.svgz)

---
# L2 Regularization

One of the oldest regularization techniques tries to prefer â€œsimplerâ€ models
by endorsing models with **smaller weights**.

~~~
Concretely, **$\boldsymbol{L^2}$-regularization** (also called **weight decay**) penalizes
models with large weights by utilizing the following error function:

$$\frac{1}{2} âˆ‘_{i=1}^N \big(y(â†’x_i; â†’w) - t_i\big)^2 + \frac{Î»}{2} \|â†’w\|^2$$

~~~
Note that the $L^2$-regularization usually is not applied on _bias_, only on the
â€œproperâ€ weights. One of the reasons for this is that without penalizing bias,
$L^2$-regularization is invariant to shifts (i.e., adding a constant to all
targets would result in the same solution with only bias increased by that
constant; if bias would be penalized, this would not be true).

~~~
For simplicity, we will not explicitly exclude the bias from the
$L^2$-regularization penalty in the slides (several textbooks also take the
same approach).

---
# L2 Regularization

![w=25%,f=right](l2_smoothness_data.png)

One way how you can look at $L^2$-regularization is that it promotes smaller
changes of the model (the gradient of linear regression with respect to the
inputs are exactly the weights, i.e., $âˆ‡_{â†’x} y(â†’x; â†’w) = â†’w$).

~~~
Considering the data points on the right, we present mean squared errors
and $L^2$ norms of the weights for three linear regression models:

![w=70%,h=center](l2_smoothness.png)
![w=70%,h=center](l2_smoothness_equations.png)

---
# L2 Regularization

The effect of $L^2$-regularization can be seen as limiting the _effective
capacity_ of the model.

![w=66%,mh=70%,v=bottom](sin_regularization.svgz)
~~~
![w=32.5%,mh=70%,v=bottom](sin_regularization_ablation.svgz)

---
# Regularizing Linear Regression

In matrix form, regularized sum of squares error for linear regression amounts
to
$$\tfrac{1}{2} \|â‡‰Xâ†’w - â†’t\|^2 + \tfrac{Î»}{2} \|â†’w\|^2.$$

When repeating the same calculation as in the unregularized case, we arrive at
$$(â‡‰X^Tâ‡‰X + Î»â‡‰I)â†’w = â‡‰X^Tâ†’t,$$
where $â‡‰I$ is an identity matrix.

~~~
<div class="algorithm">

**Input**: Dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$), constant $Î» âˆˆ â„^+$.<br>
**Output**: Weights $â†’w âˆˆ â„^D$ minimizing MSE of regularized linear regression.

- $â†’w â† (â‡‰X^Tâ‡‰X + Î»â‡‰I)^{-1}â‡‰X^Tâ†’t.$
</div>

~~~
Note that the $â‡‰X^Tâ‡‰X + Î»â‡‰I$ matrix is always regular for $Î»>0$, so another effect of
$L^2$-regularization is that the inverse always exists.

---
section: Hyperparameters
# Choosing Hyperparameters

_Hyperparameters_ are not adapted by the learning algorithm itself.

Usually a **validation set** or **development set** is used to
estimate the generalization error, allowing to update hyperparameters accordingly.
If there is not enough data (well, there is **always** not enough data),
more sophisticated approaches can be used.

~~~
So far, we have seen two hyperparameters, $M$ and $Î»$.

~~~
![w=88%,mw=50%,h=center](sin_errors.svgz)![w=87%,mw=50%,h=left](sin_regularization_ablation.svgz)


