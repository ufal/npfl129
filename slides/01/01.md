title: NPFL129, Lecture 1
class: title, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Introduction to Machine Learning

## JindÅ™ich LibovickÃ½ <small>(reusing materials by Milan Straka)</small>

### September 30, 2024

---
section: Organization

# Course Objectives: What you will learn

After this course you shouldâ€¦

- Be able to reason about tasks/problems **suitable for ML**
   - Know when to use classification, regression and clustering
   - Be able to choose from these methods: Linear and Logistic Regression,
     Multilayer Perceptron, Nearest Neighbors, Naive Bayes, Gradient Boosted Decision
     Trees, $k$-means clustering

~~~
- Think about learning as (mostly probabilistic) **optimization on training data**
  - Know how the ML methods learn including theoretical explanation

~~~
- Know how to properly **evaluate** ML
  - Think about generalization (and avoiding overfitting)
  - Be able to choose a suitable evaluation metric
  - Responsibly decide what model is better

~~~
- Be able to **implement ML algorithms** on a conceptual level
- Be able to **use Scikit-learn** to solve ML problems in Python

---
class: middle
# Course Objectives: What you will _not_ learn

- Data Science â€“ How to (ethically, legally, effeciently, etc.) get data and
  how to **clean** data.

- More advanced neural networks â€“ this is covered in
  [NPFL138](https://ufal.mff.cuni.cz/courses/npfl138)

- Details about Large Languge Models â€“ this is covered in
  [NPFL140](https://ufal.mff.cuni.cz/courses/npfl140)


- How to apply ML in your specific field / your business

---
# Organization

**Course Website:** https://ufal.mff.cuni.cz/courses/npfl129
~~~
  - Slides, recordings, assignments, exam questions
~~~

**Course Repository:** https://github.com/ufal/npfl129
- Templates for the assignments, slide sources.

~~~

## Piazza

- Piazza will be used as a communication platform.

  You can post questions or notes,
  - **privately** to the instructors,
~~~
  - **publicly** to everyone (signed or anonymously).
~~~
    - Other students can answer these too, which allows you to get faster
      response.
~~~
    - However, **do not include even parts of your source code** in public
      questions.
~~~

- Please use Piazza for **all communication** with the instructors.
~~~
- You will get the invite link after the first lecture.

---
class: middle
# Piazza Screenshot

![w=60%,h=center](piazza.png)

---
# ReCodEx

https://recodex.mff.cuni.cz

- The assignments will be evaluated automatically in ReCodEx.
~~~
- If you have a MFF SIS account, you should be able to create an account
  using your CAS credentials and should automatically see the right group.
~~~
- Otherwise, there will be **instructions** on **Piazza** how to get
  ReCodEx account (generally you will need to send me a message with several
  pieces of information and I will send it to ReCodEx administrators in
  batches).

---
# Course Requirements

## Practicals
~~~

- There will be about 2-3 assignments a week, each with a 2-week deadline.
~~~
  - There is also another week-long second deadline, but for fewer points.
~~~
- After solving the assignment, you get non-bonus points, and sometimes also
  bonus points.
~~~
- To pass the **practicals, you need to get 70 non-bonus points**. There will be
  assignments for at least 105 non-bonus points.
~~~
- If you get **more than 70 points** (be it bonus or non-bonus), they will be
  *transferred to the exam* (but at most 40 points are transferred).

~~~
## Lecture

You need to pass a written exam.
~~~
- All questions are publicly listed on the course website.
~~~
- There are questions for 100 points in every exam, plus at most 40 surplus
  points from the practicals and plus at most 10 surplus points for **community
  work** (improving slides, â€¦).
~~~
- You need 60/75/90 points to pass with grade 3/2/1.

---
class: middle
# Schedule Irregularities

| Date | What happens |
|-----------------|-----------------------|
| Monday, October 28 | Public holiday = no English lecture |
| Thursday, October 31 | No English practicals |
| Tuesday, November 5 | Sports day = no Czech lecture |
| November 11 and 12 | Someone else will deliver the lecture |
| Tuesday, November 26 | Extra Czech lecture during the practicals slot |
| Thursday, November 28 | Extra English lecture during the practicals slot |

---
class: middle
# Today's Lecture Objectives

After this lecture you should be able to

- Explain to a non-expert what machine learning is

- Explain the difference between classification and regression

- Implement a simple linear-algebra-based algorithm for training linear regression

---
section: What is Machine Learning?
class: section
# What is Machine Learning?

---
# Definition of Machine Learning

A possible definition of learning from Mitchell (1997):
>  A computer program is said to learn from **experience E** with respect to some
>  class of **tasks T** and performance **measure P**, if its performance at tasks in
>  T, as measured by P, improves with experience E.

~~~
- Task T
    - _classification_: assigning one of $k$ categories to a given input
    - _regression_: producing a number $xâˆˆâ„$ for a given input
    - _structured prediction_, _denoising_, _density estimation_, â€¦
~~~
- Measure P
    - _accuracy_, _error rate_, _F-score_, â€¦
~~~
- Experience E
    - _supervised_: usually a dataset with desired outcomes (_labels_ or
      _targets_)
    - _unsupervised_: usually data without any annotation (raw text, raw images, â€¦)
    - _reinforcement learning_, _semi-supervised learning_, â€¦

---
# Intuitive definition: ML vs Standard Programming

## Programming

* We can **formally describe** a problem with clear concepts
* Program = unambiguous set of **instructions** that handles the concepts

*Example - e-shop:* Concepts: goods, store, customer, order, â€¦ <br />
Simple algorithms: place an order, send an order, â€¦

~~~
## Machine Learning

* We have **data** and a **measure** how our problem is solved
* Typically, we do not know how to write code that solves the problem

*Example - machine translation* there is no set of formal instruction how to
translate, but there is a large amount of data

---
# Supervised Machine Learning

![w=60%,h=center](imagenet_recognition.jpg)

---
# Unsupervised Machine Learning

![w=50%,h=center](outliers.png)

---
class: middle
# Brief Machine Learning History

![w=99%,h=center](ml_history.svgz)

---
section: Basic Methodology & Notation
class: section
#  Basic Methodology & Notation

---
# Basic ML Tasks
Assume input of $â†’x âˆˆ â„^D$. The two basic ML tasks are:

1. **Regression**: The goal is to predict a real-valued target variable $t âˆˆ â„$
   for given $â†’x$.
~~~

2. **Classification**: Assuming a fixed set of $K$ labels, the goal
   is to choose a corresponding label/class for given $â†’x$.
~~~
   - We can predict the class only.
~~~
   - We can predict the whole distribution of all classes probabilities.

~~~
We usually have a **training set**:
  * Consists of examples of $(â†’x, t)$
  * Probabilistic interpretation: Generated independently from a **data-generating distribution**

---
class: middle
# Optimization vs. Machine Learning

* **Optimization** = match the training set as well as possible
~~~
* ML's goals is **generalization** = match **previously unseen data** as well as possible

<center>
â†“

<big>â“ğŸ”®â“</big>
</center>

~~~
<center>
We typically estimate it using a <b>test set</b> of examples independent of the
training set.
<center>

</center>
(in probabilistic interpretation generated by the same data-generating distribution)
</center>

---
# Notation

- $a$, $â†’a$, $â‡‰A$, $â‡¶A$: scalar (integer or real), vector, matrix, tensor

~~~
  - all vectors are always **column** vectors
~~~
  - transposition changes a column vector into a row vector, so $â†’a^T$ is a row vector
~~~
  - we denote the **dot (scalar) product** of the vectors $â†’a$ and $â†’b$ using $â†’a^T â†’b$
    - we understand it as matrix multiplication
~~~
  - the $\|â†’a\|_2$ or just $\|â†’a\|$ is the Euclidean (or $L^2$) norm
    - $\|â†’a\|_2 = \sqrt{\sum_i a_i^2}$
~~~

- $â‡a$, $â‡â†’a$, $â‡â‡‰A$: scalar, vector, matrix random variable

~~~

- $ğ”¸$: set; $â„$ is the set of real numbers, $â„‚$ is the set of complex numbers

~~~
- $\frac{df}{dx}$: derivative of $f$ with respect to $x$

- $\frac{âˆ‚f}{âˆ‚x}$: partial derivative of $f$ with respect to $x$

~~~
- $âˆ‡_{â†’x} f(â†’x)$: gradient of $f$ with respect to $â†’x$, i.e.,
  $\left(\frac{âˆ‚f(â†’x)}{âˆ‚x_1}, \frac{âˆ‚f(â†’x)}{âˆ‚x_2}, \ldots, \frac{âˆ‚f(â†’x)}{âˆ‚x_n}\right)$

---
# Example Dataset

Assume we have the following data, generated from an underlying curve
by adding a small amount of noise.

![w=57%,h=center](sin_data.svgz)

---
# Input Data

Usually, ML algorithms are trained using
the **train set** $â‡‰X âˆˆ â„^{NÃ—D}$: a collection of $N$ instances,
each represented by $D$ real numbers.

~~~
In supervised learning, we also have a **target** $â†’t$ for every instance,
- a real number for regression, $â†’t âˆˆ â„^N$;
- a class for classification, $â†’t âˆˆ \{0, 1, â€¦, K-1\}^N$.

~~~

The input to ML learning algorithms is frequently preprocessed, i.e., the
algorithms do not always work directly on the input $â‡‰X$, but on some
modification of it. These are called **features**.

~~~
In some literature, processed inputs are called a **design
matrix** $â‡‰Î¦âˆˆ â„^{NÃ—M}$, we will denote everything as $â‡‰X$.

---
section: Linear Regression
class: section
# Linear Regression

---
class: middle
# Linear Regression

Given an input value $â†’x âˆˆ â„^D$, one of the simplest models to predict
a target real value is **linear regression**:
$$y(â†’x; â†’w, b) = x_1 w_1 + x_2 w_2 + â€¦ + x_D w_D + b = âˆ‘_{i=1}^D x_i w_i + b = â†’x^T â†’w + b.$$
The $â†’w$ are usually called _weights_ and $b$ is called _bias_.

~~~
<hr />

Sometimes it is convenient not to deal with the bias separately. Instead,
we might enlarge the input vector $â†’x$ by padding a value 1, and consider only
$â†’x^Tâ†’w$, the bias is encoded by the last weight.
Therefore, â€œweightsâ€ often contain both weights and biases.

---
# Separate Bias vs. Padding $â‡‰X$ with Ones

Using an explicit bias term in the form of $y(x) = â†’x^T â†’w + b$.
$$
\begin{bmatrix}
x_{11} \quad x_{12} \\
x_{21} \quad x_{22} \\
\vdots \\
x_{n1} \quad x_{n2} \\
\end{bmatrix} \cdot
\begin{bmatrix}
w_1 \\ w_2
\end{bmatrix} + b
=
\begin{bmatrix}
w_1 x_{11} + w_2 x_{12} + b \\
w_1 x_{21} + w_2 x_{22} + b \\
\vdots \\
w_1 x_{n1} + w_2 x_{n2} + b
\end{bmatrix}
$$

~~~
With extra $1$ padding in $â‡‰X$ and an additional $b$ weight representing the
bias.
$$
\begin{bmatrix}
x_{11} & x_{12} & 1 \\
x_{21} & x_{22} & 1 \\
& \vdots & \\
x_{n1} & x_{n2} & 1 \\
\end{bmatrix} \cdot
\begin{bmatrix}
w_1 \\ w_2 \\ b
\end{bmatrix}
=
\begin{bmatrix}
w_1 x_{11} + w_2 x_{12} + b \\
w_1 x_{21} + w_2 x_{22} + b \\
\vdots \\
w_1 x_{n1} + w_2 x_{n2} + b
\end{bmatrix}
$$

---
# Linear Regression

We have a dataset of $N$ input values $â†’x_1, â€¦, â†’x_N$ and targets
$t_1, â€¦, t_N$.

Find weight values = minimize an **error function**
between the real target values and their predictions.

~~~
A popular and simple error function is _mean squared error_:
![w=40%,f=right](mse.svgz)

$$\operatorname{MSE}(â†’w) = \frac{1}{N} âˆ‘_{i=1}^N \big(y(â†’x_i; â†’w) - t_i\big)^2.$$

~~~
Often, _sum of squares_
$$\frac{1}{2} âˆ‘_{i=1}^N \big(y(â†’x_i; â†’w) - t_i\big)^2$$
is used instead. Minimizing it is equal to minimizing MSE, but the math
comes out nicer.

---
# Linear Regression

Several ways how to minimize the error function â€“ linear regression + sum of
squares error have an explicit solution.

Our goal is to minimize:

$$\tfrac{1}{2}âˆ‘_i^N (â†’x_i^Tâ†’w - t_i)^2.$$

~~~
If we denote $â‡‰X âˆˆ â„^{NÃ—D}$ the matrix of input values with $â†’x_i$ on a row $i$
and $â†’t âˆˆ â„^N$ the vector of target values, we can write it as
$$\tfrac{1}{2}\|â‡‰Xâ†’w - â†’t\|^2,$$
~~~
because
$$\|â‡‰Xâ†’w - â†’t\|^2 = âˆ‘_i \big((â‡‰Xâ†’w - â†’t)_i\big)^2 = âˆ‘_i \big((â‡‰Xâ†’w)_i - t_i)\big)^2 = âˆ‘_i (â†’x_i^Tâ†’w - t_i)^2.$$

---
# Minimization â€“ Unconstrained, Single Real Variable

Assume we have a function and we want to find its minimum.

![w=30%,h=center](extremas.svgz)

~~~
We usually use the Fermat's theorem (interior extremum theorem):

> Let $f : â„ â†’ â„$ be a function. If it has a minimum (or a maximum) in $x$
> and if it has a derivative in $x$, then
> $\frac{âˆ‚f}{âˆ‚x} = 0.$

---
# Minimization â€“ Unconstrained, Multiple Real Variables

The previous theorem can be generalized to the multivariate case:

Let $f : â„^D â†’ â„$ be a function. If it has a minimum or a maximum in
$â†’x = (x_1, x_2, â€¦, x_D)$ and if it has a derivative in $â†’x$, then
for all $i$, $\frac{âˆ‚f}{âˆ‚x_i} = 0$. In other words, $âˆ‡_{â†’x} f(â†’x) = â†’0$.

~~~
![w=50%](multivariate_minimum.svgz)![w=50%](multivariate_saddle.svgz)

---
# Linear Regression

In order to find a minimum of $\tfrac{1}{2}âˆ‘_i^N (â†’x_i^Tâ†’w - t_i)^2$,
we can inspect values where the derivative of the
error function is zero, with respect to all weights $w_j$.

~~~
$$\frac{âˆ‚}{âˆ‚w_j} \frac{1}{2}âˆ‘_i^N (â†’x_i^Tâ†’w - t_i)^2 = \frac{1}{2} âˆ‘_i^N \left(2(â†’x_i^Tâ†’w - t_i) x_{ij}\right) = âˆ‘_i^N x_{ij}(â†’x_i^Tâ†’w - t_i)$$

~~~
Therefore, we want for all $j$ that $âˆ‘_i^N x_{ij}(â†’x_i^Tâ†’w - t_i) = 0$.

~~~
We can rewrite the explicit sum into $â‡‰X_{*,j}^T(â‡‰Xâ†’w - â†’t) = 0$,
~~~
then write the equations for all $j$ together using matrix notation as
$â‡‰X^T(â‡‰Xâ†’w - â†’t) = â†’0$,
~~~
and finally, rewrite to
$$â‡‰X^Tâ‡‰Xâ†’w = â‡‰X^T â†’t.$$

~~~
The matrix $â‡‰X^Tâ‡‰X$ is of size $DÃ—D$. If it is invertible, we can compute its
inverse and
$$â†’w = (â‡‰X^Tâ‡‰X)^{-1}â‡‰X^Tâ†’t.$$

---
# Linear Regression
<div class="algorithm">

**Input**: Dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$).<br>
**Output**: Weights $â†’w âˆˆ â„^D$ minimizing MSE of linear regression.

- $â†’w â† (â‡‰X^Tâ‡‰X)^{-1}â‡‰X^Tâ†’t.$
</div>

~~~
The algorithm has complexity $ğ“(ND^2)$, assuming $Nâ‰¥D$.

~~~
When the matrix $â‡‰X^Tâ‡‰X$ is singular, we can solve $â‡‰X^Tâ‡‰Xâ†’w = â‡‰X^T â†’t$ using
SVD.

---
# Linear Regression Example: Polynomial Features

We want to predict a $t âˆˆ â„$ for a given $x âˆˆ â„$. Linear regression with â€œrawâ€ input vectors $â†’x = (x)$ can only model straight lines.

~~~
![w=58%,f=right](sin_lr.svgz)

If we consider input vectors $â†’x = (x^0, x^1, â€¦, x^M)$ for a given
$M â‰¥ 0$, the linear regression is able to model polynomials of degree $M$.
The prediction is then computed as
$$w_0 x^0 + w_1 x^1 + â€¦ + w_M x^M.$$

~~~
The weights are the coefficients of a polynomial of degree $M$.

---
# Linear Regression Example

To plot the error, the _root mean squared error_
$\operatorname{RMSE}=\sqrt{\operatorname{MSE}}$ is frequently used.

![w=60%,f=right](sin_errors.svgz)
~~~

The displayed error nicely illustrates two main challenges in machine learning:
- _underfitting_
- _overfitting_

---
section: Summary
class: summary
# Today's Lecture Objectives

After this lecture you should be able to

- Explain to a non-expert what machine learning is

- Explain the difference between classification and regression

- Implement a simple linear-algebra-based algorithm for training linear regression
