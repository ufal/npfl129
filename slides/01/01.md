title: NPFL129, Lecture 1
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Introduction to Machine Learning

## Milan Straka

### October 07, 2019

---
section: Machine Learning
# Machine Learning

A possible definition of learning from Mitchell (1997):
>  A computer program is said to learn from experience E with respect to some
>  class of tasks T and performance measure P, if its performance at tasks in
>  T, as measured by P, improves with experience E.

~~~
- Task T
    - _classification_: assigning one of $k$ categories to a given input
    - _regression_: producing a number $xâˆˆâ„$ for a given input
    - _structured prediction_, _denoising_, _density estimation_, â€¦
- Experience E
    - _supervised_: usually a dataset with desired outcomes (_labels_ or
      _targets_)
    - _unsupervised_: usually data without any annotation (raw text, raw images, â€¦)
    - _reinforcement learning_, _semi-supervised learning_, â€¦
- Measure P
    - _accuracy_, _error rate_, _F-score_, â€¦

---
# Deep Learning Highlights
- Image recognition

~~~ ~
# Deep Learning Highlights
![w=60%,h=center](imagenet_recognition.jpg)

~~~ ~~
- Object detection
~~~ ~
# Deep Learning Highlights
![w=100%,v=middle](object_detection.pdf)

~~~ ~~
- Image segmentation,
~~~ ~
# Deep Learning Highlights
![w=100%,v=middle](image_segmentation.pdf)

~~~ ~~
- Human pose estimation
~~~ ~
# Deep Learning Highlights
![w=100%,v=middle](human_pose_estimation.pdf)

~~~ ~~
- Image labeling
~~~ ~
# Deep Learning Highlights
![w=75%,h=center](image_labeling.pdf)

~~~ ~~
- Visual question answering
~~~ ~
# Deep Learning Highlights
![w=70%,h=center](vqa.pdf)

~~~ ~~
- Speech recognition and generation
~~~ ~
# Deep Learning Highlights

<audio controls style="width: 32%"><source src="https://google.github.io/tacotron/publications/tacotron2/demos/washington_gen.wav"></audio>
<audio controls style="width: 32%"><source src="https://google.github.io/tacotron/publications/tacotron2/demos/columbia_gen.wav"></audio>
<audio controls style="width: 32%"><source src="https://google.github.io/tacotron/publications/tacotron2/demos/fox_question.wav"></audio>

![w=100%](tacotron_comparison.pdf)

~~~ ~~
- Lip reading
~~~ ~
# Deep Learning Highlights
![w=70%,h=center](lrw_showcase.pdf)
![w=70%,h=center](lipnet_saliency.pdf)

~~~ ~~
- Machine translation
~~~ ~
# Deep Learning Highlights
![w=44%,h=center](attention_visualization.pdf)

~~~ ~~
- Machine translation without parallel data
~~~ ~
# Deep Learning Highlights
![w=70%,h=center](umt_ideas.pdf)
![w=30%,h=center](umt_comparison.pdf)

~~~ ~~
- Chess, Go and Shogi
~~~ ~
# Deep Learning Highlights
![w=95%,h=center](a0_results.pdf)

~~~ ~~
- Multiplayer Capture the flag
~~~ ~
# Deep Learning Highlights
![w=100%,v=middle](ctf_overview.pdf)

~~~ ~~

---
class: wide
# Introduction to Machine Learning History

![w=99%,h=center](figure1_ANN_history.jpg)

---
# Machine and Representation Learning

![w=35%,h=center](machine_learning.pdf)

---
section: TL;DR
# Basic Machine Learning Settings

Assume we have an input of $â†’x âˆˆ â„^d$.

~~~
Then the two basic ML tasks are:
1. **regression**: The goal of a regression is to predict real-valued target
   variable $t âˆˆ â„$ of the given input.

~~~
2. **classification**: Assuming we have a fixed set of $K$ labels, the goal
   of a classification is to choose a corresponding label/class for a given
   input.
~~~
   - We can predict the class only.
~~~
   - We can predict the whole distribution of all classes probabilities.

~~~
We usually have a **training set**, which is assumed to consist of examples
of $(â†’x, t)$ generated independently from a **data generating distribution**.

~~~
The goal of _optimization_ is to match the training set as well as possible.

~~~
However, the goal of _machine learning_ is to perform well on _previously
unseen_ data, to achieve lowest **generalization error** or **test error**. We
typically estimate it using a **test set** of examples independent of the
training set, but generated by the same data generating distribution.

---
# Notation

- $a$, $â†’a$, $â‡‰A$, $â‡¶A$: scalar (integer or real), vector, matrix, tensor

- $â‡a$, $â‡â†’a$, $â‡â‡‰A$: scalar, vector, matrix random variable

~~~
- $\frac{df}{dx}$: derivative of $f$ with respect to $x$

- $\frac{âˆ‚f}{âˆ‚x}$: partial derivative of $f$ with respect to $x$

~~~
- $âˆ‡_â†’x f$: gradient of $f$ with respect to $â†’x$, i.e.,
  $\left(\frac{âˆ‚f(â†’x)}{âˆ‚x_1}, \frac{âˆ‚f(â†’x)}{âˆ‚x_2}, \ldots, \frac{âˆ‚f(â†’x)}{âˆ‚x_n}\right)$

---
section: Linear Regression
# Example Dataset

Assume we have the following data, generated from an underlying curve
by adding a small amount of Gaussian noise.

![w=57%,h=center](sin_data.pdf)

---
# Linear Regression

Given an input value $â†’x âˆˆ â„^d$, one of the simplest models to predict
a target real value is **linear regression**:
$$f(â†’x; â†’w, b) = â†’x^T â†’w + b.$$
The $â†’w$ are usually called _weights_ and $b$ is called _bias_.

~~~
Sometimes it is convenient not to deal with the bias separately. Instead,
we might enlarge the input vector $â†’x$ by padding a value 1, and consider only
$â†’x^Tâ†’w$, where the role of a bias is accomplished by the last weight.
Therefore, when we say â€œweightsâ€, we usually mean both weights and biases.

---
# Linear Regression

Assume we have a dataset of $N$ input values $â†’x_1, â€¦, â†’x_N$ and targets
$t_1, â€¦, t_N$.

To find the values of weights, we usually minimize an **error function**
between the real target values and their predictions.

~~~
A popular and simple error function is _mean squared error_:
![w=40%,f=right](mse.pdf)

$$\operatorname{MSE}(â†’w) = \frac{1}{N} âˆ‘_{i=1}^N \big(f(â†’x_i; â†’w) - t_i\big)^2.$$

~~~
Often, _sum of squares_
$$\frac{1}{2} âˆ‘_{i=1}^N \big(f(â†’x_i; â†’w) - t_i\big)^2$$
is used instead, because the math comes out nicer.

---
# Linear Regression

There are several ways how to minimize the error function, but in the case of
linear regression and mean squared error, there exists an explicit solution.

Assuming $â‡‰X âˆˆ â„^{NÃ—D}$ is a matrix of input values with $â†’x_i$ on a row $i$,
and $â†’t âˆˆ â„^N$ is a vector of target values, we are aiming to minimize
$$\tfrac{1}{2} ||â‡‰Xâ†’w - â†’t||^2 = \tfrac{1}{2} (â‡‰Xâ†’w - â†’t)^T (â‡‰Xâ†’w - â†’t).$$

~~~
In order to find a minimum, we can inspect values where the derivative of the
error function is zero.

$$\begin{aligned}
\frac{âˆ‚}{âˆ‚â†’w} \tfrac{1}{2} (â‡‰Xâ†’w - â†’t)^T (â‡‰Xâ†’w - â†’t) &= \frac{âˆ‚}{âˆ‚â†’w} \tfrac{1}{2} (â†’w^Tâ‡‰X^T - â†’t^T) (â‡‰Xâ†’w - â†’t) \\
  &= \tfrac{1}{2}â‡‰X^T(â‡‰Xâ†’w - â†’t) + \tfrac{1}{2}â‡‰X^T(â‡‰Xâ†’w - â†’t) \\
  &= â‡‰X^Tâ‡‰Xâ†’w - â‡‰X^Tâ†’t.
\end{aligned}$$

For the derivative is therefore zero when $â‡‰X^Tâ‡‰Xâ†’w = â‡‰X^Tâ†’t$, so when
$â†’w = (â‡‰X^Tâ‡‰X)^{-1}â‡‰X^Tâ†’t.$

---
# Linear Regression
<div class="algorithm">

**Input**: Dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$).<br>
**Output**: Weights $â†’w âˆˆ â„^D$ minimizing MSE of linear regression.

- $â†’w â† (â‡‰X^Tâ‡‰X)^{-1}â‡‰X^Tâ†’t.$
</div>

~~~
The algorithm has complexity $ğ“(ND^2)$, assuming $Nâ‰¥D$.

~~~
Note that there exist better solutions which do not require the $(â‡‰X^Tâ‡‰X)^{-1}$
to exist (i.e., they solve directly the $â‡‰X^Tâ‡‰Xâ†’w = â‡‰X^Tâ†’t$ equation), but they
have the same complexity.

---
# Linear Regression Example

Assume our input vectors comprise of $â†’x = (x^0, x^1, â€¦, x^M)$, for $M â‰¥ 0$.

![w=60%,h=center](sin_lr.pdf)

---
# Linear Regression Example

To plot the error, the _root mean squared error_
$\operatorname{RMSE}=\sqrt{\operatorname{MSE}}$ is frequently used.

![w=60%,f=right](sin_errors.pdf)
~~~

The displayed error nicely illustrates two main challenges in machine learning:
- _underfitting_
- _overfitting_

---
section: Regularization
# Model Capacity
We can control whether a model underfits or overfits by modifying its _capacity_.
~~~
- representational capacity
- effective capacity

~~~
![w=80%,h=center](generalization_error.pdf)

---
# Linear Regression Overfitting

Note that employing more data also usually alleviates overfitting (the relative
capacity of the model is decreased).

![w=100%](sin_overfitting.pdf)

---
# Regularization

**Regularization** in a broad sense is any change in a machine learning
algorithm that is designed to _reduce generalization error_  but not necessarily
its training error).

~~~
$L_2$ regularization (also called weighted decay) penalizes models
with large weights:

$$\frac{1}{2} âˆ‘_{i=1}^N \big(f(â†’x_i; â†’w) - t_i\big)^2 + \frac{Î»}{2} ||â†’w||^2$$

![w=60%,h=center](sin_regularization.pdf)

---
# Regularizing Linear Regression

In matrix form, regularized sum of squares error for linear regression amounts
to
$$\tfrac{1}{2} ||â‡‰Xâ†’w - â†’t||^2 + \tfrac{Î»}{2} ||â†’w||^2  = \tfrac{1}{2} (â‡‰Xâ†’w - â†’t)^T (â‡‰Xâ†’w - â†’t) + \tfrac{Î»}{2} â†’w^tâ†’w.$$

When repeating the same calculation as in the unregularized case, we arrive at
$$(â‡‰X^Tâ‡‰X + Î»â‡‰I)â†’w = â‡‰X^Tâ†’t,$$
where $â‡‰I$ is an identity matrix.

~~~
<div class="algorithm">

**Input**: Dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$), constant $Î» âˆˆ â„^+$.<br>
**Output**: Weights $â†’w âˆˆ â„^D$ minimizing MSE of regularized linear regression.

- $â†’w â† (â‡‰X^Tâ‡‰X + Î»â‡‰I)^{-1}â‡‰X^Tâ†’t.$
</div>

---
# Choosing Hyperparameters

_Hyperparameters_ are not adapted by the learning algorithm itself.

Usually a **validation set** or **development set** is used to
estimate the generalization error, allowing to update hyperparameters accordingly.
If there is not enough data (well, there is **always** not enough data),
more sophisticated approaches can be used.

~~~
So far, we have seen two hyperparameters, $M$ and $Î»$.

~~~
![w=45%,h=center](sin_regularization_ablation.pdf)
