title: NPFL129, Lecture 5
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Derivation of Softmax,<br> Support Vector Machines

## Milan Straka

### November 18, 2019

---
section: Refresh
# Lagrange Multipliers â€“ Equality Constraints

![w=28%,f=right](../04/lagrange_equalities.pdf)

Given a funtion $J(â†’x)$, we can find a maximum with respect to a vector
$â†’x âˆˆ â„^d$, by investigating the critical points $âˆ‡_â†’x J(â†’x) = 0$.

Consider now finding maximum subject to a a constraing $g(â†’x) = 0$.

- Note that $âˆ‡_â†’x g(â†’x)$ is orthogonal to the surface of the constraing, because
  if $â†’x$ and a nearby point $â†’x+â†’Îµ$ lie on the surface, from the Taylor
  expansion $g(â†’x+â†’Îµ) â‰ˆ g(â†’x) + â†’Îµ^T âˆ‡_â†’x g(â†’x)$ we get $â†’Îµ^T âˆ‡_â†’x g(â†’x) â‰ˆ 0$.

- In the seeked maximum, $âˆ‡_â†’x f(â†’x)$ must also be orthogonal to the constraing
  surface (or else moving in the direction of the derivative would increase the
  value).

- Therefore, there must exist $Î»$ such that $âˆ‡_â†’x f + Î»âˆ‡_â†’x g = 0$.

---
# Lagrange Multipliers â€“ Equality Constraints

![w=28%,f=right](../04/lagrange_equalities.pdf)

We therefore introduce the _Lagrangian function_
$$L(â†’x, Î») â‰ f(â†’x) + Î»g(â†’x).$$

We can then find the maximum under the constraing by inspecting critical points
of $L(â†’x, Î»)$ with respect to both $â†’x$ and $Î»$:
- $\frac{âˆ‚L}{âˆ‚Î»} = 0$ leads to $g(â†’x)=0$;
- $\frac{âˆ‚L}{âˆ‚â†’x} = 0$ is the previously derived $âˆ‡_â†’x f + Î»âˆ‡_â†’x g = 0$.

---
class: dbend
# Calculus of Variations

Many optimization techniques depend on minimizing a function $J(â†’w)$ with
respect to a vector $â†’w âˆˆ â„^d$, by investigating the critical points $âˆ‡_â†’w J(â†’w) = 0$.

A function of a function, $J[f]$, is known as a **functional**, for example
entropy $H[â‹…]$.

Similarly to partial derivatives, we can take **functional derivatives** of
a functional $J[f]$ with respect to individual values $f(â†’x)$ for all points
$â†’x$. The functional derivative of $J$ with respect to a function $f$ in a point
$â†’x$ is denoted as
$$\frac{âˆ‚}{âˆ‚f(â†’x)} J.$$

For this class, we will use only the following theorem, which states that for
all differentiable functions $f$ and differentiable functions $g(f(â†’x), â†’x)$ with
continuous derivatives, it holds that
$$\frac{âˆ‚}{âˆ‚f(â†’x)} âˆ«g(f(â†’x), â†’x) \dâ†’x = \frac{âˆ‚}{âˆ‚y} g(y, â†’x).$$

---
class: dbend
# Calculus of Variations

An intuitive view is to think about $f(â†’x)$ as a vector of uncountably many
elements (for every value $â†’x)$. In this interpretation the result is analogous
to computing partial derivatives of a vector $â†’w âˆˆ â„^d$:
$$\frac{âˆ‚}{âˆ‚w_i} âˆ‘_j g(w_j, â†’x) = \frac{âˆ‚}{âˆ‚w_i} g(w_i, â†’x).$$
$$\frac{âˆ‚}{âˆ‚f(â†’x)} âˆ«g(f(â†’x), â†’x) \dâ†’x = \frac{âˆ‚}{âˆ‚y} g(y, â†’x).$$

---
class: dbend
# Continuous Distribution with Maximum Entropy

What distribution over $â„$ maximizes entropy $H[p] = -ğ”¼_x \log p(x)$?

For continuous values, the entropy is an integral $H[p] = -âˆ«p(x) \log p(x) \d x$.

We cannot just maximize $H$ with respect to a function $p$, because:
- the result might not be a probability distribution â€“ we need to add
  a constraint that $âˆ«p(x) \d x=1$;
~~~
- the problem is unspecified because a distribution can be shifted without
  changing entropy â€“ we add a constraing $ğ”¼[x] = Î¼$;
~~~
- because entropy increases as variance increases, we ask which distribution
  with a _fixed_ variance $Ïƒ^2$ has maximum entropy â€“ adding a constraing
  $\Var(x) = Ïƒ^2$.

---
class: dbend
# Function with Maximum Entropy

Lagrangian of all the constraings and the entropy function is
$$L(p; Î¼, Ïƒ^2) = Î»_1 \Big(âˆ«p(x) \d x - 1\Big) + Î»_2 \big(ğ”¼[x] - Î¼\big) + Î»_3\big(\Var(x) - Ïƒ^2\big) + H[p].$$

~~~
By expanding all definitions to integrals, we get
$$\begin{aligned}
L(p; Î¼, Ïƒ^2) =& âˆ«\Big(Î»_1 p(x) + Î»_2 p(x) x Î»_3 p(x) (x - Î¼)^2 - p(x)\log p(x) \Big) \d x - \\
              & -Î»_1 - Î¼ Î»_2 - Ïƒ^2Î»_3.
\end{aligned}$$

~~~
The functional derivative of $L$ is:
$$\frac{âˆ‚}{âˆ‚p(x)} L(p; Î¼, Ïƒ^2) = Î»_1 + Î»_2 x + Î»_3 (x - Î¼)^2 - 1 - \log p(x) = 0.$$

---
class: dbend
# Function with Maximum Entropy
Rearrangint the functional derivative of $L$:
$$\frac{âˆ‚}{âˆ‚p(x)} L(p; Î¼, Ïƒ^2) = Î»_1 + Î»_2 x + Î»_3 (x - Î¼)^2 - 1 - \log p(x) = 0.$$
we obtain
$$p(x) = \exp\Big(Î»_1 + Î»_2 x + Î»_3 (x-Î¼)^2 - 1\Big).$$

~~~
We can verify that setting $Î»_1 = 1 - \log Ïƒ \sqrt{2Ï€}$, $Î»_2=0$ and $Î»_3=-1/(2Ïƒ^2)$
fulfils all the constraints, arriving at
$$p(x) = ğ“(x; Î¼, Ïƒ^2).$$

---
section: SoftMax Derivation
class: dbend
# Derivation of Softmax using Maximum Entropy

Let $ğ• = \{(â†’x_1, t_1), (â†’x_2, t_2), â€¦, (â†’x_N, t_N)\}$ be training data
of a $K$-class classification, with $â†’x_i âˆˆ â„^D$ and $t_i âˆˆ \{1, 2, â€¦, K\}$.

~~~
We want to model it using a function $Ï€: â„^D â†’ â„^K$
so that $Ï€(â†’x)$ gives a distribution of classes for input $â†’x$.

~~~
We impose the following conditions on $Ï€$:
- $$Ï€(â†’x)_j â‰¥ 0$$
~~~
- $$âˆ‘_{j=1}^K Ï€(â†’x)_j = 1$$
~~~
- $$âˆ€_{k âˆˆ \{1, 2, â€¦, D\}}, âˆ€_{j âˆˆ \{1, 2, â€¦, K\}}: âˆ‘_{i=1}^N Ï€(â†’x_i)_j x_{i,k} = âˆ‘_{i=1}^N \Big[t_i == j\Big] x_{i,k}$$

---
class: dbend
# Derivation of Softmax using Maximum Entropy

There are many such $Ï€$, one particularly bad is
$$Ï€(â†’x) = \begin{cases}
  t_i&\textrm{if there exists }i: â†’x_i = â†’x, \\
  0&\textrm{otherwise}.\end{cases}$$

~~~
Therefore, we want to find a more general $Ï€$ â€“ we will aim for one with maximum
entropy.

---
class: dbend
# Derivation of Softmax using Maximum Entropy

We therefore want to minimize $-âˆ‘_{i=1}^N âˆ‘_{j=1}^K Ï€(â†’x_i)_j \log(Ï€(â†’x_i)_j)$
given
- $Ï€(â†’x)_j â‰¥ 0$,
- $âˆ‘_{i=j}^K Ï€(â†’x)_j = 1$,
- $âˆ€_{k âˆˆ \{1, â€¦, D\}}, âˆ€_{j âˆˆ \{1, â€¦, K\}}: âˆ‘_{i=1}^N Ï€(â†’x_i)_j x_{i,k} = âˆ‘_{i=1}^N \big[t_i == j\big] x_{i,k}$.

~~~
We therefore form a Lagrangian
$$\begin{aligned}
L =& âˆ‘_{k=1}^D âˆ‘_{j=1}^K Î»_{k,j} \Big(âˆ‘_{i=1}^N Ï€(â†’x_i)_j x_{i,k} - \big[t_i == j\big] x_{i,k}\Big)\\
   & -âˆ‘_{i=1}^N Î²_i \Big(âˆ‘_{j=1}^K Ï€(â†’x_i)_j - 1\Big) \\
   & -âˆ‘_{i=1}^N âˆ‘_{j=1}^K Ï€(â†’x_i)_j \log(Ï€(â†’x_i)_j)
\end{aligned}$$

---
class: dbend
# Derivation of Softmax using Maximum Entropy

We now compute partial derivatives of the Lagrangian, notably the values
$$\frac{âˆ‚}{âˆ‚Ï€(â†’x_i)_j}L.$$

~~~
We arrive at
$$\frac{âˆ‚}{âˆ‚Ï€(â†’x_i)_j}L = â†’Î»_{*,j} â†’x_i + Î²_i - \log(Ï€(â†’x_i)_j) - 1.$$

~~~
Setting the Lagrangian to zero, we get $â†’Î»_{*,j} â†’x_i + Î²_i - \log(Ï€(â†’x_i)_j) - 1 = 0,$
which we rewrite to
$$Ï€(â†’x_i)_j = e^{â†’Î»_{*,j}â†’x_i +Î²_i-1}.$$

~~~
Such a forms guarantees $Ï€(â†’x_i)_j > 0$, which we did not include in the
conditions.

---
class: dbend
# Derivation of Softmax using Maximum Entropy

In order to find out the $Î²_i$ values, we turn to the constraint
$$âˆ‘_j Ï€(â†’x_i)_j = âˆ‘_j e^{â†’Î»_{*,j}â†’x_i +Î²_i-1} = 1,$$
from which we get
$$e^{Î²_i} = \frac{1}{âˆ‘_j e^{â†’Î»_{*,j}â†’x_i-1}},$$

~~~
yielding
$$Ï€(â†’x_i)_j = \frac{e^{â†’Î»_{*,j}â†’x_i}}{âˆ‘_k e^{â†’Î»_{*,k}â†’x_i}}.$$

---
section: KernelLR
# Kernel Linear Regression

Consider linear regression with cubic features
$$Ï†(â†’x) = \scriptsize\begin{bmatrix} 1 \\ x_1 \\ x_2 \\ â€¦ \\ x_1^2 \\ x_1x_2 \\ â€¦ \\ x_2x_1 \\ â€¦ \\ x_1^3 \\ x_1^2x_2 \\ â€¦ \end{bmatrix}.$$

~~~
The SGD update for linear regression is then
$$â†’w â† â†’w + Î±\big(t - â†’w^T Ï†(â†’x)\big) Ï†(â†’x).$$

---
# Kernel Linear Regression

When dimensionality of input is $D$, one step of SGD takes $ğ“(D^3)$.

~~~
Surprisingly, we can do better under some circumstances. We start by
noting that we can write the parameters $â†’w$ as a linear combination
of the input features $Ï†(â†’x_i)$.

~~~
By induction, $â†’w = 0 = âˆ‘_i 0 â‹… Ï†(â†’x_i)$, and assuming $â†’w = âˆ‘_i Î²_i â‹… Ï†(â†’x_i)$,
after a SGD update we get
$$\begin{aligned}
â†’w â†& â†’w + Î±âˆ‘_i \big(t_i - â†’w^T Ï†(â†’x_i)\big) Ï†(â†’x_i)\\
   =& âˆ‘_i \Big(Î²_i + Î± \big(t_i - â†’w^T Ï†(â†’x_i)\big)\Big) Ï†(â†’x_i).
\end{aligned}$$

~~~
A individual update is $Î²_i â† Î²_i + Î±\Big(t_i - â†’w^T Ï†(â†’x_i)\Big)$, and
substituting for $â†’w$ we get
$$Î²_i â† Î²_i + Î±\Big(t_i - âˆ‘\nolimits_j Î²_j Ï†(â†’x_j)^T Ï†(â†’x_i)\Big).$$

---
# Kernel Linear Regression

We can formulate the alternative linear regression algorithm (it would be called
a _dual formulation_):

<div class="algorithm">

**Input**: Dataset ($â‡‰X = \{â†’x_1, â†’x_2, â€¦, â†’x_N\} âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$), learning rate $Î± âˆˆ â„^+$.<br>

- Set $Î²_i â† 0$
- Compute all values $K(â†’x_i, â†’x_j) = Ï†(â†’x_i)^T Ï†(â†’x_j)$
- Repeat until convergence
  - Update the coordinates, either according to a full gradient update:
    - $â†’Î² â† â†’Î² + Î±(â†’t-Kâ†’Î²)$
  - or alternatively use single-batch SGD, arriving at:
    - for $i$ in random permutation of $\{1, â€¦, N\}$:
      - $Î²_i â† Î²_i + Î±\Big(t_i - âˆ‘\nolimits_j Î²_j K(â†’x_i, â†’x_j)\Big)$

    In vector notation, we can write $â†’Î² â† â†’Î² + Î±(â†’t-Kâ†’Î²)$.
</div>

~~~
The predictions are then performed by computing $y(â†’x) = â†’w^T Ï†(â†’x) = âˆ‘_i Î²_i â†’Ï†(â†’x_i)^T â†’Ï†(â†’x)$.

---
section: Kernels
# Kernel Trick

A single SGD update of all $Î²_i$ then takes $ğ“(N^2)$, given that we can
evaluate scalar dot product of $Ï†(â†’x_j)^T Ï†(â†’x_i)$ quickly.

~~~
$$\begin{aligned}
Ï†(â†’x)^T Ï†(â†’z) =& 1 + âˆ‘_i x_i z_i + âˆ‘_{i,j} x_i x_j z_i z_j + âˆ‘_{i,j,k} x_i x_j x_k z_i z_j z_k \\
              =& 1 + âˆ‘_i x_i z_i + \Big(âˆ‘_i x_i z_i\Big)^2 + \Big(âˆ‘_i x_i z_i\Big)^3 \\
              =& 1 + â†’x^T â†’z + \big(â†’x^T â†’z\big)^2 + \big(â†’x^T â†’z\big)^3.
\end{aligned}$$

---
# Kernels

We define a _kernel_ corresponding to a feature map $Ï†$ as a function
$$K(â†’x, â†’z) â‰ Ï†(â†’x)^t Ï†(â†’z).$$

~~~
There is quite a lot of theory behind kernel construction. The most often used
kernels are:

~~~
- polynomial kernel or degree $d$
  $$K(â†’x, â†’z) = (Î³ â†’x^Tâ†’z + 1)^d,$$
  which corresponds to a feature map generating all combinations of up to $d$
  input features;
~~~
- Gaussian (or RBF) kernel
  $$K(â†’x, â†’z) = e^{-Î³||â†’x-â†’z||^2},$$
  corresponding to a scalar product in an infinite-dimensional space (it is
  in a sense a combination of polynomial kernels of all degrees).

---
section: SVM
# Support Vector Machines

Let us return to a binary classification task. The perceptron algorithm
guaranteed finding some separating hyperplane if it existed; we now consider
finding the one with _maximum margin_.

![w=100%,h=center](svm_margin.pdf)

---
# Support Vector Machines

Assume we have a dataset $â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{-1, 1\}^N$, feature map $Ï†$ and model
$$y(â†’x) â‰ â†’Ï†(â†’x)^T â†’w + b.$$

~~~
![w=30%,f=right](../03/binary_classification.pdf)

We already know that the distance of a point $â†’x_i$ to the decision boundary is
$$\frac{|y(â†’x_i)|}{||â†’w||} = \frac{t_i y(â†’x_i)}{||â†’w||}.$$

~~~
We therefore want to maximize
$$\argmax_{w,b} \frac{1}{||â†’w||} \min_i \big[t_i (â†’Ï†(â†’x)^T â†’w + b)\big].$$

However, this problem is difficult to optimize directly.

---
# Support Vector Machines

Because the model is invariant to multiplying $â†’w$ and $b$ by a constant, we can
say that for the points closest to the decision boundary, it will hold that
$$t_i y(â†’x_i) = 1.$$

~~~
Then for all the points we will have $t_i y(â†’x_i) â‰¥ 1$ and we can simplify
$$\argmax_{w,b} \frac{1}{||â†’w||} \min_i \big[t_i (â†’Ï†(â†’x)^T â†’w + b)\big]$$
to
$$\argmin_{w,b} \frac{1}{2} ||â†’w||^2 \textrm{~given that~~}t_i y(â†’x_i) â‰¥ 1.$$

---
section: KKT
# Lagrange Multipliers â€“ Inequality Constraints

Given a funtion $J(â†’x)$, we can find a maximum with respect to a vector
$â†’x âˆˆ â„^d$, by investigating the critical points $âˆ‡_â†’x J(â†’x) = 0$.

We even know how to incorporate constraints of form $g(â†’x) = 0$.

~~~
![w=25%,f=right](lagrange_inequalities.pdf)

We now describe how to include inequality constraints $g(â†’x) â‰¥ 0$.

~~~
The optimum can either be attained for $g(â†’x) > 0$, when the constraint is said
to be _inactive_, or for $g(â†’x) = 0$, when the constraint is sayd to be
_active_.

~~~
In the inactive case, the maximum is again a critical point of the Langrangian,
with $Î»=0$.
~~~
When maximum is on boundary, it corresponds to a critical point
with $Î»â‰ 0$ â€“ but note that this time the sign of the multiplier matters, because
maximum is attained only when gradient of $f(â†’x)$ is oriented away from the region
$g(â†’x) â‰¥ 0$. We therefore require $âˆ‡f(â†’x) = - Î»âˆ‡g(â†’x)$ for $Î»>0$.

~~~
In both cases, $Î» g(â†’x) = 0$.

---
section: KKT
# Karush-Khun-Tucker Conditions

![w=25%,f=right](lagrange_inequalities.pdf)

Therefore, the solution to a maximization problem of $f(x)$ subject to $g(â†’x)â‰¥0$
can be found by inspecting all points where the derivation of the Lagrangian is zero,
subject to the following conditions:
$$\begin{aligned}
g(â†’x) &â‰¥ 0 \\
Î» &â‰¥ 0 \\
Î» g(â†’x) &= 0
\end{aligned}$$

~~~
# Necessary and Sufficient KKT Conditions

The above conditions are necessary conditions for a minimum. However, it can be
proven that in the following settings, the conditions are also **sufficient**:
- if the objective to optimize is a _convex_ function,
~~~
- the inequality constraings are continuously differentiable convex functions,
~~~
- the equality constraints are affine functions (linear functions with an
  offset).


---
section: Dual SVM Formulation
# Support Vector Machines

In order to solve the constrained problem of
$$\argmin_{w,b} \frac{1}{2} ||â†’w||^2 \textrm{~given that~~}t_i y(â†’x_i) â‰¥ 1,$$
~~~
we write the Lagrangian with multipliers $â†’a=(a_1, â€¦, a_N)$ as
$$L = \frac{1}{2} ||â†’w||^2 - âˆ‘_i a_i \big[t_i y(â†’x_i) - 1\big].$$

~~~
Setting the derivatives with respect to $â†’w$ and $b$ to zero, we get
$$\begin{aligned}
â†’w =& âˆ‘_i a_i t_iÏ†(â†’x_i) \\
 0 =& âˆ‘_i a_i t_i \\
\end{aligned}$$

---
# Support Vector Machines

Substituting these to the Lagrangian, we get
$$L = âˆ‘_i a_i -  \frac{1}{2} âˆ‘_i âˆ‘_j a_i a_j t_i t_j K(â†’x_i, â†’x_j)$$
with respect to the constraints $âˆ€_i: a_i â‰¥ 0$, $âˆ‘_i a_i t_i = 0$
and kernel $K(â†’x, â†’z) = Ï†(â†’x)^T Ï†(â†’z).$

~~~
The solution of this Lagrangian will fulfil the KKT conditions, meaning that
$$\begin{aligned}
a_i &â‰¥ 0 \\
t_i y(â†’x_i) - 1 &â‰¥ 0 \\
a_i \big(t_i y(â†’x_i) - 1\big) &= 0.
\end{aligned}$$

~~~
Therefore, either a point is on a boundary, or $a_i=0$. Given that the
predictions for point $â†’x$ are given by $y(â†’x) = âˆ‘ a_i t_i K(â†’x, â†’x_i) + b$,
we need to keep only the points on the boundary, the so-called **support vectors**.

---
# Support Vector Machines

The dual formulation allows us to use non-linear kernels.

![w=100%](svm_gaussian.pdf)
