title: NPFL129, Lecture 5
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Derivation of Softmax,<br> Kernel Methods

## Milan Straka

### November 02, 2019

---
section: LagrangeMult
# Lagrange Multipliers â€“ Equality Constraints

![w=28%,f=right](lagrange_equalities.svgz)

Given a funtion $J(â†’x)$, we can find a maximum with respect to a vector
$â†’x âˆˆ â„^d$, by investigating the critical points $âˆ‡_â†’x J(â†’x) = 0$.

~~~
Consider now finding maximum subject to a constraint $g(â†’x) = 0$.

~~~
- Note that $âˆ‡_â†’x g(â†’x)$ is orthogonal to the surface of the constraint, because
  if $â†’x$ and a nearby point $â†’x+â†’Îµ$ lie on the surface, from the Taylor
  expansion $g(â†’x+â†’Îµ) â‰ˆ g(â†’x) + â†’Îµ^T âˆ‡_â†’x g(â†’x)$ we get $â†’Îµ^T âˆ‡_â†’x g(â†’x) â‰ˆ 0$.

~~~
- In the seeked maximum, $âˆ‡_â†’x f(â†’x)$ must also be orthogonal to the constraint
  surface (or else moving in the direction of the derivative would increase the
  value).

~~~
- Therefore, there must exist $Î»$ such that $âˆ‡_â†’x f + Î»âˆ‡_â†’x g = 0$.

---
# Lagrange Multipliers â€“ Equality Constraints

![w=28%,f=right](lagrange_equalities.svgz)

We therefore introduce the _Lagrangian function_
$$L(â†’x, Î») â‰ f(â†’x) + Î»g(â†’x).$$

~~~
We can then find the maximum under the constraint by inspecting critical points
of $L(â†’x, Î»)$ with respect to both $â†’x$ and $Î»$:
- $\frac{âˆ‚L}{âˆ‚Î»} = 0$ leads to $g(â†’x)=0$;
- $\frac{âˆ‚L}{âˆ‚â†’x} = 0$ is the previously derived $âˆ‡_â†’x f + Î»âˆ‡_â†’x g = 0$.



---
section: VariationalCalc
class: dbend
# Calculus of Variations

Many optimization techniques depend on minimizing a function $J(â†’w)$ with
respect to a vector $â†’w âˆˆ â„^d$, by investigating the critical points $âˆ‡_â†’w J(â†’w) = 0$.

~~~
A function of a function, $J[f]$, is known as a **functional**, for example
entropy $H[â‹…]$.

~~~
Similarly to partial derivatives, we can take **functional derivatives** of
a functional $J[f]$ with respect to individual values $f(â†’x)$ for all points
$â†’x$. The functional derivative of $J$ with respect to a function $f$ in a point
$â†’x$ is denoted as
$$\frac{âˆ‚}{âˆ‚f(â†’x)} J.$$

~~~
For this course, we use only the following theorem stating that for
all differentiable functions $f$ and differentiable functions $g(y=f(â†’x), â†’x)$ with
continuous derivatives, it holds that
$$\frac{âˆ‚}{âˆ‚f(â†’x)} âˆ«g(f(â†’x), â†’x) \dâ†’x = \frac{âˆ‚}{âˆ‚y} g(y, â†’x).$$

---
section: VariationalCalc
class: dbend
# Calculus of Variations

An intuitive view is to think about $f(â†’x)$ as a vector of uncountably many
elements (for every value $â†’x)$. In this interpretation the result is analogous
to computing partial derivatives of a vector $â†’w âˆˆ â„^d$:
$$\frac{âˆ‚}{âˆ‚w_i} âˆ‘_j g(w_j, â†’x) = \frac{âˆ‚}{âˆ‚w_i} g(w_i, â†’x).$$
$$\frac{âˆ‚}{âˆ‚f(â†’x)} âˆ«g(f(â†’x), â†’x) \dâ†’x = \frac{âˆ‚}{âˆ‚y} g(y, â†’x).$$

---
section: NAsMaxEnt
class: dbend
# Function with Maximum Entropy

What distribution over $â„$ maximizes entropy $H[p] = -ğ”¼_x \log p(x)$?

~~~
For continuous values, the entropy is an integral $H[p] = -âˆ«p(x) \log p(x) \d x$.

~~~
We cannot just maximize $H$ with respect to a function $p$, because:
- the result might not be a probability distribution â€“ we need to add
  a constraint that $âˆ«p(x) \d x=1$;
~~~
- the problem is underspecified because a distribution can be shifted without
  changing entropy â€“ we add a constraint $ğ”¼[x] = Î¼$;
~~~
- because entropy increases as variance increases, we ask which distribution
  with a _fixed_ variance $Ïƒ^2$ has maximum entropy â€“ adding a constraint
  $\Var(x) = Ïƒ^2$.

---
class: dbend
# Function with Maximum Entropy

Lagrangian of all the constraints and the entropy function is
$$L(p; Î¼, Ïƒ^2) = Î»_1 \Big(âˆ«p(x) \d x - 1\Big) + Î»_2 \big(ğ”¼[x] - Î¼\big) + Î»_3\big(\Var(x) - Ïƒ^2\big) + H[p].$$

~~~
By expanding all definitions to integrals, we get
$$\begin{aligned}
L(p; Î¼, Ïƒ^2) =& âˆ«\Big(Î»_1 p(x) + Î»_2 p(x) x + Î»_3 p(x) (x - Î¼)^2 - p(x)\log p(x) \Big) \d x - \\
              & -Î»_1 - Î¼ Î»_2 - Ïƒ^2Î»_3.
\end{aligned}$$

~~~
The functional derivative of $L$ is:
$$\frac{âˆ‚}{âˆ‚p(x)} L(p; Î¼, Ïƒ^2) = Î»_1 + Î»_2 x + Î»_3 (x - Î¼)^2 - 1 - \log p(x) = 0.$$

---
class: dbend
# Function with Maximum Entropy
Rearrangint the functional derivative of $L$:

$$\frac{âˆ‚}{âˆ‚p(x)} L(p; Î¼, Ïƒ^2) = Î»_1 + Î»_2 x + Î»_3 (x - Î¼)^2 - 1 - \log p(x) = 0.$$

we obtain
$$p(x) = \exp\Big(Î»_1 + Î»_2 x + Î»_3 (x-Î¼)^2 - 1\Big).$$

~~~
We can verify that setting $Î»_1 = 1 - \log Ïƒ \sqrt{2Ï€}$, $Î»_2=0$ and $Î»_3=-1/(2Ïƒ^2)$
fulfils all the constraints, arriving at
$$p(x) = ğ“(x; Î¼, Ïƒ^2).$$
