title: NPFL129, Lecture 5
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Derivation of Softmax, k-NN

## Milan Straka

### November 02, 2019

---
section: LagrangeM
# Lagrange Multipliers ‚Äì Equality Constraints

![w=28%,f=right](lagrange_equalities.svgz)

Given a funtion $f(‚Üíx)$, we can find a maximum with respect to a vector
$‚Üíx ‚àà ‚Ñù^d$, by investigating the critical points $‚àá_‚Üíx f(‚Üíx) = 0$.

~~~
Consider now finding maximum subject to a constraint $g(‚Üíx) = 0$.

~~~
- Note that $‚àá_‚Üíx g(‚Üíx)$ is orthogonal to the surface of the constraint, because
  if $‚Üíx$ and a nearby point $‚Üíx+‚ÜíŒµ$ lie on the surface, from the Taylor
  expansion $g(‚Üíx+‚ÜíŒµ) ‚âà g(‚Üíx) + ‚ÜíŒµ^T ‚àá_‚Üíx g(‚Üíx)$ we get $‚ÜíŒµ^T ‚àá_‚Üíx g(‚Üíx) ‚âà 0$.

~~~
- In the seeked maximum, $‚àá_‚Üíx f(‚Üíx)$ must also be orthogonal to the constraint
  surface (or else moving in the direction of the derivative would increase the
  value).

~~~
- Therefore, there must exist $Œª$ such that $‚àá_‚Üíx f + Œª‚àá_‚Üíx g = 0$.

---
# Lagrange Multipliers ‚Äì Equality Constraints

![w=28%,f=right](lagrange_equalities.svgz)

We therefore introduce the _Lagrangian function_
$$L(‚Üíx, Œª) ‚âù f(‚Üíx) + Œªg(‚Üíx).$$

~~~
We can then find the maximum under the constraint by inspecting critical points
of $L(‚Üíx, Œª)$ with respect to both $‚Üíx$ and $Œª$:
- $\frac{‚àÇL}{‚àÇŒª} = 0$ leads to $g(‚Üíx)=0$;
- $\frac{‚àÇL}{‚àÇ‚Üíx} = 0$ is the previously derived $‚àá_‚Üíx f + Œª‚àá_‚Üíx g = 0$.

~~~
If there are multiple equality constraints, we can use induction; therefore,
every constraint gets its own $Œª$.

---
section: NAsMaxEnt
class: dbend
# Calculus of Variations

Many optimization techniques depend on minimizing a function $J(‚Üíw)$ with
respect to a vector $‚Üíw ‚àà ‚Ñù^d$, by investigating the critical points $‚àá_‚Üíw J(‚Üíw) = 0$.

~~~
A function of a function, $J[f]$, is known as a **functional**, for example
entropy $H[‚ãÖ]$.

~~~
Similarly to partial derivatives, we can take **functional derivatives** of
a functional $J[f]$ with respect to individual values $f(‚Üíx)$ for all points
$‚Üíx$. The functional derivative of $J$ with respect to a function $f$ in a point
$‚Üíx$ is denoted as
$$\frac{‚àÇ}{‚àÇf(‚Üíx)} J.$$

~~~
For this course, we use only the following theorem stating that for
all differentiable functions $f$ and differentiable functions $g\big(y=f(‚Üíx), ‚Üíx\big)$ with
continuous derivatives, it holds that
$$\frac{‚àÇ}{‚àÇf(‚Üíx)} ‚à´g\big(f(‚Üíx'), ‚Üíx'\big) \d‚Üíx' = \frac{‚àÇ}{‚àÇy} g(y, ‚Üíx).$$

---
class: dbend
# Calculus of Variations

An intuitive view is to think about $f(‚Üíx)$ as a vector of uncountably many
elements (for every value $‚Üíx)$. In this interpretation the result is analogous
to computing partial derivatives of a vector $‚Üíw ‚àà ‚Ñù^d$:
$$\frac{‚àÇ}{‚àÇw_i} ‚àë_j g(w_j, ‚Üíx) = \frac{‚àÇ}{‚àÇw_i} g(w_i, ‚Üíx).$$
$$\frac{‚àÇ}{‚àÇf(‚Üíx)} ‚à´g\big(f(‚Üíx'), ‚Üíx'\big) \d‚Üíx' = \frac{‚àÇ}{‚àÇy} g(y, ‚Üíx).$$

---
class: dbend
# Function with Maximum Entropy

What distribution over $‚Ñù$ maximizes entropy $H[p] = -ùîº_x [\log p(x)]$?

~~~
For continuous values, the entropy is an integral $H[p] = -‚à´p(x) \log p(x) \d x$.

~~~
We cannot just maximize $H$ with respect to a function $p$, because:
- the result might not be a probability distribution ‚Äì we need to add
  a constraint that $‚à´p(x) \d x=1$;
~~~
- the problem is underspecified because a distribution can be shifted without
  changing entropy ‚Äì we add a constraint $ùîº[x] = Œº$;
~~~
- because entropy increases as variance increases, we ask which distribution
  with a _fixed_ variance $œÉ^2$ has maximum entropy ‚Äì adding a constraint
  $\Var(x) = œÉ^2$.

---
class: dbend
# Function with Maximum Entropy

Lagrangian $L(p(x), x, ‚ÜíŒª; Œº, œÉ^2)$ of all the constraints and the entropy function is
$$L = Œª_1 \Big(‚à´p(x) \d x - 1\Big) + Œª_2 \big(ùîº[x] - Œº\big) + Œª_3\big(\Var(x) - œÉ^2\big) + H[p].$$

~~~
By expanding all definitions to integrals, we get
$$\begin{aligned}
L(p(x), x, ‚ÜíŒª; Œº, œÉ^2) =& ‚à´\Big(Œª_1 p(x) + Œª_2 p(x) x + Œª_3 p(x) (x - Œº)^2 - p(x)\log p(x) \Big) \d x - \\
                        & -Œª_1 - Œº Œª_2 - œÉ^2Œª_3.
\end{aligned}$$

~~~
The functional derivative of $L$ is:
$$\frac{‚àÇ}{‚àÇp(x)} L(p(x), x, ‚ÜíŒª; Œº, œÉ^2) = Œª_1 + Œª_2 x + Œª_3 (x - Œº)^2 - 1 - \log p(x) = 0.$$

---
class: dbend
# Function with Maximum Entropy

Rearranging the functional derivative of $L$:

$$\frac{‚àÇ}{‚àÇp(x)} L(p(x), x, ‚ÜíŒª; Œº, œÉ^2) = Œª_1 + Œª_2 x + Œª_3 (x - Œº)^2 - 1 - \log p(x) = 0.$$

we obtain
$$p(x) = \exp\Big(Œª_1 + Œª_2 x + Œª_3 (x-Œº)^2 - 1\Big).$$

~~~
We can verify that setting $Œª_1 = 1 - \log œÉ \sqrt{2œÄ}$, $Œª_2=0$ and $Œª_3=-1/(2œÉ^2)$
fulfils all the constraints, arriving at
$$p(x) = ùìù(x; Œº, œÉ^2).$$

---
section: SoftMax
# Derivation of Softmax using Maximum Entropy

Let $‚áâX = \{(‚Üíx_1, t_1), (‚Üíx_2, t_2), ‚Ä¶, (‚Üíx_N, t_N)\}$ be training data
of a $K$-class classification, with $‚Üíx_i ‚àà ‚Ñù^D$ and $t_i ‚àà \{1, 2, ‚Ä¶, K\}$.

~~~
We want to model it using a function $œÄ: ‚Ñù^D ‚Üí ‚Ñù^K$
so that $œÄ(‚Üíx)$ gives a distribution of classes for input $‚Üíx$.

~~~
We impose the following conditions on $œÄ$:
- $$‚àÄ\,1 ‚â§ k ‚â§ K: œÄ(‚Üíx)_k ‚â• 0,$$
~~~
- $$‚àë_{k=1}^K œÄ(‚Üíx)_k = 1,$$
~~~
- $$‚àÄ\,1 ‚â§ j ‚â§ D, ‚àÄ\,1 ‚â§ k ‚â§ K: ‚àë_{i=1}^N œÄ(‚Üíx_i)_k x_{i,j} = ‚àë_{i=1}^N \Big[t_i == k\Big] x_{i,j}.$$

---
# Derivation of Softmax using Maximum Entropy

There are many such $œÄ$, one particularly bad is
$$œÄ(‚Üíx) = \begin{cases}
  ‚Üí1_{t_i}&\textrm{if there exists }i: ‚Üíx_i = ‚Üíx, \\
  ‚Üí1_0&\textrm{otherwise},\end{cases}$$
where $‚Üí1_i$ is a vector of zeros, except for position $i$, which is equal to 1.

~~~
Therefore, we want to find a more **general** $œÄ$ ‚Äì consequently, we turn to the principle of maximum
entropy and search for $œÄ$ with maximum entropy.

---
# Derivation of Softmax using Maximum Entropy

We want to maximize $-‚àë_{i=1}^N ‚àë_{k=1}^K œÄ(‚Üíx_i)_k \log(œÄ(‚Üíx_i)_k)$
given
- $‚àÄ\,1 ‚â§ i ‚â§ N, ‚àÄ\,1 ‚â§ k ‚â§ K: œÄ(‚Üíx_i)_k ‚â• 0$,
- $‚àÄ\,1 ‚â§ i ‚â§ N: ‚àë_{k=1}^K œÄ(‚Üíx_i)_k = 1$,
- $‚àÄ\,1 ‚â§ j ‚â§ D, ‚àÄ\,1 ‚â§ k ‚â§ K: ‚àë_{i=1}^N œÄ(‚Üíx_i)_k x_{i,j} = ‚àë_{i=1}^N \big[t_i == k\big] x_{i,j}$.

~~~
We therefore form a Lagrangian (ignoring the first inequality constraint):
$$\begin{aligned}
L =& ‚àë_{j=1}^D ‚àë_{k=1}^K Œª_{j,k} \Big(‚àë_{i=1}^N œÄ(‚Üíx_i)_k x_{i,j} - \big[t_i == k\big] x_{i,j}\Big)\\
   & +‚àë_{i=1}^N Œ≤_i \Big(‚àë_{k=1}^K œÄ(‚Üíx_i)_k - 1\Big) \\
   & -‚àë_{i=1}^N ‚àë_{k=1}^K œÄ(‚Üíx_i)_k \log(œÄ(‚Üíx_i)_k)
\end{aligned}$$

---
# Derivation of Softmax using Maximum Entropy

We now compute partial derivatives of the Lagrangian, notably the values
$$\frac{‚àÇ}{‚àÇœÄ(‚Üíx_i)_k}L.$$

~~~
We arrive at
$$\frac{‚àÇ}{‚àÇœÄ(‚Üíx_i)_k}L = ‚Üíx_i^T ‚ÜíŒª_{*,k} + Œ≤_i - \log(œÄ(‚Üíx_i)_k) - 1.$$

~~~
Setting the Lagrangian to zero, we get $‚Üíx_i^T ‚ÜíŒª_{*,k} + Œ≤_i - \log(œÄ(‚Üíx_i)_k) - 1 = 0,$
which we rewrite to
$$œÄ(‚Üíx_i)_k = e^{‚Üíx_i^T ‚ÜíŒª_{*,k} + Œ≤_i - 1}.$$

~~~
Such a forms guarantees $œÄ(‚Üíx_i)_k > 0$, which we did not include in the
conditions.

---
# Derivation of Softmax using Maximum Entropy

In order to find out the $Œ≤_i$ values, we turn to the constraint
$$‚àë_k œÄ(‚Üíx_i)_k = ‚àë_k e^{‚Üíx_i^T ‚ÜíŒª_{*,k} +Œ≤_i-1} = 1,$$
from which we get
$$e^{Œ≤_i} = \frac{1}{‚àë_k e^{‚Üíx_i^T ‚ÜíŒª_{*,k} - 1}},$$

~~~
yielding
$$œÄ(‚Üíx_i)_k = e^{‚Üíx_i^T ‚ÜíŒª_{*,k} + Œ≤_i - 1} = \frac{e^{‚Üíx_i^T ‚ÜíŒª_{*,k}}}{‚àë_{k'} e^{‚Üíx_i^T ‚ÜíŒª_{*,k'}}} = \softmax(‚Üíx_i ‚áâŒª)_k.$$

---
section: F-score
# F1-score

When evaluating binary classification, we have used **accuracy** so far.

~~~
![w=36%,f=right](true_false_positives.svgz)

However, there are other metric we might want to consider. One of them is
$F_1$-score.

~~~
Consider the following **confusion matrix**:
| |Target positive|Target negative|
|-|---------------|---------------|
|Predicted<br>positive| **True Positive** (**TP**) | **False Positive** (**FP**) |
|Predicted<br>negative| **False Negative** (**FN**) | **True Negative** (**TN**) |

~~~
Accuracy can be computed as
$$\accuracy = \frac{\TP+\TN}{\TP+\TN+\FP+\FN}.$$

---
# F1-score

![w=50%,h=center,mw=36%,f=right](true_false_positives.svgz)

| |Target positive|Target negative|
|-|---------------|---------------|
|Predicted<br>positive| **True Positive** (**TP**) | **False Positive** (**FP**) |
|Predicted<br>negative| **False Negative** (**FN**) | **True Negative** (**TN**) |

In some cases, we are mostly interested in positive examples.

~~~
![w=100%,h=center,mw=36%,f=right](precision_recall.svgz)

We define **precision** (percentage of correct predictions in predicted examples)
and **recall** (percentage of correct predictions in the gold examples) as
$$\begin{aligned}
  \precision =& \frac{\TP}{\TP+\FP}, \\
  \recall =& \frac{\TP}{\TP+\FN}.
\end{aligned}$$

---
# F1-score

![w=46%,f=right](roc_pr.png)

The precision and recall go ‚Äúagainst each other‚Äù: increasing the classifier
threshold usually increases recall and decreases precision, and vice versa.

~~~
We therefore define a single **$\boldsymbol{F_1}$-score** as a harmonic mean of precision and recall:
$$\begin{aligned}
  F_1 =& \frac{2}{\precision^{-1} + \recall^{-1}} \\
      =& \frac{2 ‚ãÖ \precision ‚ãÖ \recall}{\precision + \recall}
         = \frac{\TP~~~~~+~~~~~\TP}{\TP+\FP+\TP+\FN}.
\end{aligned}$$

~~~
The $F_1$ score can be generalized to $F_Œ≤$ score, where recall is $Œ≤$ times
more important than precision; $F_2$ favoring recall and $F_{0.5}$ favoring
precision are commonly used.

$$ F_Œ≤ = \frac{(1 + Œ≤^2) ‚ãÖ \precision ‚ãÖ \recall}{Œ≤^2 ‚ãÖ \precision + \recall}
       = \frac{\TP~~~~~+~~~~~\hphantom{()}Œ≤^2\TP}{\TP+\FP+Œ≤^2(\TP+\FN)}.$$

---
# Precision-Recall Curve

![w=46%,f=right](roc_pr.png)

Changing the threshold in logistic regression allows us to trade off precision
for recall and vice versa. Therefore, we can tune it on the development set to
achieve highest possible $F_1$ score, if required.

~~~
Also, if we want to evaluate $F_1$-score without considering a specific
threshold, the **area under curve** (AUC) is sometimes used as a metric.

---
# F1-Score in Multiclass Classification

To extend $F_1$-score to multiclass classification, we expect one of the classes
to be _negative_ and the others _different kinds of positive_. For each of the
possitive classes, we compute the same confusion matrix as in the binary case
(considering all other labels as negative ones), and then combine the results
in one of the following ways:

~~~
- **micro-averaged** $F_1$ (or just **micro** $F_1$): we first sum all the TP,
  FP and FN of the individual binary classifications and compute the final
  $F_1$-score (this way, the frequency of the individual classes is taken into
  account);

~~~
- **macro-averaged** $F_1$ (or just **macro** $F_1$): we first compute the
  $F_1$-scores of the individual binary classifications and then compute
  an unweighted average (therefore, the frequence of the classes is ignored).

---
section: ROC
# ROC Curve

The precision-recall curve is useful when we are interested
in the positive examples (i.e., we are ignoring true negative instances).
In case we want to consider also the true negatives, we might instead
use the **Receiver Operating Characteristic** (ROC) curve.

~~~
In the ROC curve, we consider two measures of a binary classifier under changing
threshold:
- **true positive rate** or **sensitivity** (probability of detection):
  $\frac{\TP}{\textrm{target positives}} = \frac{\TP}{\TP + \FN}$;
- **false positive rate** or **1-specificity** (probability of false alarm):
  $\frac{\FP}{\textrm{target negatives}} = \frac{\FP}{\FP + \TN}$;

![w=50%](roc_pr.png)![w=70%,mw=50%,h=center](roc_curve.svgz)

---
#  Binary Confusion Metric Measures Overview

| |Target positive|Target negative| |
|-|---------------|---------------|-|
|Predicted positive| **True Positive** (**TP**) | **False Positive** (**FP**)<br>Type I Error | **precision**<br>$\frac{\TP}{\TP+\FP}$ $\begin{smallmatrix}\colorbox{darkgreen}{\color{red}1}\colorbox{darkgreen}{\phantom{0}}\\\colorbox{gray}{\phantom{0}}\colorbox{gray}{\phantom{0}}\end{smallmatrix}$ |
|Predicted negative| **False Negative** (**FN**)<br>Type II Error | **True Negative** (**TN**) |
|                     | **true positive rate**, **recall**,<br> **sensitivity** $\frac{\TP}{\TP+\FN}$ $\begin{smallmatrix}\colorbox{darkgreen}{\color{red}1}\colorbox{gray}{\phantom{0}}\\\colorbox{darkgreen}{\phantom{0}}\colorbox{gray}{\phantom{0}}\end{smallmatrix}$ | **false positive rate** $\frac{\FP}{\FP+\TN}$ $\begin{smallmatrix}\colorbox{gray}{\phantom{0}}\colorbox{darkgreen}{\color{red}1}\\\colorbox{gray}{\phantom{0}}\colorbox{darkgreen}{\phantom{0}}\end{smallmatrix}$<br>**specificity** $\frac{\TN}{\TN+\FP}$ $\begin{smallmatrix}\colorbox{gray}{\phantom{0}}\colorbox{darkgreen}{\phantom{0}}\\\colorbox{gray}{\phantom{0}}\colorbox{darkgreen}{\color{red}1}\end{smallmatrix}$ |

- $F_1$-score = $\frac{2 ‚ãÖ \precision ‚ãÖ \recall}{\precision + \recall} = \frac{\TP~~~~+~~~~\TP}{\TP+\FP+\TP+\FN}$ $\begin{smallmatrix}\colorbox{darkgreen}{\color{red}2}\colorbox{darkgreen}{\phantom{0}}\\\colorbox{darkgreen}{\phantom{0}}\colorbox{gray}{\phantom{0}}\end{smallmatrix}$

- accuracy = $\frac{\TP+\TN}{\TP+\FP+\FN+\TN}$ $\begin{smallmatrix}\colorbox{darkgreen}{\color{red}1}\colorbox{darkgreen}{\phantom{0}}\\\colorbox{darkgreen}{\phantom{0}}\colorbox{darkgreen}{\color{red}1}\end{smallmatrix}$

---
section: (Non)ParametricModels
# Parametric and Nonparametric Models

All the machine learning models which we discussed so far are **parametric**,
because they use a _fixed_ number of parameters (usually depending on the
number of features, $K$ for multiclass classification, hidden layer in MLPs, ‚Ä¶).

~~~
However, there also exist **nonparametric** models. Even if the name seems to
suggest they do not have any parameters, they have a non-fixed number of
parameters, because the number of parameters usually depend on the size of the
training data ‚Äì therefore, the model size usually grows with the size of the
training data.

---
section: k-NN
# k-Nearest Neighbors

![w=38%,f=right](k_nn.svgz)

A simple but sometimes effective nonparametric method for both classification
and regression is **$\boldsymbol k$-nearest neighbors** algorithm.

~~~
The training phase of the $k$-nearest neighbors algorithm is trivial, it consists of
only storing the whole train set (the so-called **lazy learning**).

~~~
For a given test example, the main idea is to use the targets of the most
similar training data to perform the prediction.

---
# k-Nearest Neighbors

![w=38%,f=right](k_nn.svgz)

Several hyperparameters influence the behaviour of the prediction phase:

~~~
- **k**: consider $k$ most similar training examples (higher $k$ usually
  decrease variance, but increase bias);

~~~
- **metric**: a function used to find the nearest neighbors; common choices
  are metrics based on $L_p$ norms (with usual values of $p$ being $1$, $2$, $3$, $‚àû$).
  For $‚Üíx, ‚Üíy ‚àà ‚Ñù^D$, the distance is measured as $||‚Üíx-‚Üíy||_p$, where
  $$||x||_p = \Big(‚àë\nolimits_i |x_i|^p\Big)^{1/p};$$
~~~
- **weights**: optionally, more similar examples can be considered with bigger
  weights:
  - _uniform_: all $k$ nearest neighbors are considered equally;
  - _inverse_: the weight of an example is proportional to the inverse of distance;
  - _softmax_: the weights are proportional to $\softmax$ of negative distances.

---
# k-Nearest Neighbors

### Regression

To perform regression when $k$ nearest neighbors have values $t_i$ and weights
$w_i$, we predict
$$t = ‚àë_i \frac{w_i}{‚àë\nolimits_j w_j} ‚ãÖ t_i.$$

~~~
### Classification

For uniform weights, we can use **voting** during prediction ‚Äì the most
frequent class is predicted (with ties broken arbitrarily).

~~~
Otherwise, we weight the categorical distributions $t_i ‚àà R^K$ (with the
training target classes represented using one-hot encoding),
predicting a distribution
$$‚Üít = ‚àë_i \frac{w_i}{‚àë\nolimits_j w_j} ‚ãÖ ‚Üít_i.$$
The predicted class is then the one with largets probability, i.e.,
$\argmax\nolimits_k ‚àë_i w_i t_{i,k}$.

---
# k-Nearest Neighbors

A trivial implementation of the $k$-nearest neighbors algorithm is extremely
demanding during the inference, requiring to measure distances of a given
example to all training instances.

~~~
However, several data structures capable of speeding up the $k$-nearest neighbor
search exist, like
- $k$-$d$ trees, which allow both a static or dynamic construction and can perform
  nearest neighbor queries of uniformly random points in logarithmic time on
  average, but which become inefficient for high-dimensional data;

- ball trees, R-trees, ‚Ä¶
