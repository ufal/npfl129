title: NPFL129, Lecture 5
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Derivation of Softmax, k-NN, Kernel Methods

## Milan Straka

### November 02, 2019

---
section: LagrangeM
# Lagrange Multipliers â€“ Equality Constraints

![w=28%,f=right](lagrange_equalities.svgz)

Given a funtion $J(â†’x)$, we can find a maximum with respect to a vector
$â†’x âˆˆ â„^d$, by investigating the critical points $âˆ‡_â†’x J(â†’x) = 0$.

~~~
Consider now finding maximum subject to a constraint $g(â†’x) = 0$.

~~~
- Note that $âˆ‡_â†’x g(â†’x)$ is orthogonal to the surface of the constraint, because
  if $â†’x$ and a nearby point $â†’x+â†’Îµ$ lie on the surface, from the Taylor
  expansion $g(â†’x+â†’Îµ) â‰ˆ g(â†’x) + â†’Îµ^T âˆ‡_â†’x g(â†’x)$ we get $â†’Îµ^T âˆ‡_â†’x g(â†’x) â‰ˆ 0$.

~~~
- In the seeked maximum, $âˆ‡_â†’x f(â†’x)$ must also be orthogonal to the constraint
  surface (or else moving in the direction of the derivative would increase the
  value).

~~~
- Therefore, there must exist $Î»$ such that $âˆ‡_â†’x f + Î»âˆ‡_â†’x g = 0$.

---
# Lagrange Multipliers â€“ Equality Constraints

![w=28%,f=right](lagrange_equalities.svgz)

We therefore introduce the _Lagrangian function_
$$L(â†’x, Î») â‰ f(â†’x) + Î»g(â†’x).$$

~~~
We can then find the maximum under the constraint by inspecting critical points
of $L(â†’x, Î»)$ with respect to both $â†’x$ and $Î»$:
- $\frac{âˆ‚L}{âˆ‚Î»} = 0$ leads to $g(â†’x)=0$;
- $\frac{âˆ‚L}{âˆ‚â†’x} = 0$ is the previously derived $âˆ‡_â†’x f + Î»âˆ‡_â†’x g = 0$.

~~~
If there are multiple equality constraints, we can use induction; therefore,
every constraint gets its own $Î»$.

---
section: NAsMaxEnt
class: dbend
# Calculus of Variations

Many optimization techniques depend on minimizing a function $J(â†’w)$ with
respect to a vector $â†’w âˆˆ â„^d$, by investigating the critical points $âˆ‡_â†’w J(â†’w) = 0$.

~~~
A function of a function, $J[f]$, is known as a **functional**, for example
entropy $H[â‹…]$.

~~~
Similarly to partial derivatives, we can take **functional derivatives** of
a functional $J[f]$ with respect to individual values $f(â†’x)$ for all points
$â†’x$. The functional derivative of $J$ with respect to a function $f$ in a point
$â†’x$ is denoted as
$$\frac{âˆ‚}{âˆ‚f(â†’x)} J.$$

~~~
For this course, we use only the following theorem stating that for
all differentiable functions $f$ and differentiable functions $g(y=f(â†’x), â†’x)$ with
continuous derivatives, it holds that
$$\frac{âˆ‚}{âˆ‚f(â†’x)} âˆ«g(f(â†’x), â†’x) \dâ†’x = \frac{âˆ‚}{âˆ‚y} g(y, â†’x).$$

---
class: dbend
# Calculus of Variations

An intuitive view is to think about $f(â†’x)$ as a vector of uncountably many
elements (for every value $â†’x)$. In this interpretation the result is analogous
to computing partial derivatives of a vector $â†’w âˆˆ â„^d$:
$$\frac{âˆ‚}{âˆ‚w_i} âˆ‘_j g(w_j, â†’x) = \frac{âˆ‚}{âˆ‚w_i} g(w_i, â†’x).$$
$$\frac{âˆ‚}{âˆ‚f(â†’x)} âˆ«g(f(â†’x), â†’x) \dâ†’x = \frac{âˆ‚}{âˆ‚y} g(y, â†’x).$$

---
class: dbend
# Function with Maximum Entropy

What distribution over $â„$ maximizes entropy $H[p] = -ğ”¼_x \log p(x)$?

~~~
For continuous values, the entropy is an integral $H[p] = -âˆ«p(x) \log p(x) \d x$.

~~~
We cannot just maximize $H$ with respect to a function $p$, because:
- the result might not be a probability distribution â€“ we need to add
  a constraint that $âˆ«p(x) \d x=1$;
~~~
- the problem is underspecified because a distribution can be shifted without
  changing entropy â€“ we add a constraint $ğ”¼[x] = Î¼$;
~~~
- because entropy increases as variance increases, we ask which distribution
  with a _fixed_ variance $Ïƒ^2$ has maximum entropy â€“ adding a constraint
  $\Var(x) = Ïƒ^2$.

---
class: dbend
# Function with Maximum Entropy

Lagrangian of all the constraints and the entropy function is
$$L(p; Î¼, Ïƒ^2) = Î»_1 \Big(âˆ«p(x) \d x - 1\Big) + Î»_2 \big(ğ”¼[x] - Î¼\big) + Î»_3\big(\Var(x) - Ïƒ^2\big) + H[p].$$

~~~
By expanding all definitions to integrals, we get
$$\begin{aligned}
L(p; Î¼, Ïƒ^2) =& âˆ«\Big(Î»_1 p(x) + Î»_2 p(x) x + Î»_3 p(x) (x - Î¼)^2 - p(x)\log p(x) \Big) \d x - \\
              & -Î»_1 - Î¼ Î»_2 - Ïƒ^2Î»_3.
\end{aligned}$$

~~~
The functional derivative of $L$ is:
$$\frac{âˆ‚}{âˆ‚p(x)} L(p; Î¼, Ïƒ^2) = Î»_1 + Î»_2 x + Î»_3 (x - Î¼)^2 - 1 - \log p(x) = 0.$$

---
class: dbend
# Function with Maximum Entropy
Rearrangint the functional derivative of $L$:

$$\frac{âˆ‚}{âˆ‚p(x)} L(p; Î¼, Ïƒ^2) = Î»_1 + Î»_2 x + Î»_3 (x - Î¼)^2 - 1 - \log p(x) = 0.$$

we obtain
$$p(x) = \exp\Big(Î»_1 + Î»_2 x + Î»_3 (x-Î¼)^2 - 1\Big).$$

~~~
We can verify that setting $Î»_1 = 1 - \log Ïƒ \sqrt{2Ï€}$, $Î»_2=0$ and $Î»_3=-1/(2Ïƒ^2)$
fulfils all the constraints, arriving at
$$p(x) = ğ“(x; Î¼, Ïƒ^2).$$

---
section: SoftMax
class: dbend
# Derivation of Softmax using Maximum Entropy

Let $ğ• = \{(â†’x_1, t_1), (â†’x_2, t_2), â€¦, (â†’x_N, t_N)\}$ be training data
of a $K$-class classification, with $â†’x_i âˆˆ â„^D$ and $t_i âˆˆ \{1, 2, â€¦, K\}$.

~~~
We want to model it using a function $Ï€: â„^D â†’ â„^K$
so that $Ï€(â†’x)$ gives a distribution of classes for input $â†’x$.

~~~
We impose the following conditions on $Ï€$:
- $$Ï€(â†’x)_j â‰¥ 0$$
~~~
- $$âˆ‘_{j=1}^K Ï€(â†’x)_j = 1$$
~~~
- $$âˆ€_{k âˆˆ \{1, 2, â€¦, D\}}, âˆ€_{j âˆˆ \{1, 2, â€¦, K\}}: âˆ‘_{i=1}^N Ï€(â†’x_i)_j x_{i,k} = âˆ‘_{i=1}^N \Big[t_i == j\Big] x_{i,k}$$

---
class: dbend
# Derivation of Softmax using Maximum Entropy

There are many such $Ï€$, one particularly bad is
$$Ï€(â†’x) = \begin{cases}
  t_i&\textrm{if there exists }i: â†’x_i = â†’x, \\
  0&\textrm{otherwise}.\end{cases}$$

~~~
Therefore, we want to find a more general $Ï€$ â€“ we will aim for one with maximum
entropy.

---
class: dbend
# Derivation of Softmax using Maximum Entropy

We therefore want to maximize $-âˆ‘_{i=1}^N âˆ‘_{j=1}^K Ï€(â†’x_i)_j \log(Ï€(â†’x_i)_j)$
given
- $Ï€(â†’x)_j â‰¥ 0$,
- $âˆ‘_{i=j}^K Ï€(â†’x)_j = 1$,
- $âˆ€_{k âˆˆ \{1, â€¦, D\}}, âˆ€_{j âˆˆ \{1, â€¦, K\}}: âˆ‘_{i=1}^N Ï€(â†’x_i)_j x_{i,k} = âˆ‘_{i=1}^N \big[t_i == j\big] x_{i,k}$.

~~~
We therefore form a Lagrangian (ignoring the first inequality constraint):
$$\begin{aligned}
L =& âˆ‘_{k=1}^D âˆ‘_{j=1}^K Î»_{k,j} \Big(âˆ‘_{i=1}^N Ï€(â†’x_i)_j x_{i,k} - \big[t_i == j\big] x_{i,k}\Big)\\
   & +âˆ‘_{i=1}^N Î²_i \Big(âˆ‘_{j=1}^K Ï€(â†’x_i)_j - 1\Big) \\
   & -âˆ‘_{i=1}^N âˆ‘_{j=1}^K Ï€(â†’x_i)_j \log(Ï€(â†’x_i)_j)
\end{aligned}$$

---
class: dbend
# Derivation of Softmax using Maximum Entropy

We now compute partial derivatives of the Lagrangian, notably the values
$$\frac{âˆ‚}{âˆ‚Ï€(â†’x_i)_j}L.$$

~~~
We arrive at
$$\frac{âˆ‚}{âˆ‚Ï€(â†’x_i)_j}L = â†’Î»_{*,j} â†’x_i + Î²_i - \log(Ï€(â†’x_i)_j) - 1.$$

~~~
Setting the Lagrangian to zero, we get $â†’Î»_{*,j} â†’x_i + Î²_i - \log(Ï€(â†’x_i)_j) - 1 = 0,$
which we rewrite to
$$Ï€(â†’x_i)_j = e^{â†’Î»_{*,j}â†’x_i + Î²_i - 1}.$$

~~~
Such a forms guarantees $Ï€(â†’x_i)_j > 0$, which we did not include in the
conditions.

---
class: dbend
# Derivation of Softmax using Maximum Entropy

In order to find out the $Î²_i$ values, we turn to the constraint
$$âˆ‘_j Ï€(â†’x_i)_j = âˆ‘_j e^{â†’Î»_{*,j}â†’x_i +Î²_i-1} = 1,$$
from which we get
$$e^{Î²_i} = \frac{1}{âˆ‘_j e^{â†’Î»_{*,j}â†’x_i-1}},$$

~~~
yielding
$$Ï€(â†’x_i)_j = \frac{e^{â†’Î»_{*,j}â†’x_i}}{âˆ‘_k e^{â†’Î»_{*,k}â†’x_i}}.$$

---
section: F-score
# F1-score

When evaluating binary classification, we have used **accuracy** so far.

~~~
![w=36%,f=right](true_false_positives.svgz)

However, there are other metric we might want to consider. One of them is
$F_1$-score.

~~~
| |Target<br>positive|Target<br>negative|
|-|------------------|------------------|
|Predicted<br>positive| **True Positive** (**TP**) | **False Positive** (**FP**) |
|Predicted<br>negative| **False Negative** (**FN**) | **True Negative** (**TN**) |

~~~
Accuracy can be computed as
$$\mathrm{accuracy} = \frac{\mathit{TP}+\mathit{TN}}{\mathit{TP}+\mathit{TN}+\mathit{FP}+\mathit{FN}}.$$

---
# F1-score

![w=50%,h=center,mw=36%,f=right](true_false_positives.svgz)

| |Target<br>positive|Target<br>negative|
|-|------------------|------------------|
|Predicted<br>positive| **True Positive** (**TP**) | **False Positive** (**FP**) |
|Predicted<br>negative| **False Negative** (**FN**) | **True Negative** (**TN**) |

~~~
![w=100%,h=center,mw=36%,f=right](precision_recall.svgz)

We define **precision** (percentage of correct predictions in the predicted examples)
and **recall** (percentage of correct predictions in the gold examples) as
$$\begin{aligned}
  \mathrm{precision} =& \frac{\mathit{TP}}{\mathit{TP}+\mathit{FP}}, \\
  \mathrm{recall} =& \frac{\mathit{TP}}{\mathit{TP}+\mathit{FN}}.
\end{aligned}$$

---
# F1-score

![w=47%,f=right](roc_pr.png)

The precision and recall go â€œagainst each otherâ€ in a sense.

~~~
We therefore define **$\boldsymbol{F_1}$-score** as a harmonic mean of precision and recall:
$$\begin{aligned}
  F_1 =& \frac{2}{\mathrm{precision}^{-1} + \mathrm{recall}^{-1}} \\
      =& \frac{2 â‹… \mathrm{precision} â‹… \mathrm{recall}}{\mathrm{precision} + \mathrm{recall}}.
\end{aligned}$$

~~~
The $F_1$ score can be generalized to $F_Î²$ score, where recall is $Î²$ times
more important than precision; $F_2$ favoring recall and $F_{0.5}$ favoring
precision are commonly used.

$$ F_Î² = \frac{(1 + Î²)^2 â‹… \mathrm{precision} â‹… \mathrm{recall}}{Î²^2 â‹… \mathrm{precision} + \mathrm{recall}}.$$

---
# Precision-Recall Curve, ROC Curve

<div style="width: 37%; float: right">
![w=100%](roc_pr.png)
![w=100%](roc_curve.svgz)
</div>

Changing the threshold in logistic regression allows us to trade off precision
for recall and vice versa. Therefore, we can tune it on the development set to
achieve highest possible $F_1$ score, if required.

~~~
Note that $F_1$ score does not consider true negatives at all.

Therefore, apart from the precision-recall curve, the **Receiver Operating
Characteristic** (ROC) curve is also used to describe binary classifiers. In the
ROC curve, we consider:
- _true positive rate_ (recall; probability of detection);
- _false positive rate_ (probability of false alarm).

~~~
When evaluating a binary classifier, the **area under curve** (AUC) is sometimes
also used as a metric.

---
section: (Non)ParametricMethods
# Parametric and Nonparametric Methods

All the machine learning models which we discussed so far are **parametric**,
because they use a _fixed_ number of parameters (usually depending on the
number of features, $K$ for multiclass classification, hidden layer in MLPS, â€¦).

~~~
However, there also exist **nonparametric** methods. Even if the name seems to
suggest they do not have any parameters, they have a non-fixed number of
parameters, because the number of parameters usually depend on the size of the
training data.

---
section: k-NN
# k-Nearest Neighbors

A simple but sometimes effective nonparametric method for both classification
and regression is **k-nearest neighbors** algorithm.

**Work in progress**

~~~
- $k=1$

~~~
- $k > 1$

~~~
- weighted variant

---
section: KernelLR
# Kernel Linear Regression

Consider linear regression with cubic features
$$Ï†(â†’x) = \scriptsize\begin{bmatrix} 1 \\ x_1 \\ x_2 \\ â€¦ \\ x_1^2 \\ x_1x_2 \\ â€¦ \\ x_2x_1 \\ â€¦ \\ x_1^3 \\ x_1^2x_2 \\ â€¦ \end{bmatrix}.$$

~~~
The SGD update for linear regression is then
$$â†’w â† â†’w + Î±\big(t - â†’w^T Ï†(â†’x)\big) Ï†(â†’x).$$

---
# Kernel Linear Regression

When dimensionality of input is $D$, one step of SGD takes $ğ“(D^3)$.

~~~
Surprisingly, we can do better under some circumstances. We start by
noting that we can write the parameters $â†’w$ as a linear combination
of the input features $Ï†(â†’x_i)$.

~~~
By induction, $â†’w = 0 = âˆ‘_i 0 â‹… Ï†(â†’x_i)$, and assuming $â†’w = âˆ‘_i Î²_i â‹… Ï†(â†’x_i)$,
after a SGD update we get
$$\begin{aligned}
â†’w â†& â†’w + Î±âˆ‘_i \big(t_i - â†’w^T Ï†(â†’x_i)\big) Ï†(â†’x_i)\\
   =& âˆ‘_i \Big(Î²_i + Î± \big(t_i - â†’w^T Ï†(â†’x_i)\big)\Big) Ï†(â†’x_i).
\end{aligned}$$

~~~
A individual update is $Î²_i â† Î²_i + Î±\Big(t_i - â†’w^T Ï†(â†’x_i)\Big)$, and
substituting for $â†’w$ we get
$$Î²_i â† Î²_i + Î±\Big(t_i - âˆ‘\nolimits_j Î²_j Ï†(â†’x_j)^T Ï†(â†’x_i)\Big).$$

---
# Kernel Linear Regression

We can formulate an alternative linear regression algorithm (a so-called
**dual formulation**):

<div class="algorithm">

**Input**: Dataset ($â‡‰X = \{â†’x_1, â†’x_2, â€¦, â†’x_N\} âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$), learning rate $Î± âˆˆ â„^+$.<br>

- Set $Î²_i â† 0$
- Compute all values $K(â†’x_i, â†’x_j) = Ï†(â†’x_i)^T Ï†(â†’x_j)$
- Repeat until convergence
  - Update the coordinates, either according to a full gradient update:
    - $â†’Î² â† â†’Î² + Î±(â†’t-Kâ†’Î²)$
  - or alternatively use single-batch SGD, arriving at:
    - for $i$ in random permutation of $\{1, â€¦, N\}$:
      - $Î²_i â† Î²_i + Î±\Big(t_i - âˆ‘\nolimits_j Î²_j K(â†’x_i, â†’x_j)\Big)$
</div>

~~~
The predictions are then performed by computing
$$y(â†’x) = â†’w^T Ï†(â†’x) = âˆ‘\nolimits_i Î²_i â†’Ï†(â†’x_i)^T â†’Ï†(â†’x).$$

---
section: Kernels
# Kernel Trick

A single SGD update of all $Î²_i$ then takes $ğ“(N^2)$, given that we can
evaluate scalar dot product of $Ï†(â†’x_j)^T Ï†(â†’x_i)$ quickly.

~~~
$$\begin{aligned}
Ï†(â†’x)^T Ï†(â†’z) =& 1 + âˆ‘_i x_i z_i + âˆ‘_{i,j} x_i x_j z_i z_j + âˆ‘_{i,j,k} x_i x_j x_k z_i z_j z_k \\
              =& 1 + âˆ‘_i x_i z_i + \Big(âˆ‘_i x_i z_i\Big)^2 + \Big(âˆ‘_i x_i z_i\Big)^3 \\
              =& 1 + â†’x^T â†’z + \big(â†’x^T â†’z\big)^2 + \big(â†’x^T â†’z\big)^3.
\end{aligned}$$

---
# Kernels

We define a _kernel_ corresponding to a feature map $Ï†$ as a function
$$K(â†’x, â†’z) â‰ Ï†(â†’x)^t Ï†(â†’z).$$

~~~
There is quite a lot of theory behind kernel construction. The most often used
kernels are:

~~~
- polynomial kernel or degree $d$
  $$K(â†’x, â†’z) = (Î³ â†’x^Tâ†’z)^d,$$
  which corresponds to a feature map generating all combinations of exactly
  $d$ input features;
~~~
- polynomial kernel or degree at most $d$
  $$K(â†’x, â†’z) = (Î³ â†’x^Tâ†’z + 1)^d,$$
  which corresponds to a feature map generating all combinations of up to
  $d$ input features;

---
# Kernels

- Gaussian (or RBF) kernel
  $$K(â†’x, â†’z) = e^{-Î³||â†’x-â†’z||^2},$$
  corresponding to a scalar product in an infinite-dimensional space (it is
  in a sense a combination of polynomial kernels of all degrees, details will
  appear later).
