title: NPFL129, Lecture 2
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Perceptron and Logistic Regression

## Milan Straka

### October 21, 2019

---
section: Perceptron
# Binary Classification

Binary classification is a classification in two classes.

~~~
To extend linear regression to binary classification, we might seek
a _threshold_ and the classify an input as negative/positive
depending whether $â†’x^Tâ†’w$ is smaller/larger than a given threshold.

~~~
Zero value is usually used as the threshold, both because it
is symmetric and also because the _bias_ parameter acts as a trainable threshold
anyway.

---
# Perceptron

The perceptron algorithm is probably the oldest one for training
weights of a binary classification. Assuming the target value
$t âˆˆ \{-1, +1\}$, the goal is to find weights $â†’w$ such that
for all train data
$$\operatorname{sign}(â†’w^T â†’x_i) = t_i,$$
or equivalently
$$t_i â†’w^T â†’x_i > 0.$$

~~~
Note that a set is called **linearly separable**, if there exist
a weight vector $â†’w$ such that the above equation holds.

---
# Perceptron

The perceptron algorithm was invented by Rosenblat in 1958.

<div class="algorithm">

**Input**: Linearly separable dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{-1, +1\}$).<br>
**Output**: Weights $â†’w âˆˆ â„^D$ such that $t_i â†’x_i^Tâ†’w > 0$ for all $i$.

- $â†’w â† 0$
- until all examples are classified correctly, process example $i$:
  - $y â† â†’w^Tâ†’x_i$
  - if $t_i y < 0$ (incorrectly classified example):
    - $â†’w â† â†’w + t_i â†’x_i$
</div>

~~~
We will prove that the algorithm always arrives at some correct set of
weights $â†’w$ if the training set is linearly separable.

---
# Perceptron as SGD

Consider the main part of the perceptron algorithm:

<div class="algorithm">

  - $y â† â†’w^Tâ†’x_i$
  - if $t_i y < 0$ (incorrectly classified example):
    - $â†’w â† â†’w + t_i â†’x_i$
</div>

~~~
We can derive the algorithm using on-line gradient descent, using
the following loss function
$$L(f(â†’x; â†’w), t) â‰ \begin{cases} -t â†’x^T â†’w & \textrm{if~}t â†’x^T â†’w < 0 \\ 0 & \textrm{otherwise}\end{cases}
  = \max(0, -tâ†’x^T â†’w) = \ReLU(-tâ†’x^T â†’w).$$

~~~
In this specific case, the value of the learning rate does not actually matter,
because multiplying $â†’w$ by a constant does not change a prediction.

---
# Proof of Perceptron Convergence

Let $â†’w_*$ be some weights separating the training data and let $â†’w_k$ be the
weights after $t$ non-trivial updates of the perceptron algorithm, with $â†’w_0$
being 0.

~~~
We will prove that the angle $Î±$ between $â†’w_*$ and $â†’w_k$ decreases at each step.
Note that
$$\cos(Î±) = \frac{â†’w_*^T â†’w_k}{||â†’w_*||â‹…||â†’w_k||}.$$

---
# Proof of Perceptron Convergence

Assume that the maximum norm of any training example $||â†’x||$ is bounded by $R$,
and that $Î³$ is the minimum margin of $â†’w_*$, so $t â†’w_* â†’x â‰¥ Î³.$

~~~
First consider the dot product of $â†’w_*$ and $â†’w_k$:
$$â†’w_*^T â†’w_k = â†’w_*^T (â†’w_{k-1} + t_k â†’x_k) â‰¥ â†’w_*^T â†’w_{k-1} + Î³.$$
By iteratively applying this equation, we get
$$â†’w_*^T â†’w_k â‰¥ kÎ³.$$

~~~
Now consider the length of $â†’w_k$:
$$\begin{aligned}
||â†’w_k||^2 &= ||â†’w_{k-1} + t_kâ†’x_k||^2 = ||â†’w_{k-1}||^2 + 2tâ†’w_{k-1}^Tâ†’x_k + ||â†’x_k||^2
\end{aligned}$$

~~~
Because $â†’x_k$ was misclassified, we know that $t â†’w_{k-1}^T â†’x_k < 0$, so
$||â†’w_k||^2 â‰¤ ||â†’w_{k-1}||^2 + R^2.$

---
# Proof of Perceptron Convergence

Putting everything together, we get
$$\cos(Î±) = \frac{â†’w_*^T â†’w_k}{||â†’w_*||â‹…||â†’w_k||} â‰¥ \frac{kÎ³}{\sqrt{kR^2}||â†’w_*||}.$$

~~~
Therefore, the $\cos(Î±)$ increases during every update. Because the value of
$\cos(Î±)$ is at most one, we can compute the upper bound on the number
of steps when the algorithm converges as
$$1 â‰¤ \frac{kÎ³}{\sqrt{kR^2}||â†’w_*||}\textrm{~or~}k â‰¥ \frac{R^2||â†’w_*||^2}{Î³^2}.$$

---
# Perceptron Issues

Perceptron has several drawbacks:
- If the input set is not linearly separable, the algorithm never finishes.
~~~
- The algorithm cannot be easily extended to classification into more than two
  classes.
~~~
- The algorithm performs only prediction, it is not able to return the
  probabilities of predictions.

---
section: Logistic Regression
# Logistic Regression

An extension of perceptron, which models the conditional probabilities
of $p(C_0|â†’x)$ and of $p(C_1|â†’x)$. Logistic regression can in fact
handle also more than two classes, but we will stay in binary classification
settings for now.

~~~
Logistic regression employs the following parametrization of the conditional
class probabilities:
$$\begin{aligned}
  P(C_1 | â†’x) &= Ïƒ(â†’x^t â†’w + â†’b) \\
  P(C_0 | â†’x) &= 1 - P(C_1 | â†’x),
\end{aligned}$$
where $Ïƒ$ is a _sigmoid function_
$$Ïƒ(x) = \frac{1}{1+e^{-x}}.$$

~~~
Can be trained using an SGD algorithm.

---
# Logistic Regression

To give some meaning to the sigmoid function, starting with
$$P(C_1 | â†’x) = Ïƒ(f(â†’x; â†’w))$$
we can arrive at
$$f(â†’x; â†’w) = \log\left(\frac{P(C_1 | â†’x)}{P(C_0 | â†’x)}\right),$$
where the prediction of the model $f(â†’x; â†’w)$ is called a _logit_
and it is a logarithm of odds of the two classes probabilities.

---
section: MLE
# Maximum Likelihood Estimation

Let $ğ• = \{(â†’x_1, t_1), (â†’x_2, t_2), â€¦, (â†’x_N, t_N)\}$ be training data drawn
independently from the data-generating distribution $p_\textrm{data}$. We denote
the empirical data distribution as $pÌ‚_\textrm{data}$.

~~~
Let $p_\textrm{model}(t | â†’x; â†’w)$ be a family of distributions.

~~~
The *maximum likelihood estimation* of $â†’w$ is:

$$\begin{aligned}
â†’w_\mathrm{ML} &= \argmax_â†’w p_\textrm{model}(\mathbb X; â†’w) \\
               &= \argmax_â†’w âˆ\nolimits_{i=1}^N p_\textrm{model}(t_i | â†’x_i; â†’w) \\
               &= \argmin_â†’w âˆ‘\nolimits_{i=1}^N -\log p_\textrm{model}(t_i | â†’x_i; â†’w) \\
               &= \argmin_â†’w ğ”¼_{â‡â†’x âˆ¼ pÌ‚_\textrm{data}} [-\log p_\textrm{model}(t | â†’x; â†’w)] \\
\end{aligned}$$

---
# Properties of Maximum Likelihood Estimation

Assume that the true data generating distribution $p_\textrm{data}$ lies within the model
family $p_\textrm{model}(â‹…; â†’w)$, and assume there exists a unique
$â†’w_{p_\textrm{data}}$ such that $p_\textrm{data} = p_\textrm{model}(â‹…; â†’w_{p_\textrm{data}})$.

~~~
- MLE is a _consistent_ estimator. If we denote $â†’w_m$ to be the parameters
  found by MLE for a training set with $m$ examples generated by the data
  generating distribution, then $â†’w_m$ converges in probability to
  $â†’w_{p_\textrm{data}}$.

  Formally, for any $Îµ > 0$, $P(||â†’w_m - â†’w_{p_\textrm{data}}|| > Îµ) â†’ 0$
  as $m â†’ âˆ$.

~~~
- MLE is in a sense most _statistic efficient_. For any consistent estimator, we
  might consider the average distance of $â†’w_m$ and $â†’w_{p_\textrm{data}}$,
  formally $ğ”¼_{â‡â†’x_1, \ldots, â‡â†’x_m âˆ¼ p_\textrm{data}} [||â†’w_m - â†’w_{p_\textrm{data}}||_2^2]$.
  It can be shown (Rao 1945, CramÃ©r 1946) that no consistent estimator has
  lower mean squared error than the maximum likelihood estimator.

~~~
Therefore, for reasons of consistency and efficiency, maximum likelihood is
often considered the preferred estimator for machine learning.

---
section: LR
# Logistic Regression
<div class="algorithm">

**Input**: Input dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{0, +1\}$), learning rate $Î± âˆˆ â„^+$.<br>

- $â†’w â† 0$
- until convergence (or until patience is over), process example $i$:
  - $â†’w â† â†’w + Î±âˆ‡_â†’w \log p(t_i|â†’x_i; â†’w)$
</div>

---
# Common Probability Distributions
## Bernoulli Distribution
The Bernoulli distribution is a distribution over a binary random variable.
It has a single parameter $Ï† âˆˆ [0, 1]$, which specifies the probability of the random
variable being equal to 1.

~~~
$$\begin{aligned}
  P(x) &= Ï†^x (1-Ï†)^{1-x} \\
  ğ”¼[x] &= Ï† \\
  \Var(x) &= Ï†(1-Ï†)
\end{aligned}$$

~~~
## Categorical Distribution
Extension of the Bernoulli distribution to random variables taking one of $k$ different
discrete outcomes. It is parametrized by $â†’p âˆˆ [0, 1]^k$ such that $âˆ‘_{i=1}^{k} p_{i} = 1$.
$$\begin{aligned}
  P(â†’x) &= âˆ\nolimits_i^k p_i^{x_i} \\
  ğ”¼[x_i] &= p_i, \Var(x_i) = p_i(1-p_i) \\
\end{aligned}$$

---
# Information Theory

## Self Information

Amount of _surprise_ when a random variable is sampled.
~~~
- Should be zero for events with probability 1.
~~~
- Less likely events are more surprising.
~~~
- Independent events should have _additive_ information.

~~~
$$I(x) â‰ -\log P(x) = \log \frac{1}{P(x)}$$

~~~
## Entropy

Amount of _surprise_ in the whole distribution.
$$H(P) â‰ ğ”¼_{â‡xâˆ¼P}[I(x)] = -ğ”¼_{â‡xâˆ¼P}[\log P(x)]$$

~~~
- for discrete $P$: $H(P) = -âˆ‘_x P(x) \log P(x)$
- for continuous $P$: $H(P) = -âˆ« P(x) \log P(x)\,\mathrm dx$

---
# Information Theory

## Cross-Entropy

$$H(P, Q) â‰ -ğ”¼_{â‡xâˆ¼P}[\log Q(x)]$$

~~~
- Gibbs inequality
    - $H(P, Q) â‰¥ H(P)$
    - $H(P) = H(P, Q) â‡” P = Q$
~~~
    - Proof: Using Jensen's inequality, we get
      $$âˆ‘_x P(x) \log \frac{Q(x)}{P(x)} â‰¤ \log âˆ‘_x P(x) \frac{Q(x)}{P(x)} = \log âˆ‘_x Q(x) = 0.$$
~~~
    - Corollary: For a categorical distribution with $n$ outcomes, $H(P) â‰¤ \log n$,
    because for $Q(x) = 1/n$ we get $H(P) â‰¤ H(P, Q) = -âˆ‘_x P(x) \log Q(x) = \log n.$
~~~
- generally $H(P, Q) â‰  H(Q, P)$

---
# Information Theory

## Kullback-Leibler Divergence (KL Divergence)

Sometimes also called _relative entropy_.

$$D_\textrm{KL}(P || Q) â‰ H(P, Q) - H(P) = ğ”¼_{â‡xâˆ¼P}[\log P(x) - \log Q(x)]$$

~~~
- consequence of Gibbs inequality: $D_\textrm{KL}(P || Q) â‰¥ 0$
- generally $D_\textrm{KL}(P || Q) â‰  D_\textrm{KL}(Q || P)$

---
# Nonsymmetry of KL Divergence

![w=100%,v=middle](kl_nonsymmetry.pdf)

---
# Common Probability Distributions
## Normal (or Gaussian) Distribution
Distribution over real numbers, parametrized by a mean $Î¼$ and variance $Ïƒ^2$:
$$ğ“(x; Î¼, Ïƒ^2) = \sqrt{\frac{1}{2Ï€Ïƒ^2}} \exp \left(-\frac{(x - Î¼)^2}{2Ïƒ^2}\right)$$

For standard values $Î¼=0$ and $Ïƒ^2=1$ we get $ğ“(x; 0, 1) = \sqrt{\frac{1}{2Ï€}} e^{-\frac{x^2}{2}}$.

![w=45%,h=center](normal_distribution.pdf)

---
# Why Normal Distribution

## Central Limit Theorem
The sum of independent identically distributed random variables
with finite variance converges to normal distribution.

~~~
## Principle of Maximum Entropy
Given a set of constraints, a distribution with maximal entropy fulfilling the
constraints can be considered the most general one, containing as little
additional assumptions as possible.

~~~
Considering distributions with a given mean and variance, it can be proven
(using variational inference) that such a distribution with _maximal entropy_
is exactly the normal distribution.
