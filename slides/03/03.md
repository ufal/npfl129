title: NPFL129, Lecture 3
class: title, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Perceptron and Logistic Regression

## Jindřich Libovický <small>(reusing materials by Milan Straka)</small>

### October 14, 2024


---
class: middle
# Today's Lecture Objectives

After this lecture you should be able to

- Think about binary classification using **geometric intuition** and use the
  **perceptron algorithm**.

- Define the **main concepts of information theory** (entropy, cross-entropy,
  KL-divergence) and prove their properties.

- Derive training objectives using the **maximum likelihood principle**.

- Implement and use **logistic regression** for binary classification with SGD.

---
section: Perceptron
class: section
# Perceptron

---
# Binary Classification

Binary classification is a classification in two classes.

~~~
The simplest way to evaluate classification is **accuracy**, which is
the ratio of input examples that were classified correctly – i.e.,
where the predicted class and the target class match.

~~~
To extend linear regression to binary classification, we might seek
a **threshold** and then classify an input as negative/positive
depending on whether $y(→x; →w) = →x^T→w + b$ is smaller/larger than a given threshold.

~~~
Zero value is usually used as the threshold, both because of symmetry and also
because the **bias** parameter acts as a trainable threshold anyway.

~~~
The set of points with prediction 0 is called a **decision boundary**.

---
# Geometric Intuition

![w=50%,h=center](binary_classification.svgz)

---
# Perceptron

The perceptron algorithm is probably the oldest one for training
weights of a binary classification. Assuming the target value
$t ∈ \{-1, +1\}$, the goal is to find weights $→w$ such that
for all train data,
$$\sign(y(→x_i; →w)) = \sign(→x_i^T →w) = t_i,$$
or equivalently,
$$t_i y(→x_i; →w) = t_i →x_i^T →w > 0.$$

~~~

![w=60%,mw=35%,h=center,f=right](linearly_separable.svgz)

Note that a set is called **linearly separable**, if there exists
a weight vector $→w$ such that the above equation holds.

---
# Perceptron

The perceptron algorithm was invented by Rosenblatt in 1958.

<div class="algorithm">

**Input**: Linearly separable dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ \{-1, +1\}^N$).<br>
**Output**: Weights $→w ∈ ℝ^D$ such that $t_i →x_i^T→w > 0$ for all $i$.

- $→w ← →0$
- until all examples are classified correctly, process example $i$:
  - $y ← →x_i^T →w$
  - if $t_i y ≤ 0$ (incorrectly classified example):
    - $→w ← →w + t_i →x_i$
</div>

~~~
We will prove that the algorithm always arrives at some correct set of
weights $→w$ if the training set is linearly separable.

---
class: dbend
# Proof of Perceptron Convergence

Let $→w_*$ be some weights correctly classifying (separating) the training data,
and let $→w_k$ be the weights after $k$ nontrivial updates of the perceptron
algorithm, with $→w_0$ being 0.

~~~
We will prove that the angle $α$ between $→w_*$ and $→w_k$ decreases at each step.
Note that
$$\cos(α) = \frac{→w_*^T →w_k}{\|→w_*\|⋅\|→w_k\|}.$$

---
class: dbend
# Proof of Perceptron Convergence

Assume that the maximum norm of any training example $\|→x\|$ is bounded by $R$,
and that $γ$ is the minimum margin of $→w_*$, so for each training example $(→x, t)$, $t →x^T →w_* ≥ γ.$

~~~
First consider the dot product of $→w_*$ and $→w_k$:
$$→w_*^T →w_k = →w_*^T (→w_{k-1} + t_k →x_k) ≥ →w_*^T →w_{k-1} + γ.$$
~~~
By iteratively applying this equation, we get
$$→w_*^T →w_k ≥ kγ.$$

~~~
Now consider the length of $→w_k$:
$$\begin{aligned}
\|→w_k\|^2 &= \|→w_{k-1} + t_k→x_k\|^2 = \|→w_{k-1}\|^2 + 2 t_k →x_k^T →w_{k-1} + \|→x_k\|^2.
\end{aligned}$$

~~~
Because $→x_k$ was misclassified, we know that $t_k →x_k^T →w_{k-1} ≤ 0$, so
$\|→w_k\|^2 ≤ \|→w_{k-1}\|^2 + R^2.$
~~~
When applied iteratively, we get $\|→w_k\|^2 ≤ k ⋅ R^2$.

---
class: dbend
# Proof of Perceptron Convergence

Putting everything together, we get
$$\cos(α) = \frac{→w_*^T →w_k}{\|→w_*\|⋅\|→w_k\|} ≥ \frac{kγ}{\sqrt{kR^2}\|→w_*\|}.$$

~~~
Therefore, the $\cos(α)$ increases during every update. Because the value of
$\cos(α)$ is at most one, we can compute the upper bound on the number
of steps when the algorithm converges as
$$1 ≥ \frac{\sqrt{k}γ}{\sqrt{R^2}\|→w_*\|}\textrm{~or~}k ≤ \frac{R^2\|→w_*\|^2}{γ^2}.$$

---
# Perceptron Issues

Perceptron has several drawbacks:
- If the input set is not linearly separable, the algorithm never finishes.

~~~
- The algorithm performs only prediction, it is not able to return the
  probabilities of predictions.
~~~
- Most importantly, Perceptron algorithm finds _some_ solution, not necessarily
  a good one, because once it finds some, it cannot perform any more updates.

![w=50%,h=center](perceptron_suboptimal.svgz)

---
section: ProbabilityBasics
class: section
# Basics of Probability

---
# Common Probability Distributions

## Bernoulli Distribution
The Bernoulli distribution is a distribution over a binary random variable.
It has a single parameter $φ ∈ [0, 1]$, which specifies the probability that
the random variable is equal to 1.

~~~
$$\begin{aligned}
  P(x) &= φ^x (1-φ)^{1-x} \\
  𝔼[x] &= φ \\
  \Var(x) &= φ(1-φ)
\end{aligned}$$

![w=60%,h=center](bernoulli_variance.svgz)

---
# Common Probability Distributions

## Categorical Distribution
Extension of the Bernoulli distribution to random variables taking one of $K$ different
discrete outcomes. It is parametrized by $→p ∈ [0, 1]^K$ such that $∑_{i=0}^{K-1} p_{i} = 1$.

~~~
We represent outcomes as vectors $∈ \{0, 1\}^K$ in **one-hot encoding**.
Therefore, an outcome $x ∈ \{0, 1, …, K-1\}$ is represented as a vector
$$→1_x ≝ \big([i = x]\big)_{i=0}^{K-1} = \big(\underbrace{0, …, 0}_{x}, 1, \underbrace{0, …, 0}_{K-x-1}\big).$$

~~~
The outcome probability, mean, and variance are very similar to the Bernoulli
distribution.
$$\begin{aligned}
  P(→x) &= ∏\nolimits_{i=0}^{K-1} p_i^{x_i} \\
  𝔼[x_i] &= p_i \\
  \Var(x_i) &= p_i(1-p_i) \\
\end{aligned}$$

---
section: InformationTheory
class: section
# Information Theory
---

# Information Theory

## Self Information

Amount of **surprise** when a random variable is sampled.
~~~
- Should be zero for events with probability 1.
~~~
- Less likely events are more surprising.
~~~
- Independent events should have **additive** information.

~~~
$$I(x) ≝ -\log P(x) = \log \frac{1}{P(x)}$$

---
# Information Theory

## Entropy

Amount of **surprise** in the whole distribution.
$$H(P) ≝ 𝔼_{⁇x∼P}[I(x)] = -𝔼_{⁇x∼P}[\log P(x)]$$

~~~
- for discrete $P$: $H(P) = -∑_x P(x) \log P(x)$
- for continuous $P$: $H(P) = -∫ P(x) \log P(x)\,\mathrm dx$

~~~
Because $\lim_{x → 0} x \log x = 0$, for $P(x) = 0$ we consider\
$P(x) \log P(x)$ to be zero.

~~~ ~~~
![w=40%,f=right](entropy_example.svgz)

- for discrete $P$: $H(P) = -∑_x P(x) \log P(x)$
- for continuous $P$: $H(P) = -∫ P(x) \log P(x)\,\mathrm dx$

Because $\lim_{x → 0} x \log x = 0$, for $P(x) = 0$ we consider
$P(x) \log P(x)$ to be zero.

~~~
Note that in the continuous case, the continuous entropy (also called
_differential entropy_) has slightly different semantics, for example, it can be
negative.

~~~
For binary logarithms, the entropy is measured in **bits**. However,
from now on, all logarithms are _natural logarithms_ with base _e_
(and then the entropy is measured in units called **nats**).

---
# Information Theory

## Cross-Entropy

$$H(P, Q) ≝ -𝔼_{⁇x∼P}[\log Q(x)]$$

~~~
### Gibbs Inequality
- $H(P, Q) ≥ H(P)$
~~~
- $H(P) = H(P, Q) ⇔ P = Q$
~~~

Proof: Consider $H(P) - H(P, Q) = ∑_x P(x) \log \frac{Q(x)}{P(x)}.$

~~~
Using the fact that $\log x ≤ (x-1)$ with equality only for $x=1$, we get
$$∑_x P(x) \log \frac{Q(x)}{P(x)} ≤ ∑_x P(x) \left(\frac{Q(x)}{P(x)}-1\right) = ∑_x Q(x) - ∑_x P(x) = 0.$$

~~~
For the equality to hold, $\frac{Q(x)}{P(x)}$ must be 1 for all $x$, i.e., $P=Q$.

---
# Information Theory

## Kullback-Leibler Divergence (KL Divergence)

Sometimes also called **relative entropy**.

$$D_\textrm{KL}(P \| Q) ≝ H(P, Q) - H(P) = 𝔼_{⁇x∼P}[\log P(x) - \log Q(x)]$$

~~~
- consequence of Gibbs inequality: $D_\textrm{KL}(P \| Q) ≥ 0$, $D_\textrm{KL}(P \| Q) = 0$ iff $P = Q$
~~~
- generally $D_\textrm{KL}(P \| Q) ≠ D_\textrm{KL}(Q \| P)$

---
# Common Probability Distributions
## Normal (or Gaussian) Distribution
A distribution over real numbers, parametrized by mean $μ$ and variance $σ^2$:
$$𝓝(x; μ, σ^2) = \sqrt{\frac{1}{2πσ^2}} \exp \left(-\frac{(x - μ)^2}{2σ^2}\right)$$

~~~
For standard values $μ=0$ and $σ^2=1$ we get $𝓝(x; 0, 1) = \sqrt{\frac{1}{2π}} e^{-\frac{x^2}{2}}$.

![w=45%,h=center](normal_distribution.svgz)

---
# Why Normal Distribution

## Central Limit Theorem
The sum of independent identically distributed random variables
with finite non-zero variance converges to normal distribution.

~~~
## Principle of Maximum Entropy
Given a set of constraints, a distribution with maximal entropy fulfilling the
constraints can be considered the most general one, containing as little
additional assumptions as possible.

~~~
Considering distributions with a **given mean and variance**, it can be proven
(using variational inference) that such a distribution with **maximum entropy**
is exactly the normal distribution.

---
section: MLE
class: section
# Maximum Likelihood Estimation
---

# Maximum Likelihood Estimation

Let $⇉X = \{→x_1, →x_2, …, →x_N\}$ be training data drawn
independently from the data-generating distribution $p_\textrm{data}$.

~~~
We denote the **empirical data distribution** as $p̂_\textrm{data}$, where
$$p̂_\textrm{data}(→x) ≝ \frac{\big|\{i: →x_i = →x\}\big|}{N}.$$

~~~
Let $p_\textrm{model}(⁇→x; →w)$ be a family of distributions.
~~~
- If the weights are fixed, $p_\textrm{model}(⁇→x{\color{lightgray}; →w})$ is a probability distribution.
~~~
- If we instead consider the training data $⇉X$ fixed, then
  $$L(→w) = p_\textrm{model}(⇉X; →w) = ∏\nolimits_{i=1}^N p_\textrm{model}(→x_i; →w)$$
  is called the **likelihood**.
~~~
  Note that even if the value of the likelihood is in range $[0, 1]$, it is not
  a probability, because the likelihood is not a probability distribution.

---
# Maximum Likelihood Estimation

Let $⇉X = \{→x_1, →x_2, …, →x_N\}$ be training data drawn
independently from the data-generating distribution $p_\textrm{data}$. We denote
the empirical data distribution as $p̂_\textrm{data}$ and let
$p_\textrm{model}(⁇→x; →w)$ be a family of distributions.

The **maximum likelihood estimation** of $→w$ is:

$\displaystyle \kern8em\mathllap{→w_\mathrm{MLE}} = \argmax_{→w} p_\textrm{model}(⇉X; →w) = \argmax_{→w} ∏\nolimits_{i=1}^N p_\textrm{model}(→x_i; →w)$

~~~
$\displaystyle \kern8em{} = \argmin_{→w} ∑\nolimits_{i=1}^N -\log p_\textrm{model}(→x_i; →w)$

~~~
$\displaystyle \kern8em{} = \argmin_{→w} 𝔼_{⁇→x ∼ p̂_\textrm{data}} [-\log p_\textrm{model}(→x; →w)]$

~~~
$\displaystyle \kern8em{} = \argmin_{→w} H(p̂_\textrm{data}(⁇→x), p_\textrm{model}(⁇→x; →w))$

~~~
$\displaystyle \kern8em{} = \argmin_{→w} D_\textrm{KL}(p̂_\textrm{data}(⁇→x)\|p_\textrm{model}(⁇→x; →w)) \color{gray} + H(p̂_\textrm{data}(⁇→x))$

---
style: .katex-display { margin: .7em 0 }
# Maximum Likelihood Estimation

MLE can be easily generalized to the conditional case, where our goal is to predict $t$ given $→x$:
$$\begin{aligned}
→w_\mathrm{MLE} &= \argmax_{→w} p_\textrm{model}(→t | ⇉X; →w) = \argmax_{→w} ∏\nolimits_{i=1}^N p_\textrm{model}(t_i | →x_i; →w) \\
                &= \argmin_{→w} ∑\nolimits_{i=1}^N -\log p_\textrm{model}(t_i | →x_i; →w) \\
                &= \argmin_{→w} 𝔼_{(⁇→x, ⁇t) ∼ p̂_\textrm{data}} [-\log p_\textrm{model}(t | →x; →w)] \\
                &= \argmin_{→w} H(p̂_\textrm{data}(⁇t | ⁇→x), p_\textrm{model}(⁇t | ⁇→x; →w)) \\
                &= \argmin_{→w} D_\textrm{KL}(p̂_\textrm{data}(⁇t | ⁇→x)\|p_\textrm{model}(⁇t | ⁇→x; →w)) \color{gray} + H(p̂_\textrm{data}(⁇t | ⁇→x))
\end{aligned}$$

~~~
where the conditional entropy is defined as
$H(p̂_\textrm{data}) = 𝔼_{(⁇→x, ⁇t) ∼ p̂_\textrm{data}} [-\log (p̂_\textrm{data}(t | →x; →w))]$
and the conditional cross-entropy as
$H(p̂_\textrm{data}, p_\textrm{model}) = 𝔼_{(⁇→x, ⁇t) ∼ p̂_\textrm{data}} [-\log (p_\textrm{model}(t | →x; →w))]$.

~~~
The resulting _loss function_ is called **negative log-likelihood** (**NLL**), or
**cross-entropy**, or **Kullback-Leibler divergence**.

---
section: LogisticRegression
class: section
# Logistic Regression

---
# Logistic Regression

An extension of perceptron, which models the conditional probabilities
of $p(C_0|→x)$ and of $p(C_1|→x)$. Logistic regression can in fact
handle also more than two classes, which we will see in the next lecture.

~~~
Logistic regression employs the following parametrization of the conditional
class probabilities:
$$\begin{aligned}
  p(C_1 | →x) &= σ(→x^T →w + b) \\
  p(C_0 | →x) &= 1 - p(C_1 | →x),
\end{aligned}$$
where $σ$ is a **sigmoid function**
$$σ(x) = \frac{1}{1+e^{-x}}.$$

~~~
It can be trained using the SGD algorithm.

---
# Sigmoid Function

The sigmoid function has values in range $(0, 1)$, is monotonically
increasing and it has a derivative of $\frac{1}{4}$ at $x=0$.

$$σ(x) = \frac{1}{1+e^{-x}}$$

~~~
$$σ'(x) = σ(x) \big(1 - σ(x)\big)$$

~~~
![w=100%](sigmoid.svgz)

---
# Logistic Regression

We denote the output of the “linear part” of logistic regression as
$$ȳ(→x; →w) = →x^T →w,$$
and the overall prediction as
$$y(→x; →w) = σ(ȳ(→x; →w)) = σ(→x^T →w).$$

---
# Logistic Regression

To train logistic regression, we use MLE (the maximum likelihood
estimation). Its application is straightforward, given that $p(C_1 | →x; →w)$ is
directly the model output $y(→x; →w)$.

~~~
Therefore, the loss for a minibatch $𝕏=\{(→x_1, t_1), (→x_2, t_2), …, (→x_N, t_N)\}$
is
$$\begin{aligned}
E(→w) = \frac{1}{N} ∑_i -\log(p(C_{t_i} | →x_i; →w)). \\
\end{aligned}$$

~~~
<div class="algorithm">

**Input**: Input dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ \{0, +1\}^N$), learning rate $α ∈ ℝ^+$.<br>

- $→w ← →0$ or we initialize $→w$ randomly
- until convergence (or patience runs out), process a minibatch of examples $𝔹$:
  - $→g ← \tfrac{1}{|𝔹|} ∑_{i∈𝔹} ∇_{→w} \Big(-\log\big(p(C_{t_i} | →x_i; →w)\big)\Big)$
  - $→w ← →w - α→g$
</div>


---
class: middle
# Practical note

Everything we learned about **features** and **$L^2$-regularization** holds for
logistic regression too.

<center><big>😮👍</center</big>

---
class: summary
# Today's Lecture Objectives

After this lecture you should be able to

- Think about binary classification using **geometric intuition** and use the
  **perceptron algorithm**.

- Define the **main concepts of information theory** (entropy, cross-entropy,
  KL-divergence) and prove their properties.

- Derive training objectives using the **maximum likelihood principle**.

- Implement and use **logistic regression** for binary classification with SGD.
