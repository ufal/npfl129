title: NPFL129, Lecture 3
class: title, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Perceptron and Logistic Regression

## JindÅ™ich LibovickÃ½ <small>(reusing materials by Milan Straka)</small>

### October 13, 2025


---
class: middle
# Today's Lecture Objectives

After this lecture you should be able to

- Think about binary classification using **geometric intuition** and use the
  **perceptron algorithm**.

- Define the **main concepts of information theory** (entropy, cross-entropy,
  KL-divergence) and prove their properties.

- Derive training objectives using the **maximum likelihood principle**.

- Implement and use **logistic regression** for binary classification with SGD.

---
section: Perceptron
class: section
# Perceptron

---
# Binary Classification

Binary classification is a classification in two classes.

~~~
The simplest way to evaluate classification is **accuracy**, which is
the ratio of input examples that were classified correctly â€“ i.e.,
where the predicted class and the target class match.

~~~
To extend linear regression to binary classification, we might seek
a **threshold** and then classify an input as negative/positive
depending on whether $y(â†’x; â†’w) = â†’x^Tâ†’w + b$ is smaller/larger than a given threshold.

~~~
Zero value is usually used as the threshold, both because of symmetry and also
because the **bias** parameter acts as a trainable threshold anyway.

~~~
The set of points with prediction 0 is called a **decision boundary**.

---
# Geometric Intuition

![w=50%,h=center](binary_classification.svgz)

---
# Perceptron

The perceptron algorithm is probably the oldest one for training
weights of a binary classification. Assuming the target value
$t âˆˆ \{-1, +1\}$, the goal is to find weights $â†’w$ such that
for all train data,
$$\sign(y(â†’x_i; â†’w)) = \sign(â†’x_i^T â†’w) = t_i,$$
or equivalently,
$$t_i y(â†’x_i; â†’w) = t_i â†’x_i^T â†’w > 0.$$

~~~

![w=60%,mw=35%,h=center,f=right](linearly_separable.svgz)

Note that a set is called **linearly separable**, if there exists
a weight vector $â†’w$ such that the above equation holds.

---
# Perceptron

The perceptron algorithm was invented by Rosenblatt in 1958.

<div class="algorithm">

**Input**: Linearly separable dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{-1, +1\}^N$).<br>
**Output**: Weights $â†’w âˆˆ â„^D$ such that $t_i â†’x_i^Tâ†’w > 0$ for all $i$.

- $â†’w â† â†’0$
- until all examples are classified correctly, process example $i$:
  - $y â† â†’x_i^T â†’w$
  - if $t_i y â‰¤ 0$ (incorrectly classified example):
    - $â†’w â† â†’w + t_i â†’x_i$
</div>

~~~
We will prove that the algorithm always arrives at some correct set of
weights $â†’w$ if the training set is linearly separable.

---
class: dbend
# Proof of Perceptron Convergence

Let $â†’w_*$ be some weights correctly classifying (separating) the training data,
and let $â†’w_k$ be the weights after $k$ nontrivial updates of the perceptron
algorithm, with $â†’w_0$ being 0.

~~~
We will prove that the angle $Î±$ between $â†’w_*$ and $â†’w_k$ decreases at each step.
Note that
$$\cos(Î±) = \frac{â†’w_*^T â†’w_k}{\|â†’w_*\|â‹…\|â†’w_k\|}.$$

---
class: dbend
# Proof of Perceptron Convergence

Assume that the maximum norm of any training example $\|â†’x\|$ is bounded by $R$,
and that $Î³$ is the minimum margin of $â†’w_*$, so for each training example $(â†’x, t)$, $t â†’x^T â†’w_* â‰¥ Î³.$

~~~
First consider the dot product of $â†’w_*$ and $â†’w_k$:
$$â†’w_*^T â†’w_k = â†’w_*^T (â†’w_{k-1} + t_k â†’x_k) â‰¥ â†’w_*^T â†’w_{k-1} + Î³.$$
~~~
By iteratively applying this equation, we get
$$â†’w_*^T â†’w_k â‰¥ kÎ³.$$

~~~
Now consider the length of $â†’w_k$:
$$\begin{aligned}
\|â†’w_k\|^2 &= \|â†’w_{k-1} + t_kâ†’x_k\|^2 = \|â†’w_{k-1}\|^2 + 2 t_k â†’x_k^T â†’w_{k-1} + \|â†’x_k\|^2.
\end{aligned}$$

~~~
Because $â†’x_k$ was misclassified, we know that $t_k â†’x_k^T â†’w_{k-1} â‰¤ 0$, so
$\|â†’w_k\|^2 â‰¤ \|â†’w_{k-1}\|^2 + R^2.$
~~~
When applied iteratively, we get $\|â†’w_k\|^2 â‰¤ k â‹… R^2$.

---
class: dbend
# Proof of Perceptron Convergence

Putting everything together, we get
$$\cos(Î±) = \frac{â†’w_*^T â†’w_k}{\|â†’w_*\|â‹…\|â†’w_k\|} â‰¥ \frac{kÎ³}{\sqrt{kR^2}\|â†’w_*\|}.$$

~~~
Therefore, the $\cos(Î±)$ increases during every update. Because the value of
$\cos(Î±)$ is at most one, we can compute the upper bound on the number
of steps when the algorithm converges as
$$1 â‰¥ \frac{\sqrt{k}Î³}{\sqrt{R^2}\|â†’w_*\|}\textrm{~or~}k â‰¤ \frac{R^2\|â†’w_*\|^2}{Î³^2}.$$

---
# Perceptron Issues

Perceptron has several drawbacks:
- If the input set is not linearly separable, the algorithm never finishes.

~~~
- The algorithm performs only prediction, it is not able to return the
  probabilities of predictions.
~~~
- Most importantly, Perceptron algorithm finds _some_ solution, not necessarily
  a good one, because once it finds some, it cannot perform any more updates.

![w=50%,h=center](perceptron_suboptimal.svgz)

---
section: ProbabilityBasics
class: section
# Basics of Probability

---
# Common Probability Distributions

## Bernoulli Distribution
The Bernoulli distribution is a distribution over a binary random variable.
It has a single parameter $Ï† âˆˆ [0, 1]$, which specifies the probability that
the random variable is equal to 1.

~~~
$$\begin{aligned}
  P(x) &= Ï†^x (1-Ï†)^{1-x} \\
  ğ”¼[x] &= Ï† \\
  \Var(x) &= Ï†(1-Ï†)
\end{aligned}$$

![w=60%,h=center](bernoulli_variance.svgz)

---
# Common Probability Distributions

## Categorical Distribution
Extension of the Bernoulli distribution to random variables taking one of $K$ different
discrete outcomes. It is parametrized by $â†’p âˆˆ [0, 1]^K$ such that $âˆ‘_{i=0}^{K-1} p_{i} = 1$.

~~~
We represent outcomes as vectors $âˆˆ \{0, 1\}^K$ in **one-hot encoding**.
Therefore, an outcome $x âˆˆ \{0, 1, â€¦, K-1\}$ is represented as a vector
$$â†’1_x â‰ \big([i = x]\big)_{i=0}^{K-1} = \big(\underbrace{0, â€¦, 0}_{x}, 1, \underbrace{0, â€¦, 0}_{K-x-1}\big).$$

~~~
The outcome probability, mean, and variance are very similar to the Bernoulli
distribution.
$$\begin{aligned}
  P(â†’x) &= âˆ\nolimits_{i=0}^{K-1} p_i^{x_i} \\
  ğ”¼[x_i] &= p_i \\
  \Var(x_i) &= p_i(1-p_i) \\
\end{aligned}$$

---
section: InformationTheory
class: section
# Information Theory
---

# Information Theory

## Self Information

Amount of **surprise** when a random variable is sampled.
~~~
- Should be zero for events with probability 1.
~~~
- Less likely events are more surprising.
~~~
- Independent events should have **additive** information.

~~~
$$I(x) â‰ -\log P(x) = \log \frac{1}{P(x)}$$

---
# Information Theory

## Entropy

Amount of **surprise** in the whole distribution.
$$H(P) â‰ ğ”¼_{â‡xâˆ¼P}[I(x)] = -ğ”¼_{â‡xâˆ¼P}[\log P(x)]$$

~~~
- for discrete $P$: $H(P) = -âˆ‘_x P(x) \log P(x)$
- for continuous $P$: $H(P) = -âˆ« P(x) \log P(x)\,\mathrm dx$

~~~
Because $\lim_{x â†’ 0} x \log x = 0$, for $P(x) = 0$ we consider\
$P(x) \log P(x)$ to be zero.

~~~ ~~~
![w=40%,f=right](entropy_example.svgz)

- for discrete $P$: $H(P) = -âˆ‘_x P(x) \log P(x)$
- for continuous $P$: $H(P) = -âˆ« P(x) \log P(x)\,\mathrm dx$

Because $\lim_{x â†’ 0} x \log x = 0$, for $P(x) = 0$ we consider
$P(x) \log P(x)$ to be zero.

~~~
Note that in the continuous case, the continuous entropy (also called
_differential entropy_) has slightly different semantics, for example, it can be
negative.

~~~
For binary logarithms, the entropy is measured in **bits**. However,
from now on, all logarithms are _natural logarithms_ with base _e_
(and then the entropy is measured in units called **nats**).

---
# Information Theory

## Cross-Entropy

$$H(P, Q) â‰ -ğ”¼_{â‡xâˆ¼P}[\log Q(x)]$$

~~~
### Gibbs Inequality
- $H(P, Q) â‰¥ H(P)$
~~~
- $H(P) = H(P, Q) â‡” P = Q$
~~~

Proof: Consider $H(P) - H(P, Q) = âˆ‘_x P(x) \log \frac{Q(x)}{P(x)}.$

~~~
Using the fact that $\log x â‰¤ (x-1)$ with equality only for $x=1$, we get
$$âˆ‘_x P(x) \log \frac{Q(x)}{P(x)} â‰¤ âˆ‘_x P(x) \left(\frac{Q(x)}{P(x)}-1\right) = âˆ‘_x Q(x) - âˆ‘_x P(x) = 0.$$

~~~
For the equality to hold, $\frac{Q(x)}{P(x)}$ must be 1 for all $x$, i.e., $P=Q$.

---
# Information Theory

## Kullback-Leibler Divergence (KL Divergence)

Sometimes also called **relative entropy**.

$$D_\textrm{KL}(P \| Q) â‰ H(P, Q) - H(P) = ğ”¼_{â‡xâˆ¼P}[\log P(x) - \log Q(x)]$$

~~~
- consequence of Gibbs inequality: $D_\textrm{KL}(P \| Q) â‰¥ 0$, $D_\textrm{KL}(P \| Q) = 0$ iff $P = Q$
~~~
- generally $D_\textrm{KL}(P \| Q) â‰  D_\textrm{KL}(Q \| P)$

---
# Common Probability Distributions
## Normal (or Gaussian) Distribution
A distribution over real numbers, parametrized by mean $Î¼$ and variance $Ïƒ^2$:
$$ğ“(x; Î¼, Ïƒ^2) = \sqrt{\frac{1}{2Ï€Ïƒ^2}} \exp \left(-\frac{(x - Î¼)^2}{2Ïƒ^2}\right)$$

~~~
For standard values $Î¼=0$ and $Ïƒ^2=1$ we get $ğ“(x; 0, 1) = \sqrt{\frac{1}{2Ï€}} e^{-\frac{x^2}{2}}$.

![w=45%,h=center](normal_distribution.svgz)

---
# Why Normal Distribution

## Central Limit Theorem
The sum of independent identically distributed random variables
with finite non-zero variance converges to normal distribution.

~~~
## Principle of Maximum Entropy
Given a set of constraints, a distribution with maximal entropy fulfilling the
constraints can be considered the most general one, containing as little
additional assumptions as possible.

~~~
Considering distributions with a **given mean and variance**, it can be proven
(using variational inference) that such a distribution with **maximum entropy**
is exactly the normal distribution.

---
section: MLE
class: section
# Maximum Likelihood Estimation
---

# Maximum Likelihood Estimation

Let $â‡‰X = \{â†’x_1, â†’x_2, â€¦, â†’x_N\}$ be training data drawn
independently from the data-generating distribution $p_\textrm{data}$.

~~~
We denote the **empirical data distribution** as $pÌ‚_\textrm{data}$, where
$$pÌ‚_\textrm{data}(â†’x) â‰ \frac{\big|\{i: â†’x_i = â†’x\}\big|}{N}.$$

~~~
Let $p_\textrm{model}(â‡â†’x; â†’w)$ be a family of distributions.
~~~
- If the weights are fixed, $p_\textrm{model}(â‡â†’x{\color{lightgray}; â†’w})$ is a probability distribution.
~~~
- If we instead consider the fixed training data $â‡‰X$, then
  $$L(â†’w) = p_\textrm{model}(â‡‰X; â†’w) = âˆ\nolimits_{i=1}^N p_\textrm{model}(â†’x_i; â†’w)$$
  is called the **likelihood**.
~~~
  Note that even if the value of the likelihood is in range $[0, 1]$, it is not
  a probability, because the likelihood is not a probability distribution.

---
# Maximum Likelihood Estimation

Let $â‡‰X = \{â†’x_1, â†’x_2, â€¦, â†’x_N\}$ be training data drawn
independently from the data-generating distribution $p_\textrm{data}$. We denote
the empirical data distribution as $pÌ‚_\textrm{data}$ and let
$p_\textrm{model}(â‡â†’x; â†’w)$ be a family of distributions.

The **maximum likelihood estimation** of $â†’w$ is:

$\displaystyle \kern8em\mathllap{â†’w_\mathrm{MLE}} = \argmax_{â†’w} p_\textrm{model}(â‡‰X; â†’w) = \argmax_{â†’w} âˆ\nolimits_{i=1}^N p_\textrm{model}(â†’x_i; â†’w)$

~~~
$\displaystyle \kern8em{} = \argmin_{â†’w} âˆ‘\nolimits_{i=1}^N -\log p_\textrm{model}(â†’x_i; â†’w)$

~~~
$\displaystyle \kern8em{} = \argmin_{â†’w} ğ”¼_{â‡â†’x âˆ¼ pÌ‚_\textrm{data}} [-\log p_\textrm{model}(â†’x; â†’w)]$

~~~
$\displaystyle \kern8em{} = \argmin_{â†’w} H(pÌ‚_\textrm{data}(â‡â†’x), p_\textrm{model}(â‡â†’x; â†’w))$

~~~
$\displaystyle \kern8em{} = \argmin_{â†’w} D_\textrm{KL}(pÌ‚_\textrm{data}(â‡â†’x)\|p_\textrm{model}(â‡â†’x; â†’w)) \color{gray} + H(pÌ‚_\textrm{data}(â‡â†’x))$

---
style: .katex-display { margin: .7em 0 }
# Maximum Likelihood Estimation

MLE can be easily generalized to the conditional case, where our goal is to predict $t$ given $â†’x$:
$$\begin{aligned}
â†’w_\mathrm{MLE} &= \argmax_{â†’w} p_\textrm{model}(â†’t | â‡‰X; â†’w) = \argmax_{â†’w} âˆ\nolimits_{i=1}^N p_\textrm{model}(t_i | â†’x_i; â†’w) \\
                &= \argmin_{â†’w} âˆ‘\nolimits_{i=1}^N -\log p_\textrm{model}(t_i | â†’x_i; â†’w) \\
                &= \argmin_{â†’w} ğ”¼_{(â‡â†’x, â‡t) âˆ¼ pÌ‚_\textrm{data}} [-\log p_\textrm{model}(t | â†’x; â†’w)] \\
                &= \argmin_{â†’w} H(pÌ‚_\textrm{data}(â‡t | â‡â†’x), p_\textrm{model}(â‡t | â‡â†’x; â†’w)) \\
                &= \argmin_{â†’w} D_\textrm{KL}(pÌ‚_\textrm{data}(â‡t | â‡â†’x)\|p_\textrm{model}(â‡t | â‡â†’x; â†’w)) \color{gray} + H(pÌ‚_\textrm{data}(â‡t | â‡â†’x))
\end{aligned}$$

~~~
where the conditional entropy is defined as
$H(pÌ‚_\textrm{data}) = ğ”¼_{(â‡â†’x, â‡t) âˆ¼ pÌ‚_\textrm{data}} [-\log (pÌ‚_\textrm{data}(t | â†’x; â†’w))]$
and the conditional cross-entropy as
$H(pÌ‚_\textrm{data}, p_\textrm{model}) = ğ”¼_{(â‡â†’x, â‡t) âˆ¼ pÌ‚_\textrm{data}} [-\log (p_\textrm{model}(t | â†’x; â†’w))]$.

~~~
The resulting _loss function_ is called **negative log-likelihood** (**NLL**), or
**cross-entropy**, or **Kullback-Leibler divergence**.

---
section: LogisticRegression
class: section
# Logistic Regression

---
# Logistic Regression

An extension of perceptron, which models the conditional probabilities
of $p(C_0|â†’x)$ and of $p(C_1|â†’x)$. Logistic regression can in fact
handle also more than two classes, which we will see in the next lecture.

~~~
Logistic regression employs the following parametrization of the conditional
class probabilities:
$$\begin{aligned}
  p(C_1 | â†’x) &= Ïƒ(â†’x^T â†’w + b) \\
  p(C_0 | â†’x) &= 1 - p(C_1 | â†’x),
\end{aligned}$$
where $Ïƒ$ is a **sigmoid function**
$$Ïƒ(x) = \frac{1}{1+e^{-x}}.$$

~~~
It can be trained using the SGD algorithm.

---
# Sigmoid Function

The sigmoid function has values in range $(0, 1)$, is monotonically
increasing and it has a derivative of $\frac{1}{4}$ at $x=0$.

$$Ïƒ(x) = \frac{1}{1+e^{-x}}$$

~~~
$$Ïƒ'(x) = Ïƒ(x) \big(1 - Ïƒ(x)\big)$$

~~~
![w=100%](sigmoid.svgz)

---
# Logistic Regression

We denote the output of the â€œlinear partâ€ of logistic regression as
$$yÌ„(â†’x; â†’w) = â†’x^T â†’w,$$
and the overall prediction as
$$y(â†’x; â†’w) = Ïƒ(yÌ„(â†’x; â†’w)) = Ïƒ(â†’x^T â†’w).$$

---
# Logistic Regression

To train logistic regression, we use MLE (the maximum likelihood
estimation). Its application is straightforward, given that $p(C_1 | â†’x; â†’w)$ is
directly the model output $y(â†’x; â†’w)$.

~~~
Therefore, the loss for a minibatch $ğ•=\{(â†’x_1, t_1), (â†’x_2, t_2), â€¦, (â†’x_N, t_N)\}$
is
$$\begin{aligned}
E(â†’w) = \frac{1}{N} âˆ‘_i -\log(p(C_{t_i} | â†’x_i; â†’w)). \\
\end{aligned}$$

~~~
<div class="algorithm">

**Input**: Input dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{0, +1\}^N$), learning rate $Î± âˆˆ â„^+$.<br>

- $â†’w â† â†’0$ or we initialize $â†’w$ randomly
- until convergence (or patience runs out), process a minibatch of examples $ğ”¹$:
  - $â†’g â† \tfrac{1}{|ğ”¹|} âˆ‘_{iâˆˆğ”¹} âˆ‡_{â†’w} \Big(-\log\big(p(C_{t_i} | â†’x_i; â†’w)\big)\Big)$
  - $â†’w â† â†’w - Î±â†’g$
</div>


---
class: middle
# Practical note

Everything we learned about **features** and **$L^2$ regularization** holds for
logistic regression too.

<center><big>ğŸ˜®ğŸ‘</center</big>

---
class: summary
# Today's Lecture Objectives

After this lecture you should be able to

- Think about binary classification using **geometric intuition** and use the
  **perceptron algorithm**.

- Define the **main concepts of information theory** (entropy, cross-entropy,
  KL-divergence) and prove their properties.

- Derive training objectives using the **maximum likelihood principle**.

- Implement and use **logistic regression** for binary classification with SGD.
