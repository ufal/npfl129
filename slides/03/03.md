title: NPFL129, Lecture 3
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Perceptron and Logistic Regression

## Milan Straka

### October 17, 2022

---
section: Perceptron
# Binary Classification

Binary classification is a classification in two classes.

~~~
The simplest way to evaluate classification is **accuracy**, which is
the ratio of input examples that were classified correctly – i.e.,
where the predicted class and the target class match.

~~~
To extend linear regression to binary classification, we might seek
a **threshold** and then classify an input as negative/positive
depending on whether $y(→x; →w) = →x^T→w + b$ is smaller/larger than a given threshold.

~~~
Zero value is usually used as the threshold, both because of symmetry and also
because the **bias** parameter acts as a trainable threshold anyway.

~~~
The set of points with prediction 0 is called a **decision boundary**.

---
# Binary Classification

![w=50%,f=right](binary_classification.svgz)

- Consider two points on the decision boundary. Because $y(→x_1; →w)=y(→x_2; →w)$,
  we have $(→x_1-→x_2)^T→w=0$, and so $→w$ is orthogonal to every vector on the
  decision surface – $→w$ is a **normal** of the boundary.

~~~
- Consider $→x$ and let $→x_⊥$ be orthogonal projection of $x$ to the boundary,
  so we can write $→x=→x_⊥ + r\frac{→w}{\|→w\|}$. Multiplying both sides
  by $→w^T$ and adding $b$, we get that the distance of $→x$ to the boundary is
  $r=\frac{y(→x)}{\|→w\|}$.

~~~
- The distance of the decision boundary from the origin is therefore
  $\frac{|b|}{\|→w\|}$.


---
# Perceptron

The perceptron algorithm is probably the oldest one for training
weights of a binary classification. Assuming the target value
$t ∈ \{-1, +1\}$, the goal is to find weights $→w$ such that
for all train data,
$$\sign(y(→x_i; →w)) = \sign(→x_i^T →w) = t_i,$$
or equivalently,
$$t_i y(→x_i; →w) = t_i →x_i^T →w > 0.$$

~~~

![w=60%,mw=35%,h=center,f=right](linearly_separable.svgz)

Note that a set is called **linearly separable**, if there exists
a weight vector $→w$ such that the above equation holds.

---
# Perceptron

The perceptron algorithm was invented by Rosenblat in 1958.

<div class="algorithm">

**Input**: Linearly separable dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ \{-1, +1\}^N$).<br>
**Output**: Weights $→w ∈ ℝ^D$ such that $t_i →x_i^T→w > 0$ for all $i$.

- $→w ← 0$
- until all examples are classified correctly, process example $i$:
  - $y ← →x_i^T →w$
  - if $t_i y ≤ 0$ (incorrectly classified example):
    - $→w ← →w + t_i →x_i$
</div>

~~~
We will prove that the algorithm always arrives at some correct set of
weights $→w$ if the training set is linearly separable.

---
# Perceptron as SGD

Consider the main part of the perceptron algorithm:

<div class="algorithm">

  - $y ← →x_i^T →w$
  - if $t_i y ≤ 0$ (incorrectly classified example):
    - $→w ← →w + t_i →x_i$
</div>

~~~
We can derive the algorithm using on-line gradient descent, using
the following loss function
$$L(y(→x; →w), t) ≝ \begin{cases} -t →x^T →w & \textrm{if~}t →x^T →w ≤ 0 \\ 0 & \textrm{otherwise}\end{cases}
  = \max(0, -t→x^T →w) = \ReLU(-t→x^T →w).$$

~~~
In this specific case, the value of the learning rate does not influence the
model behavior (different learning rates would produce models with same
predictions), because multiplying $→w$ by a constant does not change the
prediction and the loss derivative does not depend on $→w$. Note that the
_second_ condition is crucial; the first holds also for logistic regression, but
the learning rate matters there.

---
# Perceptron Example

![w=52%,h=center](perceptron_convergence.svgz)

---
class: dbend
# Proof of Perceptron Convergence

Let $→w_*$ be some weights separating the training data and let $→w_k$ be the
weights after $k$ non-trivial updates of the perceptron algorithm, with $→w_0$
being 0.

~~~
We will prove that the angle $α$ between $→w_*$ and $→w_k$ decreases at each step.
Note that
$$\cos(α) = \frac{→w_*^T →w_k}{\|→w_*\|⋅\|→w_k\|}.$$

---
class: dbend
# Proof of Perceptron Convergence

Assume that the maximum norm of any training example $\|→x\|$ is bounded by $R$,
and that $γ$ is the minimum margin of $→w_*$, so for each training example $(→x, t)$, $t →x^T →w_* ≥ γ.$

~~~
First consider the dot product of $→w_*$ and $→w_k$:
$$→w_*^T →w_k = →w_*^T (→w_{k-1} + t_k →x_k) ≥ →w_*^T →w_{k-1} + γ.$$
~~~
By iteratively applying this equation, we get
$$→w_*^T →w_k ≥ kγ.$$

~~~
Now consider the length of $→w_k$:
$$\begin{aligned}
\|→w_k\|^2 &= \|→w_{k-1} + t_k→x_k\|^2 = \|→w_{k-1}\|^2 + 2 t_k →x_k^T →w_{k-1} + \|→x_k\|^2.
\end{aligned}$$

~~~
Because $→x_k$ was misclassified, we know that $t_k →x_k^T →w_{k-1} ≤ 0$, so
$\|→w_k\|^2 ≤ \|→w_{k-1}\|^2 + R^2.$
~~~
When applied iteratively, we get $\|→w_k\|^2 ≤ k ⋅ R^2$.

---
class: dbend
# Proof of Perceptron Convergence

Putting everything together, we get
$$\cos(α) = \frac{→w_*^T →w_k}{\|→w_*\|⋅\|→w_k\|} ≥ \frac{kγ}{\sqrt{kR^2}\|→w_*\|}.$$

~~~
Therefore, the $\cos(α)$ increases during every update. Because the value of
$\cos(α)$ is at most one, we can compute the upper bound on the number
of steps when the algorithm converges as
$$1 ≤ \frac{\sqrt{k}γ}{\sqrt{R^2}\|→w_*\|}\textrm{~or~}k ≥ \frac{R^2\|→w_*\|^2}{γ^2}.$$

---
# Perceptron Issues

Perceptron has several drawbacks:
- If the input set is not linearly separable, the algorithm never finishes.

~~~
- The algorithm performs only prediction, it is not able to return the
  probabilities of predictions.
~~~
- Most importantly, Perceptron algorithm finds _some_ solution, not necessarily
  a good one, because once it finds some, it cannot perform any more updates.

![w=50%,h=center](perceptron_suboptimal.svgz)

---
section: ProbabilityBasics
# Common Probability Distributions

## Bernoulli Distribution
The Bernoulli distribution is a distribution over a binary random variable.
It has a single parameter $φ ∈ [0, 1]$, which specifies the probability of the random
variable being equal to 1.

~~~
$$\begin{aligned}
  P(x) &= φ^x (1-φ)^{1-x} \\
  𝔼[x] &= φ \\
  \Var(x) &= φ(1-φ)
\end{aligned}$$

![w=60%,h=center](bernoulli_variance.svgz)

---
# Common Probability Distributions

## Categorical Distribution
Extension of the Bernoulli distribution to random variables taking one of $K$ different
discrete outcomes. It is parameterized by $→p ∈ [0, 1]^K$ such that $∑_{i=0}^{K-1} p_{i} = 1$.

~~~
We represent outcomes as vectors $∈ \{0, 1\}^K$ in the **one-hot encoding**.
Therefore, an outcome $x ∈ \{0, 1, …, K-1\}$ is represented as a vector
$$→1_x ≝ \big([i = x]\big)_{i=0}^{K-1} = \big(\underbrace{0, …, 0}_{k}, 1, \underbrace{0, …, 0}_{K-k-1}\big).$$

~~~
The outcome probability, mean, and variance are very similar to the Bernoulli
distribution.
$$\begin{aligned}
  P(→x) &= ∏\nolimits_{i=0}^{K-1} p_i^{x_i} \\
  𝔼[x_i] &= p_i \\
  \Var(x_i) &= p_i(1-p_i) \\
\end{aligned}$$

---
section: InformationTheory
# Information Theory

## Self Information

Amount of **surprise** when a random variable is sampled.
~~~
- Should be zero for events with probability 1.
~~~
- Less likely events are more surprising.
~~~
- Independent events should have **additive** information.

~~~
$$I(x) ≝ -\log P(x) = \log \frac{1}{P(x)}$$

---
# Information Theory

## Entropy

Amount of **surprise** in the whole distribution.
$$H(P) ≝ 𝔼_{⁇x∼P}[I(x)] = -𝔼_{⁇x∼P}[\log P(x)]$$

~~~
- for discrete $P$: $H(P) = -∑_x P(x) \log P(x)$
- for continuous $P$: $H(P) = -∫ P(x) \log P(x)\,\mathrm dx$

~~~
Because $\lim_{x → 0} x \log x = 0$, for $P(x) = 0$ we consider\
$P(x) \log P(x)$ to be zero.

~~~ ~~~
![w=40%,f=right](entropy_example.svgz)

- for discrete $P$: $H(P) = -∑_x P(x) \log P(x)$
- for continuous $P$: $H(P) = -∫ P(x) \log P(x)\,\mathrm dx$

Because $\lim_{x → 0} x \log x = 0$, for $P(x) = 0$ we consider
$P(x) \log P(x)$ to be zero.

~~~
Note that in the continuous case, the continuous entropy (also called
_differential entropy_) has slightly different semantics, for example, it can be
negative.

~~~
For binary logarithms, the entropy is measured in **bits**. However,
from now on, all logarithms are _natural logarithms_ with base _e_
(and then the entropy is measured in units called **nats**).

---
# Information Theory

## Cross-Entropy

$$H(P, Q) ≝ -𝔼_{⁇x∼P}[\log Q(x)]$$

~~~
### Gibbs Inequality
- $H(P, Q) ≥ H(P)$
~~~
- $H(P) = H(P, Q) ⇔ P = Q$
~~~

Proof: Consider $H(P) - H(P, Q) = ∑_x P(x) \log \frac{Q(x)}{P(x)}.$

~~~
Using the fact that $\log x ≤ (x-1)$ with equality only for $x=1$, we get
$$∑_x P(x) \log \frac{Q(x)}{P(x)} ≤ ∑_x P(x) \left(\frac{Q(x)}{P(x)}-1\right) = ∑_x Q(x) - ∑_x P(x) = 0.$$

~~~
For the equality to hold, $\frac{Q(x)}{P(x)}$ must be 1 for all $x$, i.e., $P=Q$.

---
# Information Theory

## Corollary of the Gibbs inequality

For a categorical distribution with $n$ outcomes, $H(P) ≤ \log n$, because for
$Q(x) = 1/n$ we get $H(P) ≤ H(P, Q) = -∑_x P(x) \log Q(x) = \log n.$

~~~
## Nonsymmetry

Note that generally $H(P, Q) ≠ H(Q, P)$.

---
# Information Theory

## Kullback-Leibler Divergence (KL Divergence)

Sometimes also called **relative entropy**.

$$D_\textrm{KL}(P \| Q) ≝ H(P, Q) - H(P) = 𝔼_{⁇x∼P}[\log P(x) - \log Q(x)]$$

~~~
- consequence of Gibbs inequality: $D_\textrm{KL}(P \| Q) ≥ 0$, $D_\textrm{KL}(P \| Q) = 0$ iff $P = Q$
~~~
- generally $D_\textrm{KL}(P \| Q) ≠ D_\textrm{KL}(Q \| P)$

---
# Nonsymmetry of KL Divergence

![w=100%,v=middle](kl_nonsymmetry.svgz)

---
# Common Probability Distributions
## Normal (or Gaussian) Distribution
Distribution over real numbers, parameterized by a mean $μ$ and variance $σ^2$:
$$𝓝(x; μ, σ^2) = \sqrt{\frac{1}{2πσ^2}} \exp \left(-\frac{(x - μ)^2}{2σ^2}\right)$$

~~~
For standard values $μ=0$ and $σ^2=1$ we get $𝓝(x; 0, 1) = \sqrt{\frac{1}{2π}} e^{-\frac{x^2}{2}}$.

![w=45%,h=center](normal_distribution.svgz)

---
# Why Normal Distribution

## Central Limit Theorem
The sum of independent identically distributed random variables
with finite variance converges to normal distribution.

~~~
## Principle of Maximum Entropy
Given a set of constraints, a distribution with maximal entropy fulfilling the
constraints can be considered the most general one, containing as little
additional assumptions as possible.

~~~
Considering distributions with a given mean and variance, it can be proven
(using variational inference) that such a distribution with **maximum entropy**
is exactly the normal distribution.

---
section: MLE
# Maximum Likelihood Estimation

Let $⇉X = \{→x_1, →x_2, …, →x_N\}$ be training data drawn
independently from the data-generating distribution $p_\textrm{data}$.

~~~
We denote the **empirical data distribution** as $p̂_\textrm{data}$, where
$$p̂_\textrm{data}(→x) ≝ \frac{\big|\{i: →x_i = →x\}\big|}{N}.$$

~~~
Let $p_\textrm{model}(→x; →w)$ be a family of distributions.
~~~
- If the weights are fixed, $p_\textrm{model}(→x{\color{lightgray}; →w})$ is a probability distribution.
~~~
- If we instead consider the fixed training data $⇉X$, then
  $$L(→w) = p_\textrm{model}(⇉X; →w) = ∏\nolimits_{i=1}^N p_\textrm{model}(→x_i; →w)$$
  is called the **likelihood**.
~~~
  Note that even if the value of the likelihood is in range $[0, 1]$, it is not
  a probability, because the likelihood is not a probability distribution.

---
# Maximum Likelihood Estimation

Let $⇉X = \{→x_1, →x_2, …, →x_N\}$ be training data drawn
independently from the data-generating distribution $p_\textrm{data}$. We denote
the empirical data distribution as $p̂_\textrm{data}$ and let
$p_\textrm{model}(→x; →w)$ be a family of distributions.

The **maximum likelihood estimation** of $→w$ is:

$\displaystyle \kern14em\mathllap{→w_\mathrm{MLE} = \argmax_{→w} p_\textrm{model}(⇉X; →w)} = \argmax_{→w} ∏\nolimits_{i=1}^N p_\textrm{model}(→x_i; →w)$

~~~
$\displaystyle \kern14em{} = \argmin_{→w} ∑\nolimits_{i=1}^N -\log p_\textrm{model}(→x_i; →w)$

~~~
$\displaystyle \kern14em{} = \argmin_{→w} 𝔼_{⁇→x ∼ p̂_\textrm{data}} [-\log p_\textrm{model}(→x; →w)]$

~~~
$\displaystyle \kern14em{} = \argmin_{→w} H(p̂_\textrm{data}(→x), p_\textrm{model}(→x; →w))$

~~~
$\displaystyle \kern14em{} = \argmin_{→w} D_\textrm{KL}(p̂_\textrm{data}(→x)\|p_\textrm{model}(→x; →w)) \color{gray} + H(p̂_\textrm{data})$

---
style: .katex-display { margin: .7em 0 }
# Maximum Likelihood Estimation

MLE can be easily generalized to the conditional case, where our goal is to predict $t$ given $→x$:
$$\begin{aligned}
→w_\mathrm{MLE} = \argmax_{→w} p_\textrm{model}(→t | ⇉X; →w) &= \argmax_{→w} ∏\nolimits_{i=1}^N p_\textrm{model}(t_i | →x_i; →w) \\
                &= \argmin_{→w} ∑\nolimits_{i=1}^N -\log p_\textrm{model}(t_i | →x_i; →w) \\
                &= \argmin_{→w} 𝔼_{(⁇→x, ⁇t) ∼ p̂_\textrm{data}} [-\log p_\textrm{model}(t | →x; →w)] \\
                &= \argmin_{→w} H(p̂_\textrm{data}, p_\textrm{model}(t | →x; →w)) \\
                &= \argmin_{→w} D_\textrm{KL}(p̂_\textrm{data}\|p_\textrm{model}(t | →x; →w)) \color{gray} + H(p̂_\textrm{data})
\end{aligned}$$

~~~
where the conditional entropy is defined as
$H(p̂_\textrm{data}) = 𝔼_{(⁇→x, ⁇t) ∼ p̂_\textrm{data}} [-\log (p̂_\textrm{data}(t | →x; →w))]$
and the conditional cross-entropy as
$H(p̂_\textrm{data}, p_\textrm{model}) = 𝔼_{(⁇→x, ⁇t) ∼ p̂_\textrm{data}} [-\log (p_\textrm{model}(t | →x; →w))]$.

~~~
The resulting _loss function_ is called **negative log-likelihood** (**NLL**), or
**cross-entropy**, or **Kullback-Leibler divergence**.

---
class: dbend
# Properties of Maximum Likelihood Estimation

Assume that the true data generating distribution $p_\textrm{data}$ lies within the model<br>
family $p_\textrm{model}(⋅; →w)$. Furthermore, assume there exists a unique
$→w_{p_\textrm{data}}$ such that $p_\textrm{data} = p_\textrm{model}(⋅; →w_{p_\textrm{data}})$.

~~~
- MLE is a _consistent_ estimator. If we denote $→w_m$ to be the parameters
  found by MLE for a training set with $m$ examples generated by the data
  generating distribution, then $→w_m$ converges in probability to
  $→w_{p_\textrm{data}}$.

  Formally, for any $ε > 0$, $P(\|→w_m - →w_{p_\textrm{data}}\| > ε) → 0$
  as $m → ∞$.

~~~
- MLE is in a sense the most _statistically efficient_. For any consistent estimator,
  let us consider the average distance of $→w_m$ and $→w_{p_\textrm{data}}$:
  $𝔼_{⁇→x_1, …, ⁇→x_m ∼ p_\textrm{data}} \big[\|→w_m - →w_{p_\textrm{data}}\|^2\big]$. \
  It can be shown (Rao 1945, Cramér 1946) that no consistent estimator has
  lower mean squared error than the maximum likelihood estimator.

~~~
Therefore, for reasons of consistency and efficiency, maximum likelihood is
often considered the preferred estimator for machine learning.

---
section: LogisticRegression
# Logistic Regression

An extension of perceptron, which models the conditional probabilities
of $p(C_0|→x)$ and of $p(C_1|→x)$. Logistic regression can in fact
handle also more than two classes, which we will see in the next lecture.

~~~
Logistic regression employs the following parameterization of the conditional
class probabilities:
$$\begin{aligned}
  p(C_1 | →x) &= σ(→x^T →w + b) \\
  p(C_0 | →x) &= 1 - p(C_1 | →x),
\end{aligned}$$
where $σ$ is a **sigmoid function**
$$σ(x) = \frac{1}{1+e^{-x}}.$$

~~~
It can be trained using the SGD algorithm.

---
# Sigmoid Function

The sigmoid function has values in range $(0, 1)$, is monotonically
increasing and it has a derivative of $\frac{1}{4}$ at $x=0$.

$$σ(x) = \frac{1}{1+e^{-x}}$$

~~~
$$σ'(x) = σ(x) \big(1 - σ(x)\big)$$

~~~
![w=100%](sigmoid.svgz)

---
# Logistic Regression

We denote the output of the “linear part” of the logistic regression as
$$ȳ(→x; →w) = →x^T →w,$$
and the overall prediction as
$$y(→x; →w) = σ(ȳ(→x; →w)) = σ(→x^T →w).$$

---
# Logistic Regression

The logistic regression output $y(→x; →w)$ models the probability of class
$C_1$, $p(C_1 | →x)$.

To give some meaning to the output of the linear part $ȳ(→x; →w)$, starting with
$$p(C_1 | →x) = σ(ȳ(→x; →w)) = \frac{1}{1 + e^{-ȳ(→x; →w)}},$$
~~~
we arrive at
$$ȳ(→x; →w) = \log\left(\frac{p(C_1 | →x)}{1 - p(C_1 | →x)}\right) = \log\left(\frac{p(C_1 | →x)}{p(C_0 | →x)}\right),$$
which is called a **logit** and it is a logarithm of odds of the probabilities
of the two classes.

---
# Logistic Regression

To train the logistic regression, we use MLE (the maximum likelihood
estimation). Its application is straightforward, given that $p(C_1 | →x; →w)$ is
directly the model output $y(→x; →w)$.

~~~
Therefore, the loss for a minibatch $⇉X=\{(→x_1, t_1), (→x_2, t_2), …, (→x_N, t_N)\}$
is
$$\begin{aligned}
E(→w) = \frac{1}{N} ∑_i -\log(p(C_{t_i} | →x_i; →w)). \\
\end{aligned}$$

~~~
<div class="algorithm">

**Input**: Input dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ \{0, +1\}^N$), learning rate $α ∈ ℝ^+$.<br>

- $→w ← 0$
- until convergence (or patience runs out), process a minibatch of examples with indices $→b$:
  - $→g ← \tfrac{1}{|→b|} ∑_{i∈→b} ∇_{→w} -\log\big(p(C_{t_i} | →x_i; →w)\big)$
  - $→w ← →w - α→g$
</div>

---
section: CV
# Cross-Validation

We already talked about a **train set** and a **test set**. Given that the main
goal of machine learning is to perform well on unseen data, the test set must
not be used during training nor hyperparameter selection. Ideally, it is hidden
to us altogether.

~~~
Therefore, to evaluate a machine learning model (for example to select model
architecture, features, or hyperparameter value), we normally need the
**validation** or a **development** set.

~~~
However, using a single development set might give us noisy results. To obtain
less noisy results (i.e., with smaller variance), we can use
**cross-validation**.

~~~
![w=48%,f=right](k-fold_cross_validation.svgz)

In cross-validation, we choose multiple validation sets from the training data,
and for each one, we train a model on the rest of the training data and
evaluate on the chosen validation sets. A commonly used strategy to choose
the validation sets is called **k-fold cross-validation**. Here the training set is partitioned
into $k$ subsets of approximately the same size, and each subset takes turn
to play a role of a validation set.

---
# Cross-Validation

An extreme case of the **k-fold cross-validation** is **leave-one-out
cross-validation**, where every element is considered a separate validation
set.

~~~
Computing leave-one-out cross-validation is usually extremely inefficient for
larger training sets, but in the case of linear regression with $L^2$-regularization,
it can be evaluated efficiently.
~~~
- If you are interested, see:

  _Ryan M. Rifkin and Ross A. Lippert: Notes on Regularized Least Square_
  http://cbcl.mit.edu/publications/ps/MIT-CSAIL-TR-2007-025.pdf

~~~
- Implemented by `sklearn.linear_model.RidgeCV`.

