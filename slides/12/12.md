title: NPFL129, Lecture 12
class: title, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Statistical Hypothesis Testing, Model Comparison

## Jindřich Libovický <small>(reusing materials by Milan Straka)</small>

### December 16, 2024

---
class: middle
# Today's Lecture Objectives

After this lecture you should be able to

- Explain foundations of statistical hypothesis testing.

- Reason about multiple comparisons problem.

- Use Bootstrap Resampling and Permutation Tests to compare machine learning
  models.

---
section: Hypothesis Testing
class: section
# Statistical Hypothesis Testing

---
# Statistical Hypothesis Testing

Assume we have a hypothesis testable using observed outcomes of random variables.

~~~
There are two slightly differing views on statistical hypothesis testing:

~~~
1. In the first one, we assume we have a **null hypothesis** $H_0$, and we
   are interested in whether we can **reject it** using the observed data.

~~~
   The result is **statistically significant**, if it is very unlikely
   that the observed data have occurred given the null hypothesis.

~~~
   The **significance level** of a test is the threshold of this unlikeliness.

~~~
2. In the second view, we have two hypotheses, a null hypothesis $H_0$
   and an **alternative hypothesis** $H_1$, and we want to distinguish between
   them.

~~~
   We consider only two outcomes of the test:
   - either we “reject” the null hypothesis, if the data is very unlikely
   to have occurred given the null hypothesis; or
~~~
   - we cannot reject the null hypothesis.

~~~
   In simple cases when $H_0$ is just a negation of $H_1$, rejecting
   $H_0$ amounts to accepting $H_1$.

---
class: tablewide
# Type I and Type II Errors

Consider the _courtroom trial_ example, where, as in a criminal trial,
the defendant is considered not guilty until their guilt is proven.

~~~
In this setting, $H_0$ is “not guilty” and $H_1$ is “guilty”.

~~~
| | $H_0$ is true<br>Truly not guilty | $H_1$ is true<br>Truly guilty |
|-|-----------------------------------|-------------------------------|
| Not proven guilty<br>Not rejecting $H_0$ | Correct decision<br>True negative | Wrong decision<br>False negative<br>**Type II Error** |
| Proven guilty<br>Rejecting $H_0$ | Wrong decision<br>False positive<br>**Type I Error** | Correct decision<br>True positive |

~~~
Our goal is to limit the Type 1 errors – the test **significance level** is the
type 1 error rate.

---
# Statistical Hypothesis Testing

The crucial part of a statistical test is the **test statistic**. It is some
summary of the observed data, very often a single value (like mean), which
can be used to distinguish the null and the alternative hypothesis.

~~~
It is crucial to be able to compute the distribution of the test statistic,
which allows the **p-values** to be calculated.

~~~
A **p-value** is the probability of obtaining a test statistic value at least as extreme
as the one actually observed, assuming the validity of the null hypothesis.
A very small p-value indicates that the observed data are very unlikely under
the null hypothesis.

~~~
Given a test statistic, we usually perform one of the following tests:

![w=13%,f=right](test_right_tail.svgz)

- a one-sided right-tail test, when the p-value of $t$ is $P(\mathit{test~statistic} > t | H_0)$;

~~~
![w=13%,f=right;clear:both](test_left_tail.svgz)

- a one-sided left-tail test, when the p-value of $t$ is $P(\mathit{test~statistic} < t | H_0)$;
~~~
![w=13%,f=right;clear:both](test_two_sided.svgz)

- a two-sided test, when the p-value of $t$ is twice the minimum of
  $P(\mathit{test~statistic} < t | H_0)$ and $P(\mathit{test~statistic} > t | H_0)$.
  For a symmetrical centered distribution,
  $P(\operatorname{abs}(\mathit{test~statistic}) > \operatorname{abs}(t) | H_0)$
  can also be used.

---
# Statistical Hypothesis Testing

Therefore, the whole procedure consists of the following steps:
~~~
1. Formulate the null hypothesis $H_0$, and optionally the alternative
   hypothesis $H_1$.

~~~
1. Choose the test statistic.
~~~
1. Compute the observed value of the test statistic.
~~~
1. Calculate the p-value, which is the probability of a test
   statistic value being at least as extreme as the observed one,
   under the null hypothesis $H_0$.
~~~
1. Reject the null hypothesis $H_0$ (in favor of the alternative hypothesis
   $H_1$), if the p-value is less than the chosen significance level $α$
   (a standard is to use $α$ at most 5%; common choices include 5%, 1%, 0.5%
   or 0.1%, but vary a lot in different fields).

---
# Test Statistics

There are several kinds of test statistics:
~~~
- **one-sample tests**, where we sample values from one distribution.

~~~
  Common one-sample tests usually check for
  - the mean of the distribution to be greater than/less than/equal to zero;
  - the goodness of fit (that the data comes from a normal or categorical
    distribution of given parameters);

~~~
- **two-sample tests**, where we sample independently from two distributions;

~~~
- **paired tests**, in which case we also sample from two distributions, but
  the samples are paired (i.e., evaluating several models on the same data).

~~~
  In paired tests, we usually compute the difference between the paired members
  and perform a one-sample test on the mean of the differences.

---
# Parametric Test Statistics Distributions

There are many commonly used test statistics, with different requirements
and conditions. We only mention several commonly-used ones, but it is by no means
a comprehensive treatment.

~~~
- **Z-Test** is a test, where the test statistic can be approximated by a normal
  distribution. For example, it can be used when comparing a mean of samples
  _with known variance_ to a given value.

~~~
- In **Student's _t_-test** the test statistic follows a Student's _t_-distribution
  (where Student is the pseudonym used by the real author W. S. Gosset), which
  is the distribution of a sample mean of a normally-distributed population _with
  unknown variance_.

~~~
  Therefore, the _t_-test is used when comparing a mean of
  samples with unknown variance to a given value, or to a mean of samples from
  another distribution with the same sample size and variance.

~~~
- **Chi-squared test** utilizes a test statistic with a chi-squared
  distribution, which is a distribution of the sum of squares of $k$
  independent normally distributed variables.

~~~
  The essential Pearson's chi-squared test can be used to evaluate the goodness of
  fit of $k$ random categorical samples with respect to a given categorical
  distribution.

---
section: Multiple Comparisons
class: section
# Multiple Comparisons Problem
---


# Multiple Comparisons Problem

A **multiple comparisons problem** (or multiple testing problem) arises, if we
test many statistical hypotheses using the same observed data.

~~~
![w=72%,h=center](xkcd_significant_1.png)

---
# Multiple Comparisons Problem

![w=70%,h=center](xkcd_significant_2.png)

---
# Multiple Comparisons Problem

![w=70%,h=center](xkcd_significant_3.png)

---
# Multiple Comparisons Problem

![w=90%,mw=45%,f=left](xkcd_significant_4.png)

~~~
It is problematic if we perform many statistical tests, and only report
the ones with statistically significant results.

~~~
![w=54%,mh=60%,v=bottom](spurious_correlation.svgz)

---
section: FWER
class: section
# Family-Wise Error Rate

---
# Family-Wise Error Rate

There are several ways to handle the multiple comparisons problem; one of the
easiest (but often overly conservative) is to limit the **family-wise error
rate**, which is the probability of at least one type 1 error in the family.
$$\mathrm{FWER} = P\bigg(⋃_i \Big(p_i ≤ α\Big)\bigg).$$

~~~
One way of controlling the family-wise error rate is the **Bonferroni
correction**, which rejects the null hypothesis of a test in the family of
size $m$ when $p_i ≤ \frac{α}{m}$.

~~~
Assuming such a correction and utilizing the Boole's inequality
$P\big(⋃\nolimits_i A_i\big) ≤ ∑_i P(A_i)$, we get that
$$\mathrm{FWER} = P\bigg(⋃_i \Big(p_i ≤ \frac{α}{m}\Big)\bigg) ≤ ∑_i P\Big(p_i ≤ \frac{α}{m}\Big) = m \cdot \frac{α}{m} = α.$$

~~~
Note that there exist many more powerful methods like Holm-Bonferroni or Šidák correction.

---
section: Model Comparison
class: section
# Model Comparison
---

# Model Comparison

The goal of model comparison is to test whether some model delivers better
performance on unseen data than another one.

~~~
However, we usually only have a single fixed-size test set. For the rest of the
lecture, we assume the test set instances are independently sampled from the
data-generating distribution.

~~~
Even if comparing the models on the given test set is unbiased, we would like
to obtain some significance level of the result.

~~~
Therefore, we perform a statistical test with the alternative hypothesis that
model $y$ is better than model $z$; therefore, the null hypothesis is that
model $y$ is the same or worse than model $z$.

~~~
However, we only have one sample (the result of the model on the test set). We
therefore turn to **bootstrap resampling**.

---
section: Bootstrap Resampling
class: section
# Bootstrap Resampling

---
# Bootstrap Resampling

In order to obtain multiple samples of model performance, we exploit the fact
that the test set consists of _a collection_ of examples.

~~~
Therefore, we can generate different test sets by bootstrap resampling. Notably,
we obtain a same-sized test set by sampling the original test set examples _with
replacement_. Naturally, we can easily measure the performance of any given
model on such generated test sets.

~~~

<div class="algorithm">

**Input**: Test set $\{(→x_1, t_1), …, (→x_N, t_N)\}$, model predictions $\{y(→x_1), …, y(→x_N)\}$,
metric $E$, number of resamplings $R$.<br>
**Output**: $R$ samples of model performance.

~~~
- $\mathrm{performances} ← [\,]$
- repeat $R$ times:
  - sample $N$ test set examples with replacement, together with corresponding
    model predictions
  - measure the performance on the sampled data using the metric $E$, and append the
    result to $\mathrm{performances}$
</div>

---
# Bootstrap Resampling – Confidence Intervals

When using bootstrap resampling on a single model, we can measure the
confidence intervals of model performance.

~~~
For a given confidence level (95\% is the most common value), the **confidence
interval** is an estimate of a value range of some unknown parameter (like
a mean performance of some model on unseen data), such that the confidence
interval contains the true value of the unknown parameter with the probability
given by the confidence level.

~~~
When given the empirical distribution of model performances produced by
bootstrap resampling, we can estimate the 95% confidence interval as
a range from the 2.5th percentile to the 97.5th percentile of the empirical
distribution (the so-called _percentile bootstrap_).

---
# Paired Bootstrap Test

An analogous approach is sometimes used to perform model comparison
– a procedure sometimes called the **paired bootstrap test**.

Even if a two-sample test could be used, such a test does not consider the fact
that some of the inputs might be more difficult than others, and takes into
account cases when a weaker model achieves higher performance on a simpler test
set than a stronger model on a more difficult test set. Therefore, we perform
a **paired** test.

~~~
Our alternative hypothesis is that the mean of the model performance
differences is larger than zero, and the null hypothesis is that it is less
than or equal to zero. We then repeatedly sample a test set with repetition,
and compute the difference of the model performances on the sampled test set.
Finally, we compute the proportion of bootstrap samples where the performance
difference is less than or equal to zero.

---
# Paired Bootstrap Test Algorithm

<div class="algorithm">

**Input**: Test set $\{(→x_1, t_1), …, (→x_N, t_N)\}$, model predictions $\{y(→x_1), …, y(→x_N)\}$,<br>
model predictions $\{z(→x_1), …, z(→x_N)\}$, metric $E$, number of resamplings $R$.<br>
**Output**: Estimated probability of model $y$ performing worse than or equal to model $z$ (beware
that such a quantity is not a p-value).

~~~
- $\mathrm{differences} ← [\,]$
- repeat $R$ times:
  - sample $N$ test set examples with replacement, together with the
    corresponding predictions of the models
  - measure the performances of the models $y$ and $z$ on the sampled data using
    the metric $E$, and append their difference to $\mathrm{differences}$<br>
    $$(the performance of model $y$) - (the performance of model $z$)$$
- return the ratio of the $\mathrm{differences}$ which are less than or equal to zero
</div>


---
# Paired Bootstrap Test Visualization

For illustration, consider models for the `isnt_it_ironic` competition
utilizing either 3 (red) or 4 (green) in-word character n-grams. On the left,
there are distributions of the individual model performances, while on the right
there is a distribution of their differences.

![w=50%](bootstrap_char3_vs_char4_500.svgz)![w=48.5%](bootstrap_char3_vs_char4_500_differences.svgz)

The histograms are generated using 50 bins and 500 resamplings.

---
# Paired Bootstrap Test Visualization

For illustration, consider models for the `isnt_it_ironic` competition
utilizing either 3 (red) or 4 (green) in-word character n-grams. On the left,
there are distributions of the individual model performances, while on the right
there is a distribution of their differences.

![w=50%](bootstrap_char3_vs_char4_5000.svgz)![w=49%](bootstrap_char3_vs_char4_5000_differences.svgz)

The histograms are generated using 50 bins and 5000 resamplings.

---
# Paired Bootstrap Test Visualization

For illustration, consider models for the `isnt_it_ironic` competition
utilizing either 4 (red) or 5 (green) in-word character n-grams. On the left,
there are distributions of the individual model performances, while on the right
there is a distribution of their differences.

![w=50%](bootstrap_char4_vs_char5_5000.svgz)![w=49%](bootstrap_char4_vs_char5_5000_differences.svgz)

The histograms are generated using 50 bins and 5000 resamplings.

---
# Paired Bootstrap Test Problems

Unfortunately, the value returned by the algorithm is not really a p-value.

~~~
The reason is that the distribution of differences was obtained
**under the true distribution**.

~~~
However, to perform a statistical test, we require the distribution of the
test statistic **under the null hypothesis**.

~~~
Nevertheless, you can encounter such paired bootstrap tests “in the wild”.

---
section: Permutation Test
class: section
# Permutation Test

---
# Permutation Test

To obtain a principled p-value for a model comparison, we can turn to
a **permutation test**.

~~~
The main idea is that
> _if the models are equally good, it does not matter if we utilize predictions
> from the first or the second one._

~~~
Therefore, if we consider all possible choices of prediction origins, we
obtain a distribution of performances under the hypothesis that the
models are equally good.

~~~
Finally, the p-value is the proportion of permutations where the performance of the model in question is at least as extreme as observed.

~~~
Of course, enumerating all assignments is not feasible. Therefore, we sample
only some number of random assignments, resulting in a **random** or **Monte Carlo**
or **approximate** permutation test.

---
# Random Permutation Test Algorithm

<div class="algorithm">

**Input**: Test set $\{(→x_1, t_1), …, (→x_N, t_N)\}$, model predictions $\{y(→x_1), …, y(→x_N)\}$,<br>
model predictions $\{z(→x_1), …, z(→x_N)\}$, metric $E$, number of resamplings $R$.<br>
**Output**: Estimated p-value assuming that model $y$ performance is worse than or equal to model $z$ performance.

- $\mathrm{performances} ← [\,]$
- repeat $R$ times:
  - for each test set example, uniformly randomly choose which model to obtain
    the prediction from
  - measure the performance of the obtained test set prediction using
    the metric $E$, and append the score to $\mathrm{performances}$
- return the ratio of the $\mathrm{performances}$ which are greater than or equal to
  the performance of model $y$
</div>

---
# Random Permutation Test Visualization

Again considering the `isnt_it_ironic` models, we compare the 4-vs-3 in-word
character n-grams in the left graph and the 5-vs-4 in-word character n-grams in
the right graph, using a random permutation test with 5000 resamplings. Note that
the resulting p-values are not much different from the probabilities computed by
the paired bootstrap test.

![w=50%](random_permutation_char3_vs_char4_5000.svgz)![w=50%](random_permutation_char4_vs_char5_5000.svgz)

---
class: dbend
# Random Permutation Test Strikes Back

Formally, because we did not consider all possible assignments of predictions,
we do not obtain the true p-value, but just an approximation of it.
~~~
In other words, if the algorithm returns $β$, the probability that the real
p-value fulfills
$$p < β$$
is only roughly 50\%.

~~~
Nevertheless, we are usually interested only in deciding whether $p < α$ for
a pre-defined $α$.

~~~
In such a case, if $β < α$, the probability that $p < α$ does not hold converges
to zero as the number of resamplings increases (because of the concentration
inequalities, for example Hoeffding’s inequality; in other words, the confidence
interval of the real p-value gets smaller around $β$ as the number of resamplings
increases).
~~~
Therefore, it suffices to perform enough resamplings. For details and a tight
bound on the number of resamplings, see the paper
> <small>_Alex Gandy: Sequential Implementation of Monte Carlo Tests With Uniformly
> Bounded Resampling Risk_ [https://arxiv.org/abs/math/0612488](https://arxiv.org/abs/math/0612488).</small>

---
class: middle
# Today's Lecture Objectives

After this lecture you should be able to

- Explain foundations of statistical hypothesis testing.

- Reason about multiple comparisons problem.

- Use Bootstrap Resampling and Permutation Tests to compare machine learning
  models.
