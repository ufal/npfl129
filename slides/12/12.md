title: NPFL129, Lecture 12
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Gaussian Mixture, EM Algorithm, Bias-Variance Trade-off

## Milan Straka

### December 21, 2020

---
section: GaussianMixture
# Gaussian Mixture

Let $â†’x_1, â†’x_2, â€¦, â†’x_N$ be a collection of $N$ input examples, each being
a $D$-dimensional vector $â†’x_i âˆˆ â„^D$. Let $K$, the number of target clusters,
be given.

~~~
Our goal is to represent the data as a Gaussian mixture, which is a combination
of $K$ Gaussian in the form
$$p(â†’x) = âˆ‘_{k=1}^K Ï€_k ğ“(â†’x | â†’Î¼_k, â‡‰Î£_k).$$
Therefore, each cluster is parametrized as $ğ“(â†’x | â†’Î¼_k, â‡‰Î£_k)$.

~~~
Let $â†’z âˆˆ \{0, 1\}^K$ be a $K$-dimensional random variable, such that exactly
one $z_k$ is 1, denoting to which cluster a training example belongs.
~~~
Let the marginal distribution of $z_k$ be
$$p(z_k = 1) = Ï€_k,$$
so that the priors $Ï€_k$ represent the â€œfertilityâ€ of the clusters.
~~~
Then, $p(â†’z) = âˆ_k Ï€_k^{z_k}$.

---
# Gaussian Mixture

![w=6%,f=right](mog_latent.svgz)

We can write
$$p(â†’x) = âˆ‘_{â†’z} p(â†’z) p(â†’x | â†’z) = âˆ‘_{k=1}^K Ï€_k ğ“(â†’x | â†’Î¼_k, â‡‰Î£_k),$$
~~~
and the probability of the whole clustering is therefore
$$\log p(â‡‰X | â†’Ï€, â†’Î¼, â†’Î£) = âˆ‘_{i=1}^N \log \left(âˆ‘_{k=1}^K Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)\right).$$

~~~
To fit a Gaussian mixture model, we utilize maximum likelihood estimation and
minimize
$$ğ“›(â‡‰X) = - âˆ‘_i \log âˆ‘_{k=1}^K Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k).$$

---
# Gaussian Mixture

The derivative of the loss with respect to $â†’Î¼_k$ gives
$$\frac{âˆ‚ğ“›(â‡‰X)}{âˆ‚â†’Î¼_k} = - âˆ‘_i \frac{Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i | â†’Î¼_l, â‡‰Î£_l)} â‡‰Î£_k^{-1} \big(â†’x_i - â†’Î¼_k\big).$$

~~~
Denoting $r(z_{i,k}) = \frac{Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i | â†’Î¼_l, â‡‰Î£_l)}$,
setting the derivative equal to zero and multiplying by $â‡‰Î£_k^{-1}$, we get
$$â†’Î¼_k = \frac{âˆ‘_i r(z_{i,k}) â†’x_i}{âˆ‘_i r(z_{i,k})}.$$

~~~
The $r(z_{i,k})$ are usually called **responsibilities** and denote the
probability $p(z_k = 1 | â†’x_i)$. Note that the responsibilities depend on
$â†’Î¼_k$, so the above equation is not an analytical solution for $â†’Î¼_k$, but
can be used as an _iterative_ algorithm for converging to a local optimum.

---
# Gaussian Mixture

For $â‡‰Î£_k$, we again compute the derivative of the loss, which is technically
complicated (we need to compute a derivative of a matrix inverse, and also we
need to differentiate matrix determinant) and results in an analogous equation:
$$â†’Î£_k = \frac{âˆ‘_i r(z_{i,k}) (â†’x_i - â†’Î¼_k) (â†’x_i - â†’Î¼_k)^T}{âˆ‘_i r(z_{i,k})}.$$

~~~
To minimize the loss with respect to $â†’Ï€$, we need to include the constraint
$âˆ‘_k Ï€_k = 1$, so we form a Lagrangian $ğ“›(â‡‰X) - Î»\left(âˆ‘\nolimits_k Ï€_k - 1\right)$,
and get
$$\frac{âˆ‚ğ“›(â‡‰X)}{âˆ‚Ï€_k} = âˆ‘_i \frac{ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i | â†’Î¼_l, â‡‰Î£_l)} - Î».$$
~~~
Setting the derivative to zero and multiplying it by $Ï€_k$, we obtain $Ï€_k = \frac{1}{Î»} â‹… âˆ‘_i r(z_{i,k})$, so
$$Ï€_k = 1/N â‹… âˆ‘\nolimits_i r(z_{i,k}).$$

---
# Gaussian Mixture

<div class="algorithm">

**Input**: Input points $â†’x_1, â€¦, â†’x_N$, number of clusters $K$.

- Initialize $â†’Î¼_k, â‡‰Î£_k$ and $Ï€_k$. It is common to start by running the
  K-Means algorithm to obtain $z_{i,k}$, set $r(z_{i,k}) â† z_{i,k}$
  and use the **M step** below.

~~~
- Repeat until convergence (or until patience runs out):
  - **E step**. Evaluate the responsibilities as
    $$r(z_{i,k}) = \frac{Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i | â†’Î¼_l, â‡‰Î£_l)}.$$
~~~
  - **M step**. Maximize the log-likelihood by setting

    $$â†’Î¼_k = \frac{âˆ‘_i r(z_{i,k}) â†’x_i}{âˆ‘_i r(z_{i,k})},~~
      â†’Î£_k = \frac{âˆ‘_i r(z_{i,k}) (â†’x_i - â†’Î¼_k) (â†’x_i - â†’Î¼_k)^T}{âˆ‘_i r(z_{i,k})},~~
      Ï€_k = \frac{âˆ‘_i r(z_{i,k})}{N}.$$

</div>

---
# Gaussian Mixture

![w=75%,h=center](../11/mog_data.svgz)
![w=75%,h=center](../11/mog_illustration.svgz)

---
# Gaussian Mixture

![w=75%,h=center](mog_example.svgz)

---
class: dbend
section: EM
# EM Algorithm

The algorithm for estimating the Gaussian mixture is an example of an **EM
algorithm**.

~~~
The **EM algorithm** algorithm can be used when given a joint distribution
$p(â‡‰X, â‡‰Z | â†’w)$ over observed variables $â‡‰X$ and latent (hidden, unseen)
variables $â‡‰Z$, parametrized by $â†’w$,
~~~
we maximize
$$\log p(â‡‰X; â†’w) = \log \left(âˆ‘_â‡‰Z p(â‡‰X, â‡‰Z; â†’w)\right)$$
with respect to $â†’w$.

~~~
Usually, the latent variables $â‡‰Z$ indicate membership of the data in one of the
set of groups.

~~~
The main idea is to replace the computation of the logarithm of the sum over all
latent variable values by the expectation of a logarithm of the joint
probability under the posterior latent variable distribution $p(â‡‰Z | â‡‰X; â†’w)$.

---
class: dbend
# EM Algorithm

<div class="algorithm">

- Initialize the parameters $â†’w^\mathrm{new}$.

~~~
- Repeat until convergence (or until patience runs out):
  - $â†’w^\mathrm{old} â† â†’w^\mathrm{new}$
  - **E step**. Evaluate
    $$Q\big(â†’w | â†’w^\mathrm{old}\big) = ğ”¼_{â‡‰Z|â‡‰X, â†’w^\mathrm{old}} \big[\log p(â‡‰X, â‡‰Z; â†’w)\big].$$
  - **M step**. Maximize the log-likelihood by computing
    $$â†’w^\mathrm{new} â† \argmax_â†’w Q\big(â†’w | â†’w^\mathrm{old}\big).$$
</div>

---
class: dbend
# EM Algorithm â€“ Proof

The EM algorithm increases $Q\big(â†’w | â†’w^\mathrm{old}\big)$ on every step, and we now
prove that its increase also causes the $\log p(â‡‰X; â†’w)$ to increase, converging
to a local optimum.

~~~
First note that for any $â‡‰Z$ with nonzero probability, we can write
$$\log p(â‡‰X; â†’w) = \log p(â‡‰X, â‡‰Z; â†’w) - \log p(â‡‰Z|â‡‰X; â†’w).$$

~~~
Computing the expectation with respect to $p(â‡‰Z|â‡‰X, â†’w^\mathrm{old})$, we get
$$\begin{aligned}
  \log p(â‡‰X; â†’w)
  &= âˆ‘_â‡‰Z p(â‡‰Z|â‡‰X, â†’w^\mathrm{old}) \log p(â‡‰X, â‡‰Z; â†’w) - âˆ‘_â‡‰Z p(â‡‰Z|â‡‰X, â†’w^\mathrm{old}) \log p(â‡‰Z|â‡‰X; â†’w) \\
  &= Q\big(â†’w | â†’w^\mathrm{old}\big) + H\big(p(â‡‰Z|â‡‰X; â†’w^\mathrm{old}), p(â‡‰Z|â‡‰X; â†’w)\big).
\end{aligned}$$

~~~
The above equation holds for any $â†’w$, so also for $â†’w^\mathrm{old}$:
$$\log p(â‡‰X; â†’w^\mathrm{old}) = Q\big(â†’w^\mathrm{old} | â†’w^\mathrm{old}\big) + H\big(p(â‡‰Z|â‡‰X; â†’w^\mathrm{old}), p(â‡‰Z|â‡‰X; â†’w^\mathrm{old})\big).$$

---
class: dbend
# EM Algorithm â€“ Proof

Subtracting the second term $\log p(â‡‰X; â†’w^\mathrm{old})$ from the first $\log p(â‡‰X; â†’w)$, we obtain
$$\begin{aligned}
  & \log p(â‡‰X; â†’w) - \log p(â‡‰X; â†’w^\mathrm{old}) \\
  &= Q\big(â†’w | â†’w^\mathrm{old}\big) - Q\big(â†’w^\mathrm{old} | â†’w^\mathrm{old}\big)
   + H\big(p(â‡‰Z|â‡‰X; â†’w^\mathrm{old}), p(â‡‰Z|â‡‰X; â†’w)\big) - H\big(p(â‡‰Z|â‡‰X; â†’w^\mathrm{old})\big) \\
  &= Q\big(â†’w | â†’w^\mathrm{old}\big) - Q\big(â†’w^\mathrm{old} | â†’w^\mathrm{old}\big)
   + D_\mathrm{KL}\big(p(â‡‰Z|â‡‰X; â†’w^\mathrm{old}) || p(â‡‰Z|â‡‰X; â†’w)\big).
\end{aligned}$$

~~~
Given that KL divergence is non-negative, we get
$$\log p(â‡‰X; â†’w) - \log p(â‡‰X; â†’w^\mathrm{old}) â‰¥ Q\big(â†’w | â†’w^\mathrm{old}\big) - Q\big(â†’w^\mathrm{old} | â†’w^\mathrm{old}\big),$$
and therefore, if we increase $Q\big(â†’w | â†’w^\mathrm{old}\big)$, we also increase
the $\log p(â‡‰X; â†’w)$.

---
section: BiasVariance Trade-off
# Bias-Variance Trade-off

Consider a model $y(â†’x)$ solving a regression problem with MSE loss
$$ğ“› = ğ”¼_{â†’x,t} \big[(y(â†’x) - t)^2\big].$$

~~~
Denoting $g(â†’x) â‰ ğ”¼_{t|â†’x} \big[t\big]$, we can rewrite $\big(y(â†’x) - t\big)^2$ as
$$\begin{aligned}
  \big(y(â†’x) - t\big)^2
  & = \big(y(â†’x) - g(â†’x) + g(â†’x) - t\big)^2 \\
  & = \big(y(â†’x) - g(â†’x)\big)^2 + 2 \big(y(â†’x) - g(â†’x)\big)\big(g(â†’x) - t\big) + \big(g(â†’x) - t\big)^2.
\end{aligned}$$

~~~
When computing an expectation with respect to $p_\mathrm{data}(â†’x, t)$, we
obtain
$$\begin{aligned}
  ğ“›
  & = ğ”¼_{â†’x,t}\big[(y(â†’x) - g(â†’x))^2\big] + 2 ğ”¼_{â†’x,t}\big[(y(â†’x) - g(â†’x))(g(â†’x) - t)\big] + ğ”¼_{â†’x,t}\big[(g(â†’x) - t)^2\big] \\
  & = ğ”¼_{â†’x,t}\big[(y(â†’x) - g(â†’x))^2\big] + ğ”¼_{â†’x,t}\big[(g(â†’x) - t)^2\big],
\end{aligned}$$
because $ğ”¼_{t|â†’x}\big[g(â†’x) - t\big] = 0$.

---
# Bias-Variance Trade-off

We have decomposed the loss into two components, where the second is the â€œlabel
noiseâ€ called **irreducible error**.

~~~
We now further decompose the first component
$ğ”¼_{â†’x,t}\big[(y(â†’x) - g(â†’x))^2\big]$.

~~~
Assuming $ğ““$ is a dataset obtained from the data generating distribution, we
denote the prediction of a model trained using this dataset as $y(â†’x; ğ““)$.

~~~
$$\begin{aligned}
  \big(y(â†’x; ğ““) - g(â†’x)\big)^2
  & = \big(y(â†’x; ğ““) - ğ”¼_ğ““\big[y(â†’x; ğ““)\big] + ğ”¼_ğ““\big[y(â†’x; ğ““)\big] - g(â†’x)\big)^2 \\
  & = \big(y(â†’x; ğ““) - ğ”¼_ğ““\big[y(â†’x; ğ““)\big]\big)^2 \\
  &+ 2\big(y(â†’x; ğ““) - ğ”¼_ğ““\big[y(â†’x; ğ““)\big]\big)\big(ğ”¼_ğ““\big[y(â†’x; ğ““)\big] - g(â†’x)\big) \\
  &+ \big(ğ”¼_ğ““\big[y(â†’x; ğ““)\big] - g(â†’x)\big)^2.
\end{aligned}$$

~~~
Note that $ğ”¼_ğ““\big[y(â†’x; ğ““) - ğ”¼_ğ““[y(â†’x; ğ““)]\big]=0$, therefore,
$$ğ”¼_ğ““\big[(y(â†’x; ğ““) - g(â†’x))^2\big] = ğ”¼_ğ““\big[(y(â†’x; ğ““) - ğ”¼_ğ““[y(â†’x; ğ““)])^2\big] + ğ”¼_ğ““\big[(ğ”¼_ğ““[y(â†’x; ğ““)] - g(â†’x))^2].$$

---
# Bias-Variance Trade-off

Putting all the parts together, we get that
$$\begin{aligned}
  ğ”¼_ğ““\big[ğ“›]
  & = ğ”¼_ğ““\big[(y(â†’x; ğ““) - t)^2\big] \\
  & = \underbrace{ğ”¼_ğ““\big[(ğ”¼_ğ““[y(â†’x; ğ““)] - g(â†’x))^2]}_{\mathrm{bias}^2}
    + \underbrace{ğ”¼_ğ““\big[(y(â†’x; ğ““) - ğ”¼_ğ““[y(â†’x; ğ““)])^2\big]}_{\mathrm{variance}}
    + \underbrace{ğ”¼_ğ““\big[(g(â†’x) - t)^2\big]}_{\mathrm{irreducible~error}}
\end{aligned}$$

~~~
This is the so-called **bias-variance trade-off**, showing that the expected loss
decomposes into the three above components.

~~~
For classification problems, we can use the same decomposition on MSE of the
probabilities, and it is also possible to derive an analogy using the so-called
0-1 loss (see _A Unified Bias-Variance Decomposition_ by P. Domingos for the
exact formulation).

~~~
This decomposition has been long
interpreted as:

<div class="algorithm"

_The price to pay for achieving low bias is high variance._
</div>

---
# Bias-Variance Trade-off

![w=45%](bias_variance_lr_examples.svgz)
~~~
![w=53%](bias_variance_lr_error.svgz)

---
# Bias-Variance Trade-off

![w=45%,h=center](bias_variance_tradeoff.png)
~~~
![w=60%,h=center](bias_vs_variance.svgz)

---
# Bias-Variance Trade-off

For a k-NN search, the loss can be decomposed exactly as
$$ğ”¼\big[(y(â†’x) - t(â†’x))^2\big] = \left(t(â†’x) - \frac{1}{K} âˆ‘_{k=1}^K t\big(N_k(â†’x)\big)\right)^2 + \frac{Ïƒ^2}{K} + Ïƒ^2,$$
where $N_k(â†’x)$ is the k-nearest neighbor of $â†’x$ and $Ïƒ^2$ is the irreducible
error.

---
# Bias-Variance Trade-off

![w=64%,h=center](bias_variance_digits.svgz)

Quoting from _Neural Networks and the Bias/Variance Decomposition_ by S. Geman, 1992:

> _The basic trend is what we expect: bias falls and variance increases with the
> number of hidden units. The effects are not perfectly demonstrated (notice,
> for example, the dip in variance in the experiments with the largest numbers
> of hidden units), presumably because the phenomenon of overfitting is
> complicated by convergence issues and perhaps also by our decision to stop the
> training prematurely._

---
# Bias-Variance Trade-off

However, in past years, neural networks with increasing capacity have performed
better and better.

![w=100%](bias_variance_hypothesis.svgz)

---
section: DoubleDescent
# Double Descent

![w=100%](double_descent.svgz)

---
# Double Descent

![w=85%,h=center](relu_smooth.svgz)

---
# Double Descent

![w=69%,h=center](double_descent_rff.svgz)

---
# Double Descent

![w=46%](double_descent_mnist.svgz)![w=54%](double_descent_rf.svgz)

---
section: DeepDoubleDescent
# Deep Double Descent

![w=100%,v=middle](deep_double_descent.svgz)

---
# Deep Double Descent

![w=100%,v=middle](deep_double_descent_size_epochs.svgz)

---
# Deep Double Descent

![w=90%,h=center](deep_double_descent_width.svgz)
