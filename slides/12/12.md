title: NPFL129, Lecture 12
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Gaussian Mixture,<br>Bias-Variance Tradeoff

## Milan Straka

### December 21, 2020

---
section: GaussianMixture
# Gaussian Mixture

Let $â†’x_1, â†’x_2, â€¦, â†’x_N$ be a collection of $N$ input examples, each being
a $D$-dimensional vector $â†’x_i âˆˆ â„^D$. Let $K$, the number of target clusters,
be given.

~~~
Our goal is to represent the data as a Gaussian mixture, which is a combination
of $K$ Gaussian in the form
$$p(â†’x) = âˆ‘_{k=1}^K Ï€_k ğ“(â†’x | â†’Î¼_k, â‡‰Î£_k).$$
Therefore, each cluster is parametrized as $ğ“(â†’x | â†’Î¼_k, â‡‰Î£_k)$.

~~~
Let $â†’z âˆˆ \{0, 1\}^K$ be a $K$-dimensional random variable, such that exactly
one $z_k$ is 1, denoting to which cluster a training example belongs.
~~~
Let the marginal distribution of $z_k$ be
$$p(z_k = 1) = Ï€_k.$$

~~~
Therefore, $p(â†’z) = âˆ_k Ï€_k^{z_k}$.

---
# Gaussian Mixture

![w=6%,f=right](mog_latent.svgz)

We can write
$$p(â†’x) = âˆ‘_{â†’z} p(â†’z) p(â†’x | â†’z) = âˆ‘_{k=1}^K Ï€_k ğ“(â†’x | â†’Î¼_k, â‡‰Î£_k)$$
~~~
and the probability of the whole clustering is therefore
$$\log p(â‡‰X | â†’Ï€, â†’Î¼, â†’Î£) = âˆ‘_{i=1}^N \log \left(âˆ‘_{k=1}^K Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)\right).$$

~~~
To fit a Gaussian mixture model, we start with maximum likelihood estimation and
minimize
$$ğ“›(â‡‰X) = - âˆ‘_i \log âˆ‘_{k=1}^K Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)$$

---
# Gaussian Mixture

The derivative of the loss with respect to $â†’Î¼_k$ gives
$$\frac{âˆ‚ğ“›(â‡‰X)}{âˆ‚â†’Î¼_k} = - âˆ‘_i \frac{Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i | â†’Î¼_l, â‡‰Î£_l)} â‡‰Î£_k^{-1} \big(â†’x_i - â†’Î¼_k\big)$$

~~~
Denoting $r(z_{i,k}) = \frac{Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i | â†’Î¼_l, â‡‰Î£_l)}$,
setting the derivative equal to zero and multiplying by $â‡‰Î£_k^{-1}$, we get
$$â†’Î¼_k = \frac{âˆ‘_i r(z_{i,k}) â†’x_i}{âˆ‘_i r(z_{i,k})}.$$

~~~
The $r(z_{i,k})$ are usually called **responsibilities** and denote the
probability $p(z_k = 1 | â†’x_i)$. Note that the responsibilities depend on
$â†’Î¼_k$, so the above equation is not an analytical solution for $â†’Î¼_k$, but
can be used as an _iterative_ algorithm for converting to local optimum.

---
# Gaussian Mixture

For $â‡‰Î£_k$, we again compute the derivative of the loss, which is technically
complicated (we need to compute derivative with respect a matrix, and also we
need to differentiate matrix determinant) and results in an analogous equation
$$â†’Î£_k = \frac{âˆ‘_i r(z_{i,k}) (â†’x_i - â†’Î¼_k) (â†’x_i - â†’Î¼_k)^T}{âˆ‘_i r(z_{i,k})}.$$

~~~
To minimize the loss with respect to $â†’Ï€$, we need to include the constraint
$âˆ‘_k Ï€_k = 1$, so we form a Lagrangian $ğ“›(â‡‰X) + Î»\left(âˆ‘\nolimits_k Ï€_k - 1\right)$,
and get
$$0 = âˆ‘_i \frac{ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i | â†’Î¼_l, â‡‰Î£_l)} + Î»,$$
~~~
from which we get $Ï€_k âˆ âˆ‘_i r(z_{i,k})$ and therefore
$$Ï€_k = 1/N â‹… âˆ‘\nolimits_i r(z_{i,k}).$$

---
# Gaussian Mixture

<div class="algorithm">

**Input**: Input points $â†’x_1, â€¦, â†’x_N$, number of clusters $K$.

- Initialize $â†’Î¼_k, â‡‰Î£_k$ and $Ï€_k$. It is common to start by running the
  K-Means algorithm to obtain $z_{i,k}$, set $r(z_{i,k}) â† z_{i,k}$
  and use the **M step** below.

- Repeat until convergence (or until patience runs out):
  - **E step**. Evaluate the responsibilities as
    $$r(z_{i,k}) = \frac{Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i | â†’Î¼_l, â‡‰Î£_l)}.$$
  - **M step**. Maximize the log-likelihood by setting

    $$â†’Î¼_k = \frac{âˆ‘_i r(z_{i,k}) â†’x_i}{âˆ‘_i r(z_{i,k})},~~
      â†’Î£_k = \frac{âˆ‘_i r(z_{i,k}) (â†’x_i - â†’Î¼_k) (â†’x_i - â†’Î¼_k)^T}{âˆ‘_i r(z_{i,k})},~~
      Ï€_k = \frac{âˆ‘_i r(z_{i,k})}{N}.$$

</div>

---
# Gaussian Mixture

![w=75%,h=center](../11/mog_data.svgz)
![w=75%,h=center](../11/mog_illustration.svgz)

---
# Gaussian Mixture

![w=75%,h=center](mog_example.svgz)

---
section: EM
# EM Algorithm

The algorithm for estimating the Gaussian mixture is an example of an **EM
algorithm**.

~~~
The **EM algorithm** algorithm can be used when given a a joint distribution
$p(â‡‰X, â‡‰Z | â†’w)$ over observed variables $â‡‰X$ and latent (hidden, unseen)
variabled $â‡‰Z$, parametried by $â†’w$,
~~~
we maximize
$$\log p(â‡‰X; â†’w) = \log \left(âˆ‘_â‡‰Z p(â‡‰X, â‡‰Z; â†’w)\right)$$
with respect to $â†’w$.

~~~
The main idea is to approximate the sum over all latent variables by
the expectation of the joint probability under the posterior latent
variable distribution $p(â‡‰Z | â‡‰X; â†’w)$.

~~~
Usually, the latent variables $â‡‰Z$ indicate membership of the data in one of the
set of groups.

---
# EM Algorithm

<div class="algorithm">

- Initialize the parameters $â†’w^\mathrm{old}$.

- Repeat until convergence (or until patience runs out):
  - **E step**. Evaluate
    $$Q(â†’w | â†’w^\mathrm{old}) = ğ”¼_{â‡‰Z|â‡‰X, â†’w} \big[\log p(â‡‰X, â‡‰Z; â†’w)\big].$$
  - **M step**. Maximize the log-likelihood by computing
    $$â†’w^\mathrm{new} â† \argmax_â†’w Q(â†’w | â†’w^\mathrm{old}).$$
</div>
