title: NPFL129, Lecture 12
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Gaussian Mixture, EM Algorithm, Bias-Variance Trade-off

## Milan Straka

### December 19, 2022

---
# K-Means Clustering

<div class="algorithm">

**Input**: Input points $â†’x_1, â€¦, â†’x_N$, number of clusters $K$.

- Initialize $â†’Î¼_1, â€¦, â†’Î¼_K$ as $K$ random input points.

- Repeat until convergence (or until patience runs out):
  - Compute the best possible $z_{i, k}$. It is easy to see that the smallest $J$
    is achieved by
    $$z_{i,k} = \begin{cases} 1 & \textrm{~~if~}k = \argmin\nolimits_j \|â†’x_i - â†’Î¼_j\|^2, \\
                              0 & \textrm{~~otherwise}.\end{cases}$$

  - Compute the best possible $â†’Î¼_k = \argmin\nolimits_{â†’Î¼} âˆ‘_i z_{i,k} \|â†’x_i-â†’Î¼\|^2$.
   By computing a derivative with respect to $â†’Î¼$, we get
   $$â†’Î¼_k = \frac{âˆ‘_i z_{i,k} â†’x_i}{âˆ‘_i z_{i,k}}.$$
</div>

---
# K-Means Clustering

![w=55%,h=center](../11/kmeans_example.svgz)

---
# Gaussian Mixture vs K-Means

It could be useful to consider that different clusters might have different
radii or even be ellipsoidal.

![w=100%,h=center](../11/mog_kmeans_comparison.svgz)

---
section: MultivariateGaussian
# Multivariate Gaussian Distribution

Recall that
$$ğ“(x; Î¼, Ïƒ^2) = \sqrt{\frac{1}{2Ï€Ïƒ^2}} \exp \left(-\frac{(x - Î¼)^2}{2Ïƒ^2}\right).$$

~~~
For $D$-dimensional vector $â†’x$, the multivariate Gaussian distribution takes
the form
$$ğ“(â†’x; â†’Î¼, â‡‰Î£) â‰ \frac{1}{\sqrt{(2Ï€)^D |â‡‰Î£|}} \exp \left(-\frac{1}{2}(â†’x-â†’Î¼)^T â‡‰Î£^{-1} (â†’x-â†’Î¼) \right).$$

~~~
The biggest difference compared to the single-dimensional case is the _covariance
matrix_ $â‡‰Î£$, which is (in the non-degenerate case, which is the only one
considered here) a _symmetric positive-definite matrix_ of size $D Ã— D$.

---
# Multivariate Gaussian Distribution

If the covariance matrix is an identity, then the multivariate Gaussian
distribution simplifies to
$$ğ“(â†’x; â†’Î¼, â‡‰I) = \frac{1}{\sqrt{(2Ï€)^D}} \exp \left(-\frac{1}{2}(â†’x - â†’Î¼)^T (â†’x - â†’Î¼)\right).$$

~~~
![w=45%,f=right](multivariate_gaussian_plot_eye.svgz)

We can rewrite the exponent in this case as
$$ğ“(â†’x; â†’Î¼, â‡‰I) âˆ \exp \left(-\frac{\|â†’x - â†’Î¼\|^2}{2}\right).$$

Therefore, the constant surfaces are concentric hyperspheres (circles in 2D,
spheres in 3D) centered at the mean $â†’Î¼$.

~~~
The same holds if the covariance is $Ïƒ^2 â‡‰I$, only the hyperspheres' diameter
changes.

---
# Multivariate Gaussian Distribution

Now consider a diagonal covariance matrix $â‡‰Î›$. The exponent then simplifies to

$$ğ“(â†’x; â†’Î¼, â‡‰Î›) âˆ \exp \left(- âˆ‘\nolimits_i \frac{1}{2â‡‰Î›_{i,i}} \big(â†’x_i - â†’Î¼_i\big)^2\right).$$

~~~
![w=45%,f=right](multivariate_gaussian_plot_diag.svgz)

The constant surfaces in this case are axis-aligned hyperellipsoids (ellipses in
2D, ellipsoids in 3D) centered at the mean $â†’Î¼$ with the size of the axes
depending on the corresponding diagonal entries in the covariance matrix.

---
# Multivariate Gaussian Distribution

In the general case of a full covariance matrix, the fact that it is positive
definite implies it has real positive _eigenvalues_ $Î»_i$. Considering the
corresponding eigenvectors $â†’u_i$, it can be shown that the constant
surfaces are again hyperellipsoids centered at $â†’Î¼$, but this time rotated so that
their axes are the eigenvectors $â†’u_i$ with sizes $Î»_i^{1/2}$.

![w=48%](multivariate_gaussian_elipsoids.svgz)![w=88%,mw=50%,h=center](multivariate_gaussian_plot_full.svgz)

---
# Multivariate Gaussian Distribution

Generally, we can rewrite a positive-definite matrix $â‡‰Î£$ as $â‡‰U â‡‰Î› â‡‰U^T = (â‡‰Uâ‡‰Î›^{1/2})(â‡‰Uâ‡‰Î›^{1/2})^T$,
and then
$$â†’x âˆ¼ ğ“(â†’Î¼, â‡‰Î£) \iff â†’x âˆ¼ â†’Î¼ + â‡‰Uâ‡‰Î›^{1/2} ğ“(0, â‡‰I).$$

~~~
Therefore, when sampling from a distribution with a full covariance matrix, we
can sample from a standard multivariate $ğ“(0, â‡‰I)$, scale by the eigenvalues of
the covariance matrix, rotate according to the eigenvectors of the covariance
matrix and finally shift by $â†’Î¼$.

~~~
![w=48%,f=right](multivariate_gaussian_covariance.svgz)

Note that different forms of covariance allow more generality, but also
require more parameters:
- the $Ïƒ^2 â‡‰I$ has a single parameter,
- the $â‡‰Î›$ has $D$ parameters,
- the full covariance matrix $â‡‰Î£$ has $\binom{D+1}{2}$ parameters, i.e., $Î˜(D^2)$.

---
section: GaussianMixture
# Gaussian Mixture

Let $â†’x_1, â†’x_2, â€¦, â†’x_N$ be a collection of $N$ input examples, each being
a $D$-dimensional vector $â†’x_i âˆˆ â„^D$. Let $K$, the number of target clusters,
be given.

~~~
Our goal is to represent the data as a Gaussian mixture, which is a combination
of $K$ Gaussians in the form
$$p(â†’x) = âˆ‘_{k=1}^K Ï€_k ğ“(â†’x; â†’Î¼_k, â‡‰Î£_k).$$
Therefore, each cluster is parametrized as $ğ“(â†’x; â†’Î¼_k, â‡‰Î£_k)$.

---
# Gaussian Mixture

![w=75%,h=center](mog_data.svgz)
![w=75%,h=center](mog_illustration.svgz)

--- --
Let $â†’z âˆˆ \{0, 1\}^K$ be a $K$-dimensional random variable, such that exactly
one $z_k$ is 1, denoting to which cluster a training example belongs.
~~~
Let the marginal distribution of $z_k$ be
$$p(z_k = 1) = Ï€_k,$$
so that the priors $Ï€_k$ represent the â€œfertilityâ€ of the clusters.
~~~
Then, $p(â†’z) = âˆ_k Ï€_k^{z_k}$.

---
# Gaussian Mixture

![w=6%,f=right](mog_latent.svgz)

We can write
$$p(â†’x) = âˆ‘_{â†’z} p(â†’x, â†’z) = âˆ‘_{â†’z} p(â†’z) p(â†’x | â†’z) = âˆ‘_{k=1}^K Ï€_k ğ“(â†’x; â†’Î¼_k, â‡‰Î£_k),$$
~~~
and the log-probability of the whole clustering is therefore
$$\log p(â‡‰X; â†’Ï€, â†’Î¼, â‡‰Î£) = âˆ‘_{i=1}^N \log \left(âˆ‘_{k=1}^K Ï€_k ğ“(â†’x_i; â†’Î¼_k, â‡‰Î£_k)\right).$$

~~~
To fit a Gaussian mixture model, we utilize maximum likelihood estimation and
minimize
$$L(â†’Ï€, â†’Î¼, â‡‰Î£; â‡‰X) = - âˆ‘_i \log âˆ‘_{k=1}^K Ï€_k ğ“(â†’x_i; â†’Î¼_k, â‡‰Î£_k).$$

---
# Gaussian Mixture

The derivative of the loss with respect to $â†’Î¼_k$ gives
$$\frac{âˆ‚L(â†’Ï€, â†’Î¼, â‡‰Î£; â‡‰X)}{âˆ‚â†’Î¼_k} = - âˆ‘_i \frac{Ï€_k ğ“(â†’x_i; â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i; â†’Î¼_l, â‡‰Î£_l)} â‡‰Î£_k^{-1} \big(â†’x_i - â†’Î¼_k\big).$$

~~~
Denoting $r(z_{i,k}) = \frac{Ï€_k ğ“(â†’x_i; â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i; â†’Î¼_l, â‡‰Î£_l)}$,
setting the derivative equal to zero and multiplying by $â‡‰Î£_k$, we get
$$â†’Î¼_k = \frac{âˆ‘_i r(z_{i,k}) â†’x_i}{âˆ‘_i r(z_{i,k})}.$$

~~~
The $r(z_{i,k})$ are usually called **responsibilities** and denote the
probability $p(z_k = 1 | â†’x_i)$. Note that the responsibilities depend on
$â†’Î¼_k$, so the above equation is not an analytical solution for $â†’Î¼_k$, but
can be used as an _iterative_ algorithm for converging to a local optimum.

---
# Gaussian Mixture

For $â‡‰Î£_k$, we again compute the derivative of the loss, which is technically
complicated (we need to compute a derivative of a matrix inverse, and also we
need to differentiate a matrix determinant) and results in an analogous equation:
$$â‡‰Î£_k = \frac{âˆ‘_i r(z_{i,k}) (â†’x_i - â†’Î¼_k) (â†’x_i - â†’Î¼_k)^T}{âˆ‘_i r(z_{i,k})}.$$

~~~
To minimize the loss with respect to $â†’Ï€$, we need to include the constraint
$âˆ‘_k Ï€_k = 1$, so we form a Lagrangian $ğ“›(â†’Ï€) = L(â†’Ï€, â†’Î¼, â‡‰Î£; â‡‰X) - Î»\left(âˆ‘\nolimits_k Ï€_k - 1\right)$,
and get
$$\frac{âˆ‚ğ“›(â†’Ï€)}{âˆ‚Ï€_k} = âˆ‘_i \frac{ğ“(â†’x_i; â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i; â†’Î¼_l, â‡‰Î£_l)} - Î».$$
~~~
Setting the derivative to zero and multiplying by $Ï€_k$, we obtain $Ï€_k = \frac{1}{Î»} â‹… âˆ‘_i r(z_{i,k})$, so
$$Ï€_k = 1/N â‹… âˆ‘\nolimits_i r(z_{i,k}).$$

---
# Gaussian Mixture

<div class="algorithm">

**Input**: Input points $â†’x_1, â€¦, â†’x_N$, number of clusters $K$.

- Initialize $â†’Î¼_k, â‡‰Î£_k$ and $Ï€_k$. It is common to start by running the
  K-Means algorithm to obtain $z_{i,k}$, set $r(z_{i,k}) â† z_{i,k}$
  and use the **M step** below.

~~~
- Repeat until convergence (or until patience runs out):
  - **E step**. Evaluate the responsibilities as
    $$r(z_{i,k}) = \frac{Ï€_k ğ“(â†’x_i; â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i; â†’Î¼_l, â‡‰Î£_l)} \textcolor{gray}{= p(z_k = 1|â†’x_i, â†’Î¼, â‡‰Î£, â†’Ï€)}.$$
~~~
  - **M step**. Maximize the log-likelihood by setting

    $$â†’Î¼_k = \frac{âˆ‘_i r(z_{i,k}) â†’x_i}{âˆ‘_i r(z_{i,k})},~~
      â‡‰Î£_k = \frac{âˆ‘_i r(z_{i,k}) (â†’x_i - â†’Î¼_k) (â†’x_i - â†’Î¼_k)^T}{âˆ‘_i r(z_{i,k})},~~
      Ï€_k = \frac{âˆ‘_i r(z_{i,k})}{N}.$$

</div>

---
# Gaussian Mixture

![w=75%,h=center](mog_example.svgz)

---
class: dbend
section: EM
# EM Algorithm

The algorithm for estimating the Gaussian mixture is an example of an **EM
algorithm**.

~~~
The **EM algorithm** algorithm can be used when given a joint distribution
$p(â‡‰X, â‡‰Z; â†’w)$ over observed variables $â‡‰X$ and latent (hidden, unseen)
variables $â‡‰Z$, parametrized by $â†’w$,
~~~
we maximize
$$\log p(â‡‰X; â†’w) = \log \left(âˆ‘_{â‡‰Z} p(â‡‰X, â‡‰Z; â†’w)\right)$$
with respect to $â†’w$.

~~~
Usually, the latent variables $â‡‰Z$ indicate membership of the data in one of the
set of groups.

~~~
The main idea is to replace the computation of the logarithm of the sum over all
latent variable values by the expectation of a logarithm of the joint
probability under the posterior latent variable distribution $p(â‡‰Z | â‡‰X; â†’w)$.

---
class: dbend
# EM Algorithm

<div class="algorithm">

- Initialize the parameters $â†’w^\mathrm{new}$.

~~~
- Repeat until convergence (or until patience runs out):
  - $â†’w^\mathrm{old} â† â†’w^\mathrm{new}$
  - **E step**. Evaluate
    $$Q\big(â†’w | â†’w^\mathrm{old}\big) = ğ”¼_{â‡‰Z|â‡‰X, â†’w^\mathrm{old}} \big[\log p(â‡‰X, â‡‰Z; â†’w)\big].$$
  - **M step**. Maximize the log-likelihood by computing
    $$â†’w^\mathrm{new} â† \argmax_{â†’w} Q\big(â†’w | â†’w^\mathrm{old}\big).$$
</div>

---
class: dbend
# EM Algorithm â€“ Proof

The EM algorithm updates $â†’w$ to maximize $Q\big(â†’w | â†’w^\mathrm{old}\big)$ on every step, and we now
prove that this update of weights also causes the $\log p(â‡‰X; â†’w)$ to increase.

~~~
First note that for any $â‡‰Z$ with nonzero probability, we can write
$$\log p(â‡‰X; â†’w) = \log p(â‡‰X, â‡‰Z; â†’w) - \log p(â‡‰Z|â‡‰X; â†’w).$$

~~~
Computing the expectation with respect to $p(â‡‰Z|â‡‰X, â†’w^\mathrm{old})$, we get
$$\begin{aligned}
  \log p(â‡‰X; â†’w)
  &= âˆ‘_{â‡‰Z} p(â‡‰Z|â‡‰X; â†’w^\mathrm{old}) \log p(â‡‰X, â‡‰Z; â†’w) - âˆ‘_{â‡‰Z} p(â‡‰Z|â‡‰X; â†’w^\mathrm{old}) \log p(â‡‰Z|â‡‰X; â†’w) \\
  &= Q\big(â†’w | â†’w^\mathrm{old}\big) + H\big(p(â‡‰Z|â‡‰X; â†’w^\mathrm{old}), p(â‡‰Z|â‡‰X; â†’w)\big).
\end{aligned}$$

~~~
The above equation holds for any $â†’w$, so also for $â†’w^\mathrm{old}$:
$$\log p(â‡‰X; â†’w^\mathrm{old}) = Q\big(â†’w^\mathrm{old} | â†’w^\mathrm{old}\big) + H\big(p(â‡‰Z|â‡‰X; â†’w^\mathrm{old}), p(â‡‰Z|â‡‰X; â†’w^\mathrm{old})\big).$$

---
class: dbend
# EM Algorithm â€“ Proof

Subtracting the second term $\log p(â‡‰X; â†’w^\mathrm{old})$ from the first $\log p(â‡‰X; â†’w)$, we obtain
$$\begin{aligned}
  & \log p(â‡‰X; â†’w) - \log p(â‡‰X; â†’w^\mathrm{old}) \\
  &= Q\big(â†’w | â†’w^\mathrm{old}\big) - Q\big(â†’w^\mathrm{old} | â†’w^\mathrm{old}\big)
   + H\big(p(â‡‰Z|â‡‰X; â†’w^\mathrm{old}), p(â‡‰Z|â‡‰X; â†’w)\big) - H\big(p(â‡‰Z|â‡‰X; â†’w^\mathrm{old})\big) \\
  &= Q\big(â†’w | â†’w^\mathrm{old}\big) - Q\big(â†’w^\mathrm{old} | â†’w^\mathrm{old}\big)
   + D_\mathrm{KL}\big(p(â‡‰Z|â‡‰X; â†’w^\mathrm{old}) \big\| p(â‡‰Z|â‡‰X; â†’w)\big).
\end{aligned}$$

~~~
Given that KL divergence is nonnegative, we get
$$\log p(â‡‰X; â†’w) - \log p(â‡‰X; â†’w^\mathrm{old}) â‰¥ Q\big(â†’w | â†’w^\mathrm{old}\big) - Q\big(â†’w^\mathrm{old} | â†’w^\mathrm{old}\big),$$
so if $\argmax\nolimits_{â†’w} Q\big(â†’w | â†’w^\mathrm{old}\big)$ is larger than
$Q\big(â†’w^\mathrm{old} | â†’w^\mathrm{old}\big)$, we also increase
$\log p(â‡‰X; â†’w)$.

~~~
To show that $\log p(â‡‰X; â†’w)$ actually converges to a stationary point, some
additional regularity conditions are needed (one possibility is to require $Q(â†’w
| â†’w^\mathrm{old})$ to be continuous in both $â†’w$ and $â†’w^\mathrm{old}$). For
a more detailed treatment, see the 1983 paper _On the Convergence Properties of
the EM Algorithm_ by C. F. Jeff Wu.

---
section: BiasVariance Trade-off
# Bias-Variance Trade-off

Consider a model $y(â†’x)$ solving a regression problem with MSE loss
$$L = ğ”¼_{â†’x,t} \big[(y(â†’x) - t)^2\big].$$

~~~
Denoting $g(â†’x) â‰ ğ”¼_{t|â†’x} \big[t\big]$, we can rewrite $\big(y(â†’x) - t\big)^2$ as
$$\begin{aligned}
  \big(y(â†’x) - t\big)^2
  & = \big(y(â†’x) - g(â†’x) + g(â†’x) - t\big)^2 \\
  & = \big(y(â†’x) - g(â†’x)\big)^2 + 2 \big(y(â†’x) - g(â†’x)\big)\big(g(â†’x) - t\big) + \big(g(â†’x) - t\big)^2.
\end{aligned}$$

~~~
When computing an expectation with respect to $p_\mathrm{data}(â†’x, t)$, we
obtain
$$\begin{aligned}
  L
  & = ğ”¼_{â†’x,t}\big[(y(â†’x) - g(â†’x))^2\big] + 2 ğ”¼_{â†’x,t}\big[(y(â†’x) - g(â†’x))(g(â†’x) - t)\big] + ğ”¼_{â†’x,t}\big[(g(â†’x) - t)^2\big] \\
  & = ğ”¼_{â†’x,t}\big[(y(â†’x) - g(â†’x))^2\big] + ğ”¼_{â†’x,t}\big[(g(â†’x) - t)^2\big],
\end{aligned}$$
because $ğ”¼_{t|â†’x}\big[g(â†’x) - t\big] = 0$.

---
# Bias-Variance Trade-off

We have decomposed the loss into two components, where the second is the â€œlabel
noiseâ€ called **irreducible error**.

~~~
We now further decompose the first component
$ğ”¼_{â†’x,t}\big[(y(â†’x) - g(â†’x))^2\big]$.

~~~
Assuming $ğ““$ is a dataset obtained from the data-generating distribution, we
denote the prediction of a model trained using this dataset as $y(â†’x; ğ““)$.

~~~
$$\begin{aligned}
  \big(y(â†’x; ğ““) - g(â†’x)\big)^2
  & = \big(y(â†’x; ğ““) - ğ”¼_ğ““\big[y(â†’x; ğ““)\big] + ğ”¼_ğ““\big[y(â†’x; ğ““)\big] - g(â†’x)\big)^2 \\
  & = \big(y(â†’x; ğ““) - ğ”¼_ğ““\big[y(â†’x; ğ““)\big]\big)^2 \\
  &+ 2\big(y(â†’x; ğ““) - ğ”¼_ğ““\big[y(â†’x; ğ““)\big]\big)\big(ğ”¼_ğ““\big[y(â†’x; ğ““)\big] - g(â†’x)\big) \\
  &+ \big(ğ”¼_ğ““\big[y(â†’x; ğ““)\big] - g(â†’x)\big)^2.
\end{aligned}$$

~~~
Note that $ğ”¼_ğ““\big[y(â†’x; ğ““) - ğ”¼_ğ““[y(â†’x; ğ““)]\big]=0$, therefore, for a given $â†’x$, we have
$$ğ”¼_ğ““\big[(y(â†’x; ğ““) - g(â†’x))^2\big] = ğ”¼_ğ““\big[(y(â†’x; ğ““) - ğ”¼_ğ““[y(â†’x; ğ““)])^2\big] + \big(ğ”¼_ğ““[y(â†’x; ğ““)] - g(â†’x)\big)^2.$$

---
# Bias-Variance Trade-off

Putting all the parts together, we get that
$$\begin{aligned}
  ğ”¼_ğ““\big[L]
  & = ğ”¼_ğ““\big[ğ”¼_{â†’x,t}[(y(â†’x; ğ““) - t)^2]\big] \\
  & = ğ”¼_{â†’x,t}\Big[\underbrace{\big(ğ”¼_ğ““[y(â†’x; ğ““)] - g(â†’x)\big)^2}_{\mathrm{bias}^2}
    + \underbrace{ğ”¼_ğ““\big[\big(y(â†’x; ğ““) - ğ”¼_ğ““[y(â†’x; ğ““)]\big)^2\big]}_{\mathrm{variance}}
    + \underbrace{\big(g(â†’x) - t\big)^2}_{\mathrm{irreducible~error}}\Big].
\end{aligned}$$

~~~
This is the so-called **bias-variance trade-off**, showing that the expected loss
decomposes into the three above components.

~~~
For classification problems, we can use the same decomposition on the MSE of
probabilities, and it is also possible to derive an analogy using the so-called
0-1 loss (see _A Unified Bias-Variance Decomposition_ by P. Domingos for the
exact formulation).

~~~
This decomposition has been long
interpreted as:

<div class="algorithm"

_The price to pay for achieving low bias is high variance._
</div>

---
# Bias-Variance Trade-off

![w=45%](bias_variance_lr_examples.svgz)
~~~
![w=53%](bias_variance_lr_error.svgz)

---
# Bias-Variance Trade-off

![w=45%,h=center](bias_variance_tradeoff.png)
~~~
![w=60%,h=center](bias_vs_variance.svgz)

---
class: dbend
# Bias-Variance Trade-off

For a k-NN search, when we consider an expectation over all possible labelings
of a fixed training set, the MSE decomposes as
$$ğ”¼\big[(y(â†’x) - t(â†’x))^2\big] = \left(t(â†’x) - \frac{1}{K} âˆ‘_{k=1}^K t\big(N_k(â†’x)\big)\right)^2 + \frac{Ïƒ^2}{K} + Ïƒ^2,$$
where $N_k(â†’x)$ is the $k^\mathrm{th}$ nearest neighbor of $â†’x$ and $Ïƒ^2$ is the irreducible
error.

---
# Bias-Variance Trade-off

![w=64%,h=center](bias_variance_digits.svgz)

Quoting from _Neural Networks and the Bias/Variance Decomposition_ by S. Geman, 1992:

> _The basic trend is what we expect: bias falls and variance increases with the
> number of hidden units. The effects are not perfectly demonstrated (notice,
> for example, the dip in variance in the experiments with the largest numbers
> of hidden units), presumably because the phenomenon of overfitting is
> complicated by convergence issues and perhaps also by our decision to stop the
> training prematurely._

---
# Bias-Variance Trade-off

However, in past years, neural networks with increasing capacity have performed
better and better.

![w=100%](bias_variance_hypothesis.svgz)

---
section: DoubleDescent
# Double Descent

![w=100%](double_descent.svgz)

---
# Double Descent â€“ Overparametrized with Minimum L2

![w=50%,h=center](relu_smooth.svgz)
![w=100%,h=center](relu_smooth_2.svgz)

---
# Double Descent â€“ RFF on MNIST

![w=69%,h=center](double_descent_rff.svgz)

---
# Double Descent â€“ MLP and RF on MNIST

![w=46%](double_descent_mnist.svgz)![w=54%](double_descent_rf.svgz)

---
section: DeepDoubleDescent
# Deep Double Descent

![w=100%,v=middle](deep_double_descent.svgz)

---
# Deep Double Descent â€“ Effective Model Complexity

The authors define the **Effective Model Complexity** (**EMC**) of a training procedure $ğ“£$ with respect to distribution $ğ““$ and parameter $Îµ>0$
as
$$\operatorname{EMC}_{ğ““,Îµ}(ğ“£) â‰  \max \big\{n \,\big|\, ğ”¼_{Sâˆ¼ğ““^n}[\mathrm{Error}_S(ğ“£(S))] â‰¤ Îµ\big\},$$
where $\mathrm{Error}_S(M)$ is the mean error of a model $M$ on the train samples $S$.

~~~
**Hypothesis:** For any natural data distribution $ğ““$, neural-network-based
training procedure $ğ“£$, and small $Îµ>0$, if we consider the task of predicting
labels based on $n$ samples from $ğ““$, then:

- **Under-parameterized regime.** If $\operatorname{EMC}_{ğ““,Îµ}(ğ“£)$ is
  sufficiently smaller than $n$, any perturbation of $ğ“£$ that increases its
  effective complexity will decrease the test error.

~~~
- **Over-parameterized regime.** If $\operatorname{EMC}_{ğ““,Îµ}(ğ“£)$ is
  sufficiently larger than $n$, any perturbation of $ğ“£$ that increases its
  effective complexity will decrease the test error.

~~~
- **Critically parameterized regime.** If $\operatorname{EMC}_{ğ““,Îµ}(ğ“£) â‰ˆ n$,
  then a perturbation of $ğ“£$ that increases its effective complexity might
  decrease **or increase** the test error.

---
# Deep Double Descent

![w=100%,v=middle](deep_double_descent_size_epochs.svgz)

---
# Deep Double Descent

![w=90%,h=center](deep_double_descent_width.svgz)
