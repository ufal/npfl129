title: NPFL129, Lecture 12
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Gaussian Mixture

## Milan Straka

### December 20, 2021

---
# K-Means Clustering

<div class="algorithm">

**Input**: Input points $â†’x_1, â€¦, â†’x_N$, number of clusters $K$.

- Initialize $â†’Î¼_1, â€¦, â†’Î¼_K$ as $K$ random input points.

- Repeat until convergence (or until patience runs out):
  - Compute the best possible $z_{i, k}$. It is easy to see that the smallest $J$
    is achieved by
    $$z_{i,k} = \begin{cases} 1 & \textrm{~~if~}k = \argmin\nolimits_j \|â†’x_i - â†’Î¼_j\|^2, \\
                              0 & \textrm{~~otherwise}.\end{cases}$$

  - Compute the best possible $â†’Î¼_k = \argmin\nolimits_{â†’Î¼} âˆ‘_i z_{i,k} \|â†’x_i-â†’Î¼\|^2$.
   By computing a derivative with respect to $â†’Î¼$, we get
   $$â†’Î¼_k = \frac{âˆ‘_i z_{i,k} â†’x_i}{âˆ‘_i z_{i,k}}.$$
</div>

---
# K-Means Clustering

![w=55%,h=center](../11/kmeans_example.svgz)

---
# Gaussian Mixture vs K-Means

It could be useful to consider that different clusters might have different
radii or even be ellipsoidal.

![w=100%,h=center](../11/mog_kmeans_comparison.svgz)

---
section: MultivariateGaussian
# Multivariate Gaussian Distribution

Recall that
$$ğ“(x; Î¼, Ïƒ^2) = \sqrt{\frac{1}{2Ï€Ïƒ^2}} \exp \left(-\frac{(x - Î¼)^2}{2Ïƒ^2}\right).$$

~~~
For $D$-dimensional vector $â†’x$, the multivariate Gaussian distribution takes
the form
$$ğ“(â†’x | â†’Î¼, â‡‰Î£) â‰ \frac{1}{\sqrt{(2Ï€)^D |Î£|}} \exp \left(-\frac{1}{2}(â†’x-â†’Î¼)^T â‡‰Î£^{-1} (â†’x-â†’Î¼) \right).$$

~~~
The biggest difference compared to the single-dimensional case is the _covariance
matrix_ $â‡‰Î£$, which is (in the non-degenerate case, which is the only one
considered here) a _symmetric positive-definite matrix_ of size $D Ã— D$.

---
# Multivariate Gaussian Distribution

If the covariance matrix is an identity, then the multivariate Gaussian
distribution simplifies to
$$ğ“(â†’x | â†’Î¼, â‡‰I) = \frac{1}{\sqrt{(2Ï€)^D}} \exp \left(-\frac{1}{2}(â†’x - â†’Î¼)^T (â†’x - â†’Î¼)\right).$$

~~~
![w=45%,f=right](multivariate_gaussian_plot_eye.svgz)

We can rewrite the exponent in this case to
$$ğ“(â†’x | â†’Î¼, â‡‰I) âˆ \exp \left(-\frac{\|â†’x - â†’Î¼\|^2}{2}\right).$$

Therefore, the constant surfaces are concentric circles centered at the mean $â†’Î¼$.

~~~
The same holds if the covariance is $Ïƒ^2 â‡‰I$, only the circles' diameter
changes.

---
# Multivariate Gaussian Distribution

Now consider a diagonal covariance matrix $â‡‰Î›$. The exponent then simplifies to

$$ğ“(â†’x | â†’Î¼, â‡‰Î›) âˆ \exp \left(- âˆ‘\nolimits_i \frac{1}{2â‡‰Î›_{i,i}} \big(â†’x_i - â†’Î¼_i\big)^2\right).$$

~~~
![w=45%,f=right](multivariate_gaussian_plot_diag.svgz)

The constant surfaces in this case are axis-aligned ellipses centered at the
mean $â†’Î¼$ with size of the axes depending on the corresponding diagonal entries
in the covariance matrix.

---
# Multivariate Gaussian Distribution

In the general case of a full covariance matrix, the fact that it is positive
definite implies it has real positive _eigenvalues_ $Î»_i$. Considering the
corresponding eigenvectors $â†’u_i$, it can be shown that the constant
surfaces are again ellipses centered at $â†’Î¼$, but this time rotated so that
their axes are the eigenvectors $â†’u_i$ with sizes $Î»_i^{1/2}$.

![w=48%](multivariate_gaussian_elipsoids.svgz)![w=88%,mw=50%,h=center](multivariate_gaussian_plot_full.svgz)

---
# Multivariate Gaussian Distribution

Generally, we can rewrite a positive-definite matrix $â‡‰Î£$ as $(â‡‰Uâ‡‰Î›^{1/2})(â‡‰Uâ‡‰Î›^{1/2})^T$,
and then
$$â†’x âˆ¼ ğ“(â†’Î¼, â‡‰Î£) \iff â†’x âˆ¼ â†’Î¼ + â‡‰Uâ‡‰Î›^{1/2} ğ“(0, â‡‰I).$$

~~~
Therefore, when sampling from a distribution with a full covariance matrix, we
can sample from a standard multivariate $ğ“(0, â‡‰I)$, scale by the eigenvalues of
the covariance matrix, rotate according to the eigenvectors of the covariance
matrix and finally shifting by $â†’Î¼$.

~~~
![w=48%,f=right](multivariate_gaussian_covariance.svgz)

Note that different forms of covariance allows more generality, but also
requires more parameters:
- the $Ïƒ^2 â‡‰I$ has a single parameter,
- the $â‡‰Î›$ has $D$ parameters,
- the full covariance matrix $â‡‰Î£$ has $\binom{D+1}{2}$ parameters, i.e., $Î˜(D^2)$.

---
section: GaussianMixture
# Gaussian Mixture

Let $â†’x_1, â†’x_2, â€¦, â†’x_N$ be a collection of $N$ input examples, each being
a $D$-dimensional vector $â†’x_i âˆˆ â„^D$. Let $K$, the number of target clusters,
be given.

~~~
Our goal is to represent the data as a Gaussian mixture, which is a combination
of $K$ Gaussians in the form
$$p(â†’x) = âˆ‘_{k=1}^K Ï€_k ğ“(â†’x | â†’Î¼_k, â‡‰Î£_k).$$
Therefore, each cluster is parametrized as $ğ“(â†’x | â†’Î¼_k, â‡‰Î£_k)$.

---
# Gaussian Mixture

![w=75%,h=center](mog_data.svgz)
![w=75%,h=center](mog_illustration.svgz)

---
section: MoGClust
# Gaussian Mixture

Let $â†’x_1, â†’x_2, â€¦, â†’x_N$ be a collection of $N$ input examples, each being
a $D$-dimensional vector $â†’x_i âˆˆ â„^D$. Let $K$, the number of target clusters,
be given.

Our goal is to represent the data as a Gaussian mixture, which is a combination
of $K$ Gaussians in the form
$$p(â†’x) = âˆ‘_{k=1}^K Ï€_k ğ“(â†’x | â†’Î¼_k, â‡‰Î£_k).$$
Therefore, each cluster is parametrized as $ğ“(â†’x | â†’Î¼_k, â‡‰Î£_k)$.

~~~
Let $â†’z âˆˆ \{0, 1\}^K$ be a $K$-dimensional random variable, such that exactly
one $z_k$ is 1, denoting to which cluster a training example belongs.
~~~
Let the marginal distribution of $z_k$ be
$$p(z_k = 1) = Ï€_k,$$
so that the priors $Ï€_k$ represent the â€œfertilityâ€ of the clusters.
~~~
Then, $p(â†’z) = âˆ_k Ï€_k^{z_k}$.

---
# Gaussian Mixture

![w=6%,f=right](mog_latent.svgz)

We can write
$$p(â†’x) = âˆ‘_{â†’z} p(â†’z) p(â†’x | â†’z) = âˆ‘_{k=1}^K Ï€_k ğ“(â†’x | â†’Î¼_k, â‡‰Î£_k),$$
~~~
and the probability of the whole clustering is therefore
$$\log p(â‡‰X | â†’Ï€, â†’Î¼, â†’Î£) = âˆ‘_{i=1}^N \log \left(âˆ‘_{k=1}^K Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)\right).$$

~~~
To fit a Gaussian mixture model, we utilize maximum likelihood estimation and
minimize
$$ğ“›(â‡‰X) = - âˆ‘_i \log âˆ‘_{k=1}^K Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k).$$

---
# Gaussian Mixture

The derivative of the loss with respect to $â†’Î¼_k$ gives
$$\frac{âˆ‚ğ“›(â‡‰X)}{âˆ‚â†’Î¼_k} = - âˆ‘_i \frac{Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i | â†’Î¼_l, â‡‰Î£_l)} â‡‰Î£_k^{-1} \big(â†’x_i - â†’Î¼_k\big).$$

~~~
Denoting $r(z_{i,k}) = \frac{Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i | â†’Î¼_l, â‡‰Î£_l)}$,
setting the derivative equal to zero and multiplying by $â‡‰Î£_k^{-1}$, we get
$$â†’Î¼_k = \frac{âˆ‘_i r(z_{i,k}) â†’x_i}{âˆ‘_i r(z_{i,k})}.$$

~~~
The $r(z_{i,k})$ are usually called **responsibilities** and denote the
probability $p(z_k = 1 | â†’x_i)$. Note that the responsibilities depend on
$â†’Î¼_k$, so the above equation is not an analytical solution for $â†’Î¼_k$, but
can be used as an _iterative_ algorithm for converging to a local optimum.

---
# Gaussian Mixture

For $â‡‰Î£_k$, we again compute the derivative of the loss, which is technically
complicated (we need to compute a derivative of a matrix inverse, and also we
need to differentiate matrix determinant) and results in an analogous equation:
$$â†’Î£_k = \frac{âˆ‘_i r(z_{i,k}) (â†’x_i - â†’Î¼_k) (â†’x_i - â†’Î¼_k)^T}{âˆ‘_i r(z_{i,k})}.$$

~~~
To minimize the loss with respect to $â†’Ï€$, we need to include the constraint
$âˆ‘_k Ï€_k = 1$, so we form a Lagrangian $ğ“›(â‡‰X) - Î»\left(âˆ‘\nolimits_k Ï€_k - 1\right)$,
and get
$$\frac{âˆ‚ğ“›(â‡‰X)}{âˆ‚Ï€_k} = âˆ‘_i \frac{ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i | â†’Î¼_l, â‡‰Î£_l)} - Î».$$
~~~
Setting the derivative to zero and multiplying it by $Ï€_k$, we obtain $Ï€_k = \frac{1}{Î»} â‹… âˆ‘_i r(z_{i,k})$, so
$$Ï€_k = 1/N â‹… âˆ‘\nolimits_i r(z_{i,k}).$$

---
# Gaussian Mixture

<div class="algorithm">

**Input**: Input points $â†’x_1, â€¦, â†’x_N$, number of clusters $K$.

- Initialize $â†’Î¼_k, â‡‰Î£_k$ and $Ï€_k$. It is common to start by running the
  K-Means algorithm to obtain $z_{i,k}$, set $r(z_{i,k}) â† z_{i,k}$
  and use the **M step** below.

~~~
- Repeat until convergence (or until patience runs out):
  - **E step**. Evaluate the responsibilities as
    $$r(z_{i,k}) = \frac{Ï€_k ğ“(â†’x_i | â†’Î¼_k, â‡‰Î£_k)}{âˆ‘_{l=1}^K Ï€_l ğ“(â†’x_i | â†’Î¼_l, â‡‰Î£_l)}.$$
~~~
  - **M step**. Maximize the log-likelihood by setting

    $$â†’Î¼_k = \frac{âˆ‘_i r(z_{i,k}) â†’x_i}{âˆ‘_i r(z_{i,k})},~~
      â†’Î£_k = \frac{âˆ‘_i r(z_{i,k}) (â†’x_i - â†’Î¼_k) (â†’x_i - â†’Î¼_k)^T}{âˆ‘_i r(z_{i,k})},~~
      Ï€_k = \frac{âˆ‘_i r(z_{i,k})}{N}.$$

</div>

---
# Gaussian Mixture

![w=75%,h=center](mog_example.svgz)

