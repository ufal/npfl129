title: NPFL129, Lecture 6
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Kernel Methods, SVM

## Milan Straka

### November 09, 2020

---
section: KernelLR
# Kernel Linear Regression

Consider linear regression with cubic features
$$Ï†(â†’x) = \scriptsize\begin{bmatrix} 1 \\ x_1 \\ x_2 \\ â€¦ \\ x_1^2 \\ x_1x_2 \\ â€¦ \\ x_2x_1 \\ â€¦ \\ x_1^3 \\ x_1^2x_2 \\ â€¦ \end{bmatrix}.$$

~~~
The SGD update of a linear regression with batch with indices $â†’b$ is then
$$â†’w â† â†’w - \frac{Î±}{|â†’b|}âˆ‘_{i âˆˆ â†’b}\big(Ï†(â†’x_i)^T â†’w - t_i\big) Ï†(â†’x_i).$$

---
# Kernel Linear Regression

When dimensionality of input is $D$, one step of SGD takes $ğ“(D^3)$.

~~~
Surprisingly, we can do better under some circumstances. We start by
noting that we can write the parameters $â†’w$ as a linear combination
of the input features $Ï†(â†’x_i)$.

~~~
By induction, $â†’w = 0 = âˆ‘_i 0 â‹… Ï†(â†’x_i)$, and assuming $â†’w = âˆ‘_i Î²_i â‹… Ï†(â†’x_i)$,
after a SGD update we get
$$\begin{aligned}
â†’w &â† â†’w - \frac{Î±}{|â†’b|}âˆ‘_{i âˆˆ â†’b} \big(Ï†(â†’x_i)^T â†’w - t_i\big) Ï†(â†’x_i)\\
   &â† âˆ‘_i \Big(Î²_i + \big[i âˆˆ â†’b\big] â‹… \frac{Î±}{|â†’b|} \big(t_i - Ï†(â†’x_i)^T â†’w\big)\Big) Ï†(â†’x_i).
\end{aligned}$$

~~~
Every $Î²_i$ for $i âˆˆ â†’b$ changes to $Î²_i + \frac{Î±}{|â†’b|}\Big(t_i - Ï†(â†’x_i)^T â†’w\Big)$, so after
substituting for $â†’w$ we get
$$Î²_i â† Î²_i + \frac{Î±}{|â†’b|}\Big(t_i - âˆ‘\nolimits_j Î²_j Ï†(â†’x_i)^T Ï†(â†’x_j)\Big).$$

---
# Kernel Linear Regression

We can formulate an alternative linear regression algorithm (a so-called
**dual formulation**):

<div class="algorithm">

**Input**: Dataset ($â‡‰X = \{â†’x_1, â†’x_2, â€¦, â†’x_N\} âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$), learning rate $Î± âˆˆ â„^+$.<br>

- Set $Î²_i â† 0$
- Compute all values $K(â†’x_i, â†’x_j) = Ï†(â†’x_i)^T Ï†(â†’x_j)$
- Repeat until convergence
  - Sample a batch $â†’b$ (usually by generating a random permutation and splitting it)
  - Simultaneously for all $i âˆˆ â†’b$ (the $Î²_j$ on the right side must
    not be modified during the batch update):
    - $Î²_i â† Î²_i + \frac{Î±}{|â†’b|}\Big(t_i - âˆ‘\nolimits_j Î²_j K(â†’x_i, â†’x_j)\Big)$
</div>

~~~
The predictions are then performed by computing
$$y(â†’z) = Ï†(â†’z)^T â†’w = âˆ‘\nolimits_i Î²_i â†’Ï†(â†’z)^T â†’Ï†(â†’x_i).$$

---
# Bias in Kernel Linear Regression

Until now we did not consider _bias_. Unlike the usual formulation, where we can
â€œhideâ€ it in the weights, we usually handle it manually in the dual formulation.

~~~
Specifically, if we want to include bias in kernel linear regression, we modify
the predictions to
$$y(â†’z) = Ï†(â†’z)^T â†’w + b = âˆ‘\nolimits_i Î²_i â†’Ï†(â†’z)^T â†’Ï†(â†’x_i) + b$$
and update the bias $b$ separately.

~~~
The bias can be updated by SGD, which is what we did in the algorithms until
now; however, if we are considering a bias of an â€œoutput layerâ€, we can
even estimate it as the mean of the training targets before the training
of the rest of the weights.

---
section: Kernels
# Kernel Trick

A single SGD update of all $Î²_i$ then takes $ğ“(N^2)$, given that we can
evaluate scalar dot product of $Ï†(â†’x_i)^T Ï†(â†’x_j)$ quickly.

~~~
$$\begin{aligned}
Ï†(â†’x)^T Ï†(â†’z) &= 1 + âˆ‘_i x_i z_i + âˆ‘_{i,j} x_i x_j z_i z_j + âˆ‘_{i,j,k} x_i x_j x_k z_i z_j z_k \\
              &= 1 + âˆ‘_i x_i z_i + \Big(âˆ‘_i x_i z_i\Big)^2 + \Big(âˆ‘_i x_i z_i\Big)^3 \\
              &= 1 + â†’x^T â†’z + \big(â†’x^T â†’z\big)^2 + \big(â†’x^T â†’z\big)^3.
\end{aligned}$$

---
# Kernels

We define a _kernel_ corresponding to a feature map $Ï†$ as a function
$$K(â†’x, â†’z) â‰ Ï†(â†’x)^T Ï†(â†’z).$$

~~~
There exist quite a lot of kernels, but the most commonly used are the following:

~~~
- _Polynomial kernel or degree $d$_, also called _homogenous polynomial kernel_
  $$K(â†’x, â†’z) = (Î³ â†’x^Tâ†’z)^d,$$
  which corresponds to a feature map generating all combinations of exactly
  $d$ input features.
~~~
  Considering $(a_1 + â€¦ + a_k)^d = âˆ‘_{n_iâ‰¥0, âˆ‘n_i=d} \binom{d}{n_1,â€¦,n_k} a_1^{n_1}\cdots a_k^{n_k}$,
  we can verify that
  $$Ï†(â†’x) = \left(\sqrt{Î³^d\tbinom{d}{n_1,â€¦,n_D}}x_1^{n_1}\cdots x_D^{n_D}\right)_{n_iâ‰¥0,\,âˆ‘n_i=d}.$$
~~~
  For example, for $d=2$, $Ï†(x_1, x_2) = Î³(x_1^2, \sqrt 2 x_1 x_2, x_2^2)$.

---
# Kernels

- _Polynomial kernel or degree at most $d$_, also calles _nonhomogenous
  polynomial kernel_
  $$K(â†’x, â†’z) = (Î³ â†’x^Tâ†’z + 1)^d,$$
  which corresponds to a feature map generating all combinations of up to
  $d$ input features.

~~~
  Given that $(Î³ â†’x^Tâ†’z + 1)^d = âˆ‘_i \binom{d}{i} (Î³ â†’x^Tâ†’z)^i$, it is not
  difficult to derive that
  $$Ï†(â†’x) = \left(\sqrt{Î³^{d-n_{D+1}}d\tbinom{d}{n_1,â€¦,n_{D+1}}}x_1^{n_1}\cdots x_D^{n_D}\right)_{n_iâ‰¥0,\,âˆ‘_{i=1}^{D+1} n_i=d}.$$
~~~
  For example, for $d=2$, $Ï†(x_1, x_2) = (1, \sqrt{2Î³} x_1, \sqrt{2Î³} x_2, Î³x_1^2, \sqrt 2 Î³ x_1 x_2, Î³x_2^2)$.

---
# Kernels

- _Gaussian_ _Radial basis function (RBF)_ kernel
  $$K(â†’x, â†’z) = e^{-Î³||â†’x-â†’z||^2},$$
  corresponds to a scalar product in an infinite-dimensional space; it is
  a combination of polynomial kernels of all degrees.
~~~
  Assuming $Î³=1$ for simplicity, we get
  $$e^{-||â†’x-â†’z||^2}
    = e^{-||â†’x||^2} e^{â†’x^T â†’z} e^{-||â†’y||^2}
    = âˆ‘_{d=0}^âˆ \frac{(â†’x^T â†’z)^d}{d!} e^{-||â†’x||^2} e^{-||â†’y||^2}
    = âˆ‘_{d=0}^âˆ \bigg(\frac{e^{\frac{-||â†’x||^2}{d}}}{(d!)^{\frac{1}{d}}} \frac{e^{\frac{-||â†’y||^2}{d}}}{(d!)^\frac{1}{d}} â†’x^T â†’z \bigg)^d,$$
  which is a combination of polynomial kernels; therefore, the feature map
  corresponding to the RBF kernel is
  $$Ï†(â†’x) = \left(\frac{e^{-||â†’x||^2/d}}{(d!)^{1/d}}\sqrt{Î³^d\tbinom{d}{n_1,â€¦,n_{D+1}}}x_1^{n_1}\cdots x_D^{n_D}\right)_{d=0,1,â€¦,âˆ,\,n_iâ‰¥0,\,âˆ‘_{i=1}^D n_i=d}.$$

---
# Kernels

Note that the RBF kernel is a function of distance â€“ it â€œweightsâ€ more similar
examples more strongly. We could interpret it as an extended version of
k-nearest neighbor algorithm, one which considers all examples, each weighted by
similarity.

~~~
For illustration, we plot RBF kernel values to three points $(0, -1)$, $(1, 1)$
and $(1, -1)$ with different values of $Î³$:

![w=80%,h=center](rbf_kernel.svgz)

---
section: SVM
# Support Vector Machines

Let us return to a binary classification task. The perceptron algorithm
guaranteed finding some separating hyperplane if it existed; we now consider
finding the one with _maximum margin_.

![w=100%,h=center](svm_margin.svgz)

---
# Support Vector Machines

Assume we have a dataset $â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{-1, 1\}^N$, feature map $Ï†$ and model
$$y(â†’x) â‰ â†’Ï†(â†’x)^T â†’w + b.$$

~~~
![w=30%,f=right](../03/binary_classification.svgz)

We already know that the distance of a point $â†’x_i$ to the decision boundary is
$$\frac{|y(â†’x_i)|}{||â†’w||} = \frac{t_i y(â†’x_i)}{||â†’w||}.$$

~~~
We therefore want to maximize
$$\argmax_{w,b} \frac{1}{||â†’w||} \min_i \big[t_i (â†’Ï†(â†’x)^T â†’w + b)\big].$$

However, this problem is difficult to optimize directly.

---
# Support Vector Machines

Because the model is invariant to multiplying $â†’w$ and $b$ by a constant, we can
decide that for the points closest to the decision boundary, it will hold that
$$t_i y(â†’x_i) = 1.$$

~~~
Then for all the points we will have $t_i y(â†’x_i) â‰¥ 1$ and we can simplify
$$\argmax_{w,b} \frac{1}{||â†’w||} \min_i \big[t_i (â†’Ï†(â†’x)^T â†’w + b)\big]$$
to
$$\argmin_{w,b} \frac{1}{2} ||â†’w||^2 \textrm{~given that~~}t_i y(â†’x_i) â‰¥ 1.$$

---
section: KKT
# Lagrange Multipliers â€“ Inequality Constraints

Given a funtion $f(â†’x)$, we can find a maximum with respect to a vector
$â†’x âˆˆ â„^d$, by investigating the critical points $âˆ‡_â†’x f(â†’x) = 0$.
We even know how to incorporate constraints of form $g(â†’x) = 0$.
~~~
We now describe how to include inequality constraints $g(â†’x) â‰¥ 0$.

~~~
![w=25%,f=right](lagrange_inequalities.svgz)

We start by again forming a Lagrangian $f(â†’x) + Î»g(â†’x)$.

~~~
The optimum can either be attained for $g(â†’x) > 0$, when the constraint is said
to be **inactive**, or for $g(â†’x) = 0$, when the constraint is said to be
**active**.
~~~
In the inactive case, the maximum is again a critical point of the Langrangian
with the condition $Î»=0$.

~~~
When maximum is on a boundary, it corresponds to a critical point
with $Î»â‰ 0$ â€“ but note that this time the sign of the multiplier matters, because
maximum is attained only when gradient of $f(â†’x)$ is oriented away from the region
$g(â†’x) â‰¥ 0$. We therefore require $âˆ‡f(â†’x) = - Î»âˆ‡g(â†’x)$ for $Î»>0$.

~~~
In both cases, $Î» g(â†’x) = 0$.

---
section: KKT
# Karush-Kuhn-Tucker Conditions

![w=25%,f=right](lagrange_inequalities.svgz)

Put together, every solution to a maximization problem of $f(â†’x)$ subject to $g(â†’x)â‰¥0$
must be a maximum of the Lagrangian with respect to $â†’x$ and the following must
hold:
$$\begin{aligned}
g(â†’x) &â‰¥ 0, \\
Î» &â‰¥ 0, \\
Î» g(â†’x) &= 0.
\end{aligned}$$

~~~
Note that for a given $â†’x$, the suitable $Î»$ can be found by **minimizing** the
Lagrangian with respect to $Î»$ (and if its value is $-âˆ$, the $g(â†’x)$ constraint
was violated).

~~~
#### Minimizing Given $f(â†’x)$

If we instead want to find constrained minimum of $f(â†’x)$, we can search for
maximum of $-f(â†’x)$, which results in _minimizing_ Lagrangian $f(â†’x)-Î»g(â†’x)$
with respect to $â†’x$ and _maximizing_ it for $Î»$.

---
# Necessary and Sufficient KKT Conditions

The KKT conditions are necessary conditions for a maximum (resp. minimum).

~~~
However, it can be proven that in the following settings, the conditions are
also **sufficient**:
- if the objective to optimize is a _concave_ function (resp. _convex_ for minimization)
  with respect to $â†’x$;
~~~
- the inequality constraints are continuously differentiable convex functions;
~~~
- the equality constraints are affine functions (linear functions with an
  offset).

~~~
It is easy to verify that these conditions hold for the SVM optimization
problem.

---
section: Dual SVM Formulation
# Support Vector Machines

In order to solve the constrained problem of
$$\argmin_{w,b} \frac{1}{2} ||â†’w||^2 \textrm{~given that~~}t_i y(â†’x_i) â‰¥ 1,$$
~~~
we write the Lagrangian with multipliers $â†’a=(a_1, â€¦, a_N)$ as
$$L = \frac{1}{2} ||â†’w||^2 - âˆ‘_i a_i \big[t_i y(â†’x_i) - 1\big].$$

~~~
Setting the derivatives with respect to $â†’w$ and $b$ to zero, we get
$$\begin{aligned}
â†’w =& âˆ‘_i a_i t_iÏ†(â†’x_i), \\
 0 =& âˆ‘_i a_i t_i. \\
\end{aligned}$$

---
# Support Vector Machines

Substituting these to the Lagrangian, we get
$$L = âˆ‘_i a_i -  \frac{1}{2} âˆ‘_i âˆ‘_j a_i a_j t_i t_j K(â†’x_i, â†’x_j)$$
with respect to the constraints $âˆ€_i: a_i â‰¥ 0$, $âˆ‘_i a_i t_i = 0$
and kernel $K(â†’x, â†’z) = Ï†(â†’x)^T Ï†(â†’z).$

~~~
The solution of this Lagrangian will fulfil the KKT conditions, meaning that
$$\begin{aligned}
a_i &â‰¥ 0, \\
t_i y(â†’x_i) - 1 &â‰¥ 0, \\
a_i \big(t_i y(â†’x_i) - 1\big) &= 0.
\end{aligned}$$

~~~
Therefore, either a point is on a boundary, or $a_i=0$. Given that the
predictions for point $â†’x$ are $y(â†’x) = âˆ‘ a_i t_i K(â†’x, â†’x_i) + b$,
we need to keep only the points on the boundary, the so-called **support vectors**.
Even if SVM is nonparametric model, it stores only a subset of data.

---
# Support Vector Machines

The dual formulation allows us to use non-linear kernels.

![w=100%](svm_gaussian.svgz)
