title: NPFL129, Lecture 6
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Soft-margin SVM, SMO Algorithm, Decision Trees

## Milan Straka

### November 25, 2019

---
section: Refresh
# Kernel Linear Regression

When dimensionality of input is $D$, one step of SGD takes $ğ“(D^3)$.

Surprisingly, we can do better under some circumstances. We start by
noting that we can write the parameters $â†’w$ as a linear combination
of the input features $Ï†(â†’x_i)$.

By induction, $â†’w = 0 = âˆ‘_i 0 â‹… Ï†(â†’x_i)$, and assuming $â†’w = âˆ‘_i Î²_i â‹… Ï†(â†’x_i)$,
after a SGD update we get
$$\begin{aligned}
â†’w â†& â†’w + Î±âˆ‘_i \big(t_i - â†’w^T Ï†(â†’x_i)\big) Ï†(â†’x_i)\\
   =& âˆ‘_i \Big(Î²_i + Î± \big(t_i - â†’w^T Ï†(â†’x_i)\big)\Big) Ï†(â†’x_i).
\end{aligned}$$

A individual update is $Î²_i â† Î²_i + Î±\Big(t_i - â†’w^T Ï†(â†’x_i)\Big)$, and
substituting for $â†’w$ we get
$$Î²_i â† Î²_i + Î±\Big(t_i - âˆ‘\nolimits_j Î²_j Ï†(â†’x_j)^T Ï†(â†’x_i)\Big).$$

---
# Kernel Linear Regression

We can formulate the alternative linear regression algorithm (it would be called
a _dual formulation_):

<div class="algorithm">

**Input**: Dataset ($â‡‰X = \{â†’x_1, â†’x_2, â€¦, â†’x_N\} âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$), learning rate $Î± âˆˆ â„^+$.<br>

- Set $Î²_i â† 0$
- Compute all values $K(â†’x_i, â†’x_j) = Ï†(â†’x_i)^T Ï†(â†’x_j)$
- Repeat
  - Update the coordinates, either according to a full gradient update:
    - $â†’Î² â† â†’Î² + Î±(â†’t-Kâ†’Î²)$
  - or alternatively use single-batch SGD, arriving at:
    - for $i$ in random permutation of $\{1, â€¦, N\}$:
      - $Î²_i â† Î²_i + Î±\Big(t_i - âˆ‘\nolimits_j Î²_j K(â†’x_i, â†’x_j)\Big)$

    In vector notation, we can write $â†’Î² â† â†’Î² + Î±(â†’t-Kâ†’Î²)$.
</div>

The predictions are then performed by computing $y(â†’x) = â†’w^T Ï†(â†’x) = âˆ‘_i Î²_i â†’Ï†(â†’x_i)^T â†’Ï†(â†’x)$.

---
# Kernels

We define a _kernel_ corresponding to a feature map $Ï†$ as a function
$$K(â†’x, â†’z) â‰ Ï†(â†’x)^t Ï†(â†’z).$$

There is quite a lot of theory behind kernel construction. The most often used
kernels are:

- polynomial kernel or degree $d$
  $$K(â†’x, â†’z) = (Î³ â†’x^Tâ†’z + 1)^d,$$
  which corresponds to a feature map generating all combinations of up to $d$
  input features;
- Gaussian (or RBF) kernel
  $$K(â†’x, â†’z) = e^{-Î³||â†’x-â†’z||^2},$$
  corresponding to a scalar product in an infinite-dimensional space (it is
  in a sense a combination of polynomial kernels of all degrees).

---
# Support Vector Machines

Assume we have a dataset $â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{-1, 1\}^N$, feature map $Ï†$ and model
$$y(â†’x) â‰ â†’Ï†(â†’x)^T â†’w + b.$$

![w=30%,f=right](../03/binary_classification.pdf)

We already know that the distance of a point $â†’x_i$ to the decision boundary is
$$\frac{|y(â†’x_i)|}{||â†’w||} = \frac{t_i y(â†’x_i)}{||â†’w||}.$$

We therefore want to maximize
$$\argmax_{w,b} \frac{1}{||â†’w||} \min_i \big[t_i (â†’Ï†(â†’x)^T â†’w + b)\big].$$

However, this problem is difficult to optimize directly.

---
# Support Vector Machines

Because the model is invariant to multiplying $â†’w$ and $b$ by a constant, we can
say that for the points closest to the decision boundary, it will hold that
$$t_i y(â†’x_i) = 1.$$

Then for all the points we will have $t_i y(â†’x_i) â‰¥ 1$ and we can simplify
$$\argmax_{w,b} \frac{1}{||â†’w||} \min_i \big[t_i (â†’Ï†(â†’x)^T â†’w + b)\big]$$
to
$$\argmin_{w,b} \frac{1}{2} ||â†’w||^2 \textrm{~given that~~}t_i y(â†’x_i) â‰¥ 1.$$

---
# Support Vector Machines

In order to solve the constrained problem of
$$\argmin_{w,b} \frac{1}{2} ||â†’w||^2 \textrm{~given that~~}t_i y(â†’x_i) â‰¥ 1,$$
we write the Lagrangian with multipliers $â†’a=(a_1, â€¦, a_N)$ as
$$L = \frac{1}{2} ||â†’w||^2 - âˆ‘_i a_i \big[t_i y(â†’x_i) - 1\big].$$

Setting the derivatives with respect to $â†’w$ and $b$ to zero, we get
$$\begin{aligned}
â†’w =& âˆ‘_i a_i t_iÏ†(â†’x_i) \\
 0 =& âˆ‘_i a_i t_i \\
\end{aligned}$$

---
# Support Vector Machines

Substituting these to the Lagrangian, we get
$$L = âˆ‘_i a_i -  \frac{1}{2} âˆ‘_i âˆ‘_j a_i a_j t_i t_j K(â†’x_i, â†’x_j)$$
with respect to the constraints $âˆ€_i: a_i â‰¥ 0$, $âˆ‘_i a_i t_i = 0$
and kernel $K(â†’x, â†’z) = Ï†(â†’x)^T Ï†(â†’z).$

The solution of this Lagrangian will fulfil the KKT conditions, meaning that
$$\begin{aligned}
a_i &â‰¥ 0 \\
t_i y(â†’x_i) - 1 &â‰¥ 0 \\
a_i \big(t_i y(â†’x_i) - 1\big) &= 0.
\end{aligned}$$

Therefore, either a point is on a boundary, or $a_i=0$. Given that the
predictions for point $â†’x$ are given by $y(â†’x) = âˆ‘ a_i t_i K(â†’x, â†’x_i) + b$,
we need to keep only the points on the boundary, the so-called **support vectors**.

---
# Support Vector Machines

The dual formulation allows us to use non-linear kernels.

![w=100%](../05/svm_gaussian.pdf)

---
section: Soft-margin SVN
# Support Vector Machines for Non-linearly Separable Data

![w=28%,f=right](svm_softmargin.pdf)

Until now, we assumed the data to be linearly separable â€“ the  
**hard-margin SVM** variant. We now relax this condition to arrive at
**soft-margin SVM**.
~~~
The idea is to allow points to be in the margin or even on the _wrong side_ of
the decision boundary. We introduce _slack variables_ $Î¾_i â‰¥ 0$, one for each
training instance, defined as
$$Î¾_i = \begin{cases} 0 &\textrm{~for points fulfilling~}t_i y(â†’x_i) â‰¥ 1,\\
                      |t_i - y(â†’x_i)|&\textrm{~otherwise}.\end{cases}$$

~~~
Therefore, $Î¾_i=0$ signifies a point outside of margin, $0 < Î¾_i < 1$ denotes
a point inside the margin, $Î¾_i=1$ is a point on the decision boundary and
$Î¾_i>1$ indicates the point is on the opposite side of the separating
hyperplane.

~~~
Therefore, we want to optimize
$$\argmin_{w,b} C âˆ‘_i Î¾_i + \frac{1}{2} ||â†’w||^2 \textrm{~given that~~}t_i y(â†’x_i) â‰¥ 1 - Î¾_i\textrm{~and~}Î¾_i â‰¥ 0.$$

---
# Support Vector Machines for Non-linearly Separable Data

We again create a Lagrangian, this time with multipliers $â†’a=(a_1, â€¦, a_N)$
and also $â†’Î¼=(Î¼_1, â€¦, Î¼_N)$:
$$L = \frac{1}{2} ||â†’w||^2 + C âˆ‘_i Î¾_i - âˆ‘_i a_i \big[t_i y(â†’x_i) - 1 + Î¾_i\big] - âˆ‘_i Î¼_i Î¾_i.$$

~~~
Solving for the critical points and substituting for $â†’w$, $b$ and $â†’Î¾$ (obtaining an additional
constraint $Î¼_i = C - a_i$ compared to the previous case), we obtain the Lagrangian in the form
$$L = âˆ‘_i a_i -  \frac{1}{2} âˆ‘_i âˆ‘_j a_i a_j t_i t_j K(â†’x_i, â†’x_j),$$
which is identical to the previous case, but the constraints are a bit
different:
$$âˆ€_i: C â‰¥ a_i â‰¥ 0\textrm{~and~}âˆ‘_i a_i t_i = 0.$$

---
# Support Vector Machines for Non-linearly Separable Data

Using KKT conditions, we can see that the support vectors (examples with
$a_i>0$) are the ones with $t_i y(â†’x_i) = 1 - Î¾_i$, i.e., the examples
on the margin boundary, inside the margin and on the opposite side
of the decision boundary.

![w=50%,h=center](svm_softmargin_supports.pdf)

---
# SGD-like Formulation of Soft-Margin SVM

Note that the slack variables can be written as
$$Î¾_i = \max\big(0, 1 - t_i y(â†’x_i)\big),$$
so we can reformulate the soft-margin SVM objective using **hinge loss**
$$ğ“›_\textrm{hinge}(t, y) â‰ \max(0, 1 - t y)$$
to
$$\argmin_{w,b} C âˆ‘_i ğ“›_\textrm{hinge}\big(t_i, y(â†’x_i)\big) + \frac{1}{2} ||â†’w||^2 .$$

~~~
Such formulation is analogous to a regularized loss, where $C$ is an _inverse_
regularization strength, so $C=âˆ$ implies no regularization and $C=0$ ignores
the data entirely.

---
class: tablewide
# Comparison of Linear and Logistic Regression and SVM

For $f(â†’x; â†’w, b) â‰ â†’Ï†(â†’x)^T â†’w + b$, we have seen the following losses:

| Model | Objective | Per-Instance Loss |
|:------|:----------|:------------------|
| Linear<br>Regression | $\small \argmin_{â†’w,b} âˆ‘_i ğ“›_\textrm{MSE}\big(t_i, f(â†’x_i)\big) + \frac{1}{2} Î» \|â†’w\|^2$ | $\small ğ“›_\textrm{MSE}(t, y) = \frac{1}{2}(t - y)^2$ |
| Logistic<br>regression | $\small \argmin_{â†’w,b} âˆ‘_i ğ“›_\textrm{Ïƒ-NLL}\big(t_i, f(â†’x_i)\big) + \frac{1}{2} Î» \|â†’w\|^2$ | $\small ğ“›_\textrm{Ïƒ-NLL}(t, y) = - \log \begin{pmatrix}Ïƒ(y)^t +\\ \big(1-Ïƒ(y)\big)^{1-t}\end{pmatrix}$ |
| Softmax<br>regression | $\small \argmin_{â‡‰W,â†’b} âˆ‘_i ğ“›_\textrm{s-NLL}\big(t_i, f(â†’x_i)\big) + \frac{1}{2} Î» \|â†’w\|^2$ | $\small ğ“›_\textrm{s-NLL}(t, â†’y) = - \log \softmax(â†’y)_t$ |
| SVM | $\small \argmin_{â†’w,b} C âˆ‘_i ğ“›_\textrm{hinge}\big(t_i, f(â†’x_i)\big) + \frac{1}{2} \|â†’w\|^2$ | $\small ğ“›_\textrm{hinge}(t, y) = \max(0, 1 - ty)$ |

~~~
Note that $\small ğ“›_\textrm{MSE}(t, y) âˆ -\log\big(ğ“(t; Î¼=y, Ïƒ^2=1)\big)$ and
that $\small ğ“›_\textrm{Ïƒ-NLL}(t, y) = ğ“›_\textrm{s-NLL}(t, [y, 0])$.

---
# Binary Classification Loss Functions Comparison

To compare various functions for binary classification, we need to formulate
them all in the same settings, with $t âˆˆ \{-1, 1\}$.

~~~
- MSE: $(ty - 1)^2$, because it is $(y - 1)^2$ for $t=1$ and $(-y - t)^2$ for $t=-1$
~~~
- LR: $Ïƒ(ty)$, because it is $Ïƒ(y)$ for $t=1$ and $1-Ïƒ(y)=Ïƒ(-y)$ for $t=-1$
~~~
- SVM: $\max(0, 1 - ty)$

![w=42%,h=center](binary_losses.pdf)

---
section: SMO
# Sequential Minimal Optimization Algorithm

To solve the dual formulation of a SVM, usually Sequential Minimal Optimization
(SMO; John Platt, 1998) algorithm is used.

~~~
Before we introduce it, we start by introducing **coordinate descent**
optimization algorithm.

~~~
Consider solving unconstrained optimization problem
$$\argmin_â†’w L(w_1, w_2, â€¦, w_D).$$

~~~
Instead of the usual SGD approach, we could optimize the weights one by one,
using the following algorithm

<div class="algorithm">

- loop until convergence
  - for $i$ in $\{1, 2, â€¦, D\}$:
    - $w_i â† \argmin\nolimits_{w_i} L(w_1, w_2, â€¦, w_D)$
</div>

---
# Sequential Minimal Optimization Algorithm

<div class="algorithm">

- loop until convergence
- for $i$ in $\{1, 2, â€¦, D\}$:
  - $w_i â† \argmin\nolimits_{w_i} L(w_1, w_2, â€¦, w_D)$
</div>

![w=42%,f=right](coordinate_descent.pdf)

If the inner $\argmin$ can be performed efficiently, the coordinate descent can
be fairly efficient.


~~~
Note that we might want to choose $w_i$ in different order, for example
by trying to choose $w_i$ providing the largest decrease of $L$.

---
# Sequential Minimal Optimization Algorithm

In soft-margin SVM, we try to minimize
$$L = âˆ‘_i a_i -  \frac{1}{2} âˆ‘_i âˆ‘_j a_i a_j t_i t_j K(â†’x_i, â†’x_j),$$
such that
$$âˆ€_i: C â‰¥ a_i â‰¥ 0\textrm{~and~}âˆ‘_i a_i t_i = 0.$$

~~~
The KKT conditions for the solution can be reformulated (while staying
equivalent) as
$$\begin{aligned}
  a_i > 0 & â‡’ t_i y(â†’x_i) â‰¤ 1,~\textrm{ because }a_i > 0 â‡’ t_i y(â†’x_i) = 1 - Î¾_i\textrm{ and we have }Î¾_i â‰¥ 0,\\
  a_i < C & â‡’ t_i y(â†’x_i) â‰¥ 1,~\textrm{ because }a_i < C â‡’ Î¼_i > 0 â‡’ Î¾_i = 0\textrm{ and }t_i y(â†’x_i) â‰¥ 1 - Î¾_i, \\
  0 < a_i < C & â‡’ t_i y(â†’x_i) = 1,~\textrm{ a combination of both}.
\end{aligned}$$

---
# Sequential Minimal Optimization Algorithm

At its core, the SMO algorithm is just a coordinate descent.

~~~
It tries to find such $Î±_i$ fulfilling the KKT conditions â€“ for soft-margin SVM,
KKT conditions are sufficient conditions for optimality (the loss is convex and
inequality constraints affine).

~~~
However, note that because of the $âˆ‘a_i t_i = 0$ constraint we cannot optimize
just one $a_i$, because a single $a_i$ is determined from the others.

~~~
Therefore, in each step we pick two $a_i, a_j$ coefficients and try to minimize
the loss while fulfilling the constraints.

~~~
<div class="algorithm">

- loop until convergence (until $âˆ€ i: a_i < C â‡’ t_i y(â†’x_i) â‰¥ 1$ and $a_i > 0 â‡’  t_i y(â†’x_i) â‰¤ 1$)
  - for $i$ in $\{1, 2, â€¦, D\}$, for $j â‰  i$ in $\{1, 2, â€¦, D\}:
    - $a_i, a_j â† \argmin\nolimits_{a_i, a_j} L(a_1, a_2, â€¦, a_D)$ such that $C â‰¥ a_i â‰¥ 0$, $âˆ‘_i a_i t_i = 0$
</div>

---
# Sequential Minimal Optimization Algorithm

The SMO is an efficient algorithm, because we can compute the update to
$a_i, a_j$ efficiently, because there exists an closed form solution.

~~~
Assume that we are updating $a_i$ and $a_j$. Then from the $âˆ‘_k a_k t_k = 0$ condition we can
write $a_i t_i = -âˆ‘_{kâ‰ i} a_k t_k$. Given that $t_i^2 = 1$ and denoting $Î¶=-âˆ‘_{kâ‰ i, kâ‰ j} a_k t_k$, we get
$$a_i = t_i (Î¶-a_j t_j).$$

~~~
Minimizing $L(â†’a)$ with respect to $a_i$ and $a_j$ then amounts to minimizing
a quadratic function of $a_j$, which has an analytical solution.

~~~
Note that the real SMO algorithm has several heuristics for choosing $a_i, a_j$
such that the $L$ can be minimized the most.

---
# Sequential Minimal Optimization Algorithm Sketch

<div class="algorithm">

**Input**: Dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{-1,1\}^N$), kernel $â‡‰K$, regularization parameter $C$, tolerance $\textit{tol}$,
$\textit{max\_passes\_without\_a\_changing}$ value<br>

- Initialize $a_i â† 0$, $b â† 0$, $\textit{passes} â† 0$
- **while** $\textit{passes} < \textit{max\_passes\_without\_a\_changing}$:
  - $\textit{changed\_as} â† 0$
  - **for** $i$ in $1, 2, â€¦, N$:
    - $E_i â† y(â†’x_i) - t_i$
    - **if** ($a_i < C$ **and** $t_i E_i < -\textit{tol}$) **or** ($a_i > 0$ **and** $t_i E_i > \textit{tol}$):
      - Choose $j â‰  i$ randomly
      - Update $a_i$, $a_j$ and $b$
      - $\textit{changed\_as} â† \textit{changed\_as} + 1$
  - **if** $\textit{changed\_as} = 0$: $\textit{passes} â† \textit{passes} + 1$
  - **else**: $\textit{passes} â† 0$
</div>

---
# Sequential Minimal Optimization Algorithm Sketch

<div class="algorithm">

**Input**: Dataset ($â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{-1,1\}^N$), kernel $â‡‰K$, regularization parameter $C$, tolerance $\textit{tol}$,
$\textit{max\_passes\_without\_a\_changing}$ value<br>

- Update $a_i$, $a_j$, $b$:
  - Express $a_i$ using $a_j$
  - Find $a_j$ optimizing the loss L quadratic with respect to $a_j$
  - Clip $a_j$ so that $0 â‰¤ a_i, a_j â‰¤ C$
  - Compute corresponding $a_i$
  - Compute $b$ matching to updated $a_i$, $a_j$
</div>

---
section: Primal vs Dual
class: tablewide
style: td:nth-child(1) {width: 25%}  td:nth-child(2) {width: 35%}
# Primal versus Dual Formulation

Assume we have a dataset with $N$ training examples, each with $D$ features.
Also assume the used feature map $Ï†$ generates $F$ features.

| Property | Primal Formulation | Dual Formulation |
|:---------|:-------------------|:-----------------|
| Parameters | $F$ | $N$ |
~~~
| Model size | $F$ | $sâ‹…D$ for $s$ support vectors |
~~~
| Usual training time | $c â‹… N â‹… F$ for $c$ iterations | between $Î©(ND)$ and $ğ“(N^2D)$ |
~~~
| Inference time | $Î˜(F)$ | $Î˜(sâ‹…D)$ for $s$ support vectors |

---
section: DecisionTree
# Decision Trees

The idea of decision trees is to partition the input space into usually cuboid
regions and solving each region with a simpler model.

~~~
We focus on **Classification and Regression Trees** (CART; Breiman et al.,
1984), but there are additional variants like ID3, C4.5, â€¦

~~~
![w=80%,mw=49%,h=center](tree_partitioning.pdf)
~~~
![w=90%,mw=49%,h=center](tree_representation.pdf)

---
# Regression Decision Trees

Assume we have an input dataset $â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$. At the beginning,
the decision tree is just a single node and all input examples belong to this
node. We denote $I_ğ“£$ the set of training example indices belonging to a leaf
node $ğ“£$.

~~~
For each leaf, our model will predict the average of the training examples
belonging to that leaf, $tÌ‚_ğ“£ = \frac{1}{|I_ğ“£|} âˆ‘_{i âˆˆ I_ğ“£} t_i$.

~~~
We will use a _criterion_ $c_ğ“£$ telling us how _uniform_ or _homogeneous_ are the
training examples belonging to a leaf node $ğ“£$ â€“ for regression, we will
employ the sum of squares error between the examples belonging to the node and the predicted
value in that node; this is proportional to variance of the training examples belonging
to the leaf node $ğ“£$, multiplied by the number of the examples. Note that even
if it not _mean_ squared error, it is sometimes denoted as MSE.
$$c_\textrm{SE}(ğ“£) â‰ âˆ‘_{i âˆˆ I_ğ“£} (t_i - tÌ‚_ğ“£)^2\textrm{, where } tÌ‚_ğ“£=\frac{1}{|I_ğ“£|} âˆ‘_{i âˆˆ I_ğ“£} t_i.$$

---
# Tree Construction

To split a node, the goal is to find a feature and its value such that when
splitting a node $ğ“£$ into $ğ“£_L$ and $ğ“£_R$, the resulting regions decrease the
overall criterion value the most, i.e., the difference $c_{ğ“£_L} + c_{ğ“£_R} - c_ğ“£$
is the lowest.

~~~
Usually we have several constraints, we mention on the most common ones:
- **maximum tree depth**: we do not split nodes with this depth;
~~~
- **minimum examples to split**: we only split nodes with this many training
  examples;
~~~
- **maximum number of leaf nodes**

~~~
The tree is usually built in one of two ways:
- if the number of leaf nodes is unlimited, we usually build the tree in
  a depth-first manner, recursively splitting every leaf until some above
  constraint is invalidated;
~~~
- if the maximum number of leaf nodes is give, we usually split such leaf $ğ“£$
  where the criterion difference $c_{ğ“£_L} + c_{ğ“£_R} - c_ğ“£$ is the lowest.

---
# Classification Decision Trees

For multi-class classification, we predict such class most frequent
in the training examples belonging to a leaf $ğ“£$.

~~~
To define the criterions, let us denote the average probability for class $k$ in
a region $ğ“£$ at $p_{ğ“£}(k)$.

~~~
For classification trees, one of the following two criterions is usually used:

- **Gini index**:
  $$c_\textrm{Gini}(ğ“£) â‰ |I_ğ“£| âˆ‘_k p_ğ“£(k) \big(1 - p_ğ“£(k)\big)$$

~~~
- **Entropy Criterion**
  $$c_\textrm{entropy}(ğ“£) â‰ |I_ğ“£| H(p_ğ“£) = - |I_ğ“£| âˆ‘_k p_ğ“£(k) \log p_ğ“£(k)$$
