title: NPFL129, Lecture 6
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Kernel Methods, SVM

## Milan Straka

### November 08, 2021

---
section: KernelLR
# Kernel Linear Regression

Consider linear regression with linear, quadratic and cubic features (for
simplicity we consider $x_i$, $x_i x_j$ and $x_i x_j x_k$ for any indices)
computed by feature mapping $Ï†: â„^D â†’ â„^{1+D+D^2+D^3}$:
$$Ï†(â†’x) = \scriptsize\begin{bmatrix} 1 \\ x_1 \\ x_2 \\ â€¦ \\ x_1^2 \\ x_1x_2 \\ â€¦ \\ x_2x_1 \\ â€¦ \\ x_1^3 \\ x_1^2x_2 \\ â€¦ \end{bmatrix}.$$

~~~
The SGD update of a linear regression using a minibatch of examples with indices $â†’b$ is then
$$â†’w â† â†’w - \frac{Î±}{|â†’b|}âˆ‘\nolimits_{i âˆˆ â†’b}\big(Ï†(â†’x_i)^T â†’w - t_i\big) Ï†(â†’x_i).$$

---
# Kernel Linear Regression

When the dimensionality of the input is $D$, one step of SGD takes $ğ“(D^3)$ per
example.

~~~
Surprisingly, we can do better under some circumstances. We start by
noting that we can write the parameters $â†’w$ as a linear combination
of the input feature vectors $Ï†(â†’x_i)$.

~~~
By induction, we can start with $â†’w = 0 = âˆ‘_i 0 â‹… Ï†(â†’x_i)$,
~~~
and assuming $â†’w = âˆ‘_i Î²_i â‹… Ï†(â†’x_i)$,
after an SGD update we get
$$\begin{aligned}
â†’w &â† â†’w - \frac{Î±}{|â†’b|}âˆ‘_{i âˆˆ â†’b} \big(Ï†(â†’x_i)^T â†’w - t_i\big) Ï†(â†’x_i)\\
   &â† âˆ‘_i \Big(Î²_i - \big[i âˆˆ â†’b\big] â‹… \frac{Î±}{|â†’b|} \big(Ï†(â†’x_i)^T â†’w - t_i\big)\Big) Ï†(â†’x_i).
\end{aligned}$$

~~~
Every $Î²_i$ for $i âˆˆ â†’b$ changes to $Î²_i - \frac{Î±}{|â†’b|}\Big(Ï†(â†’x_i)^T â†’w - t_i\Big)$, so after
substituting for $â†’w$ we get
$$Î²_i â† Î²_i - \frac{Î±}{|â†’b|}\Big(âˆ‘\nolimits_j \big(Î²_j Ï†(â†’x_i)^T Ï†(â†’x_j)\big) - t_i\Big).$$

---
# Kernel Linear Regression

We can formulate an alternative linear regression algorithm (a so-called
**dual formulation**):

<div class="algorithm">

**Input**: Dataset ($â‡‰X = \{â†’x_1, â†’x_2, â€¦, â†’x_N\} âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$), learning rate $Î± âˆˆ â„^+$.<br>

- set $Î²_i â† 0$
- compute all values $â‡‰K_{i,j} = Ï†(â†’x_i)^T Ï†(â†’x_j)$
- until convergence (or patience runs out), process a minibatch of examples with indices $â†’b$:
  - simultaneously for all $i âˆˆ â†’b$ (the $Î²_j$ on the right side must
    not be modified during the batch update):
    - $Î²_i â† Î²_i - \frac{Î±}{|â†’b|}\Big(âˆ‘\nolimits_j \big(Î²_j â‡‰K_{i,j}\big) - t_i\Big)$
</div>

~~~
The predictions are then performed by computing
$$y(â†’z) = Ï†(â†’z)^T â†’w = âˆ‘\nolimits_i Î²_i â†’Ï†(â†’z)^T â†’Ï†(â†’x_i).$$

---
# Bias in Kernel Linear Regression

Until now we did not consider _bias_. Unlike the usual formulation, where we can
â€œhideâ€ it in the weights, we usually handle it manually in the dual formulation.

~~~
Specifically, if we want to include bias in kernel linear regression, we modify
the predictions to
$$y(â†’z) = Ï†(â†’z)^T â†’w + b = âˆ‘\nolimits_i Î²_i â†’Ï†(â†’z)^T â†’Ï†(â†’x_i) + b$$
and update the bias $b$ separately.

~~~
The bias can be updated by SGD, which is what we did in the algorithms until
now; however, if we are considering a bias of an _output layer_, we can
even estimate it as the mean of the training targets before the training
of the rest of the weights.

---
section: Kernels
# Kernel Trick

A single SGD update of a dual-formulation kernel linear regression takes $ğ“(N)$
per example, if we pre-compute all the $ğ“(N^2)$ dot products $Ï†(â†’x_i)^T Ï†(â†’x_j)$.
Furthermore, inference requires evaluating $ğ“(N)$ dot products $Ï†(â†’z)^T Ï†(â†’x_i)$.

~~~
Therefore, we need to compute a dot product $Ï†(â†’x)^T Ï†(â†’z)$ quickly.

~~~
Using the previously-defined $Ï†$, we get
$$\begin{aligned}
Ï†(â†’x)^T Ï†(â†’z) &= 1 + âˆ‘_i x_i z_i + âˆ‘_{i,j} x_i x_j z_i z_j + âˆ‘_{i,j,k} x_i x_j x_k z_i z_j z_k \\
              &= 1 + âˆ‘_i x_i z_i + \Big(âˆ‘_i x_i z_i\Big)^2 + \Big(âˆ‘_i x_i z_i\Big)^3 \\
              &= 1 + â†’x^T â†’z + \big(â†’x^T â†’z\big)^2 + \big(â†’x^T â†’z\big)^3.
\end{aligned}$$

~~~
Therefore, we can compute the dot product $Ï†(â†’x)^T Ï†(â†’z)$ in $ğ“(D)$ instead of
$ğ“(D^3)$.

---
# Kernels

We define a **kernel** corresponding to a feature map $Ï†$ as a function
$$K(â†’x, â†’z) â‰ Ï†(â†’x)^T Ï†(â†’z).$$

~~~
There exist quite a lot of kernels, but the most commonly used are the following:

~~~
- **Polynomial kernel of degree $d$**, also called _homogenous polynomial kernel_
  $$K(â†’x, â†’z) = (Î³ â†’x^Tâ†’z)^d,$$
  corresponds to a feature map returning all combinations of exactly $d$ input
  features.
~~~
  \
  Using $(a_1 + â€¦ + a_k)^d = âˆ‘_{n_iâ‰¥0, âˆ‘n_i=d} \binom{d}{n_1,â€¦,n_k} a_1^{n_1}\cdots a_k^{n_k}$,
  we can verify that
  $$Ï†(â†’x) = \left(\sqrt{Î³^d\tbinom{d}{n_1,â€¦,n_D}}x_1^{n_1}\cdots x_D^{n_D}\right)_{n_iâ‰¥0,\,âˆ‘n_i=d}.$$
~~~
  For example, for $d=2$, $Ï†(x_1, x_2) = Î³(x_1^2, \sqrt 2 x_1 x_2, x_2^2)$.

---
# Kernels

- **Polynomial kernel of degree at most $d$**, also called _nonhomogenous
  polynomial kernel_
  $$K(â†’x, â†’z) = (Î³ â†’x^Tâ†’z + 1)^d,$$
  corresponds to a feature map generating all combinations of up to $d$ input
  features.

~~~
  Given that $(Î³ â†’x^Tâ†’z + 1)^d = âˆ‘_i \binom{d}{i} (Î³ â†’x^Tâ†’z)^i$, it is not
  difficult to derive that
  $$Ï†(â†’x) = \left(\sqrt{Î³^{d-n_{D+1}}\binom{d}{n_1,â€¦,n_{D+1}}}x_1^{n_1}\cdots x_D^{n_D}\right)_{n_iâ‰¥0,\,âˆ‘_{i=1}^{D+1} n_i=d}.$$
~~~
  For example, for $d=2$, $Ï†(x_1, x_2) = (1, \sqrt{2Î³} x_1, \sqrt{2Î³} x_2, Î³x_1^2, \sqrt 2 Î³ x_1 x_2, Î³x_2^2)$.

---
# Kernels

- **Gaussian Radial basis function (RBF)** kernel
  $$K(â†’x, â†’z) = e^{-Î³\|â†’x-â†’z\|^2},$$
  corresponds to a scalar product in an infinite-dimensional space; it is
  a combination of polynomial kernels of all degrees.
~~~
  Assuming $Î³=1$ for simplicity, we get
  $$e^{-\|â†’x-â†’z\|^2}
    = e^{-\|â†’x\|^2 + 2â†’x^T â†’z -\|â†’z\|^2}
    = âˆ‘_{d=0}^âˆ \frac{(2â†’x^T â†’z)^d}{d!} e^{-\|â†’x\|^2 -\|â†’z\|^2}
    = âˆ‘_{d=0}^âˆ \frac{2^d e^{-\|â†’x\|^2-\|â†’z\|^2}}{d!} \Big(â†’x^T â†’z \Big)^d,$$
~~~
  which is a combination of polynomial kernels; therefore, the feature map
  corresponding to the RBF kernel is
  $$Ï†(â†’x) = \left(e^{-Î³\|â†’x\|^2}\sqrt{\frac{(2Î³)^d}{d!}\binom{d}{n_1,â€¦,n_D}}x_1^{n_1}\cdots x_D^{n_D}\right)_{dâˆˆ\{0,1,2,â€¦\},\,n_iâ‰¥0,\,âˆ‘_{i=1}^D n_i=d}.$$

---
# Kernels

Note that the RBF kernel is a function of distance â€“ it â€œweightsâ€ more similar
examples more strongly. We could interpret it as an extended version of
k-nearest neighbor algorithm, one which considers all examples, each weighted by
similarity.

~~~
For illustration, we plot RBF kernel values of three points $(0, -1)$, $(1, 1)$
and $(1, -1)$ with different values of $Î³$:

![w=80%,h=center](rbf_kernel.svgz)

---
section: SVM
# Support Vector Machines

Let us return to a binary classification task. The perceptron algorithm
guaranteed finding some separating hyperplane if it existed (but it could
find quite a bad one).

~~~
We now consider finding the one with **maximum margin**.

![w=90%,h=center,mh=82%,v=bottom](svm_margin.svgz)

---
# Support Vector Machines

Assume we have a dataset $â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{-1, 1\}^N$, a feature map $Ï†$
and a model
$$y(â†’x) â‰ â†’Ï†(â†’x)^T â†’w + b.$$

~~~
![w=30%,f=right](../03/binary_classification.svgz)

We already know that the distance of a point $â†’x_i$ to the decision boundary is
$$\frac{|y(â†’x_i)|}{\|â†’w\|}
  \stackrel{\substack{\textrm{assuming~}y\textrm{~classifies}\\\textrm{all~}â†’x_i\textrm{~correctly}}}
    {=\mathrel{\mkern-2.5mu}=\mathrel{\mkern-2.5mu}=\mathrel{\mkern-2.5mu}=\mathrel{\mkern-2.5mu}=\mathrel{\mkern-2.5mu}=\mathrel{\mkern-2.5mu}=}
  \frac{t_i y(â†’x_i)}{\|â†’w\|}.$$

~~~
We therefore want to maximize
$$\argmax_{â†’w,b} \frac{1}{\|â†’w\|} \min_i \Big[t_i \big(â†’Ï†(â†’x_i)^T â†’w + b\big)\Big].$$

However, this problem is difficult to optimize directly.

---
# Support Vector Machines

Because the model is invariant to multiplying $â†’w$ and $b$ by a constant, we can
decide that for the points closest to the decision boundary, it will hold that
$$t_i y(â†’x_i) = 1.$$

~~~
Then for all the points we will have $t_i y(â†’x_i) â‰¥ 1$ and we can simplify
$$\argmax_{â†’w,b} \frac{1}{\|â†’w\|} \min_i \Big[t_i \big(â†’Ï†(â†’x_i)^T â†’w + b\big)\Big].$$
to
$$\argmin_{â†’w,b} \frac{1}{2} \|â†’w\|^2 \textrm{~~given that~~}t_i y(â†’x_i) â‰¥ 1.$$

---
section: KKT
# Constrained Optimization â€“ Inequality Constraints

![w=30%,f=right](../05/lagrange_example.png)

Given a function $f(â†’x)$, we can find its minimum with respect to a vector
$â†’x âˆˆ â„^d$, by investigating the critical points $âˆ‡_{â†’x} f(â†’x) = 0$.

~~~
We even we can incorporate constraints of form $g(â†’x) = 0$ by forming a Lagrangian
$$ğ“›(â†’x, Î») = f(â†’x) - Î»g(â†’x)$$
and again investigating the critical points $âˆ‡_{â†’x,Î»} ğ“›(â†’x, Î») = 0$.

~~~
We now describe how to include inequality constraints $g(â†’x) â‰¥ 0$.

---
# Constrained Optimization â€“ Inequality Constraints

<div style="float: right; width: 28%">![w=95%,h=right](lagrange_inequalities.svgz)<br>![w=100%](lagrange_inequalities_cases.svgz)</div>

Our goal is to find a minimum of $f(â†’x)$ subject to a constraint $g(â†’x) â‰¥ 0$.

We start by again forming a Lagrangian $f(â†’x) - Î»g(â†’x)$.

~~~
The optimum can either be attained for $g(â†’x) > 0$, when the constraint is said
to be **inactive**, or for $g(â†’x) = 0$, when the constraint is said to be
**active**.
~~~
In the inactive case, the minimum is again a critical point of the Lagrangian
with the condition $Î»=0$.

~~~
When minimum is on a boundary, it corresponds to a critical point
with $Î»â‰ 0$ â€“ but note that this time the sign of the multiplier matters, because
minimum is attained only when gradient of $f(â†’x)$ is oriented **into** the region
$g(â†’x) â‰¥ 0$. We therefore require $âˆ‡f(â†’x) = Î»âˆ‡g(â†’x)$ for $Î»>0$.

~~~
In both cases, $Î» g(â†’x) = 0$.

---
# Minimization â€“ Inequality Constraint

Let $f(â†’x): â„^d â†’ â„$ be a function, which has a minimum in $â†’x$ subject to
an inequality constraint $g(â†’x) â‰¥ 0$. Assume that both $f$ and $g$ have continuous
partial derivatives and that $\frac{âˆ‚g}{âˆ‚x}(â†’x) â‰  0$.

~~~
![w=30%,f=right](lagrange_inequalities_cases.svgz)

Then there exists a $Î» âˆˆ â„$, such that the **Lagrangian function**
$$ğ“›(â†’x, Î») â‰ f(â†’x) - Î»g(â†’x)$$
has zero gradient in $â†’x$ and the following conditions hold:
$$\begin{aligned}
g(â†’x) &â‰¥ 0, \\
Î» &â‰¥ 0, \\
Î» g(â†’x) &= 0.
\end{aligned}$$

These conditions are known as **Karush-Kuhn-Tucker (KKT)** conditions.

---
# Minimization â€“ Inequality Constraint

It is easy to verify that if we have the minimum $â†’x$ and $Î»$ fulfilling
the KKT conditions $g(â†’x) â‰¥ 0, Î» â‰¥ 0, Î» g(â†’x) = 0,$
the Langrangian $ğ“›$ has a **maximum** in $Î»$:
~~~
- if $g(â†’x) = 0$, then $ğ“›$ does not change when changing $Î»$,
~~~
- if $g(â†’x) > 0$, then $Î»=0$ from the KKT conditions, which is a maximum of $ğ“›$.

~~~
On the other hand, if we have the minimum $â†’x$, $Î» â‰¥ 0$ and $ğ“›$ has a maximum
in $Î»$, all the KKT conditions must hold:
~~~
- if $g(â†’x) < 0$, then increasing $Î»$ would increase $ğ“›$,
~~~
- if $g(â†’x) > 0$, then decreasing $Î»$ increase $ğ“›$, so $Î» = 0$.

~~~
#### Maximizing Given $f(â†’x)$

If we instead want to find constrained maximum of $f(â†’x)$, we can search for
the minimum of $-f(â†’x)$, which results in the Lagrangian $f(â†’x) + Î»g(â†’x)$,
which we _minimize_ with respect to $Î»$.

---
# Necessary and Sufficient KKT Conditions

The KKT conditions are necessary conditions for a minimum (resp. a maximum).

~~~
However, it can be proven that in the following settings, the conditions are
also **sufficient**:
- if the objective to optimize is a _convex_ function (resp. _concave_ for maximization)
  with respect to $â†’x$;
~~~
- the inequality constraints are continuously differentiable convex functions;
~~~
- the equality constraints are affine functions (linear functions with an
  offset).

~~~
Therefore, if the above holds and if we find $â†’x$ and $Î»$ such that:
- $\frac{âˆ‚ğ“›}{âˆ‚â†’x} = 0$,
~~~
- either
  - $g(â†’x) â‰¥ 0$, $Î» â‰¥ 0$, $Î» g(â†’x) = 0$,
~~~
  - or $Î» â‰¥ 0$ and $ğ“›$ has a _maximum_ in ğ“›,

~~~
then $â†’x$ is a minimum of the function $f(â†’x)$ subject to an inequality
constraint $g(â†’x) â‰¥ 0$.

~~~
It is easy to verify that these conditions hold for the SVM optimization
problem.

---
section: Dual SVM Formulation
# Support Vector Machines

In order to solve the constrained problem of
$$\argmin_{â†’w,b} \frac{1}{2} \|â†’w\|^2 \textrm{~~given that~~}t_i y(â†’x_i) â‰¥ 1,$$
~~~
we write the Lagrangian with multipliers $â†’a=(a_1, â€¦, a_N)$ as
$$ğ“› = \frac{1}{2} \|â†’w\|^2 - âˆ‘_i a_i \big[t_i y(â†’x_i) - 1\big].$$

~~~
Setting the derivatives with respect to $â†’w$ and $b$ to zero, we get
$$\begin{aligned}
â†’w =& âˆ‘_i a_i t_iÏ†(â†’x_i), \\
 0 =& âˆ‘_i a_i t_i. \\
\end{aligned}$$

---
# Support Vector Machines

Substituting these to the Lagrangian, we get
$$ğ“› = âˆ‘\nolimits_i a_i -  \frac{1}{2} âˆ‘\nolimits_i âˆ‘\nolimits_j a_i a_j t_i t_j K(â†’x_i, â†’x_j)$$
with respect to the constraints $a_i â‰¥ 0$, $âˆ‘_i a_i t_i = 0$
and a kernel $K(â†’x, â†’z) = Ï†(â†’x)^T Ï†(â†’z).$

~~~
The solution of this Lagrangian will fulfil the KKT conditions, meaning that
$$\begin{aligned}
a_i &â‰¥ 0, \\
t_i y(â†’x_i) - 1 &â‰¥ 0, \\
a_i \big(t_i y(â†’x_i) - 1\big) &= 0.
\end{aligned}$$

~~~
Therefore, either a point $â†’x_i$ is on a boundary, or $a_i=0$. Given that the
prediction for $â†’x$ is $y(â†’x) = âˆ‘_i a_i t_i K(â†’x, â†’x_i) + b$,
we only need to keep the training points $â†’x_i$ that are on the boundary, the
so-called **support vectors**. Therefore, even though SVM is a nonparametric
model, it needs to store only a subset of the training data.

---
# Support Vector Machines

The dual formulation allows us to use non-linear kernels.

![w=100%](svm_gaussian.svgz)
