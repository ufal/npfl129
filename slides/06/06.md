title: NPFL129, Lecture 6
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Kernel Methods, SVM

## Milan Straka

### November 09, 2019

---
section: KernelLR
# Kernel Linear Regression

Consider linear regression with cubic features
$$Ï†(â†’x) = \scriptsize\begin{bmatrix} 1 \\ x_1 \\ x_2 \\ â€¦ \\ x_1^2 \\ x_1x_2 \\ â€¦ \\ x_2x_1 \\ â€¦ \\ x_1^3 \\ x_1^2x_2 \\ â€¦ \end{bmatrix}.$$

~~~
The SGD update for linear regression is then
$$â†’w â† â†’w + Î±\big(t - â†’w^T Ï†(â†’x)\big) Ï†(â†’x).$$

---
# Kernel Linear Regression

When dimensionality of input is $D$, one step of SGD takes $ğ“(D^3)$.

~~~
Surprisingly, we can do better under some circumstances. We start by
noting that we can write the parameters $â†’w$ as a linear combination
of the input features $Ï†(â†’x_i)$.

~~~
By induction, $â†’w = 0 = âˆ‘_i 0 â‹… Ï†(â†’x_i)$, and assuming $â†’w = âˆ‘_i Î²_i â‹… Ï†(â†’x_i)$,
after a SGD update we get
$$\begin{aligned}
â†’w â†& â†’w + Î±âˆ‘_i \big(t_i - â†’w^T Ï†(â†’x_i)\big) Ï†(â†’x_i)\\
   =& âˆ‘_i \Big(Î²_i + Î± \big(t_i - â†’w^T Ï†(â†’x_i)\big)\Big) Ï†(â†’x_i).
\end{aligned}$$

~~~
A individual update is $Î²_i â† Î²_i + Î±\Big(t_i - â†’w^T Ï†(â†’x_i)\Big)$, and
substituting for $â†’w$ we get
$$Î²_i â† Î²_i + Î±\Big(t_i - âˆ‘\nolimits_j Î²_j Ï†(â†’x_j)^T Ï†(â†’x_i)\Big).$$

---
# Kernel Linear Regression

We can formulate an alternative linear regression algorithm (a so-called
**dual formulation**):

<div class="algorithm">

**Input**: Dataset ($â‡‰X = \{â†’x_1, â†’x_2, â€¦, â†’x_N\} âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ â„^N$), learning rate $Î± âˆˆ â„^+$.<br>

- Set $Î²_i â† 0$
- Compute all values $K(â†’x_i, â†’x_j) = Ï†(â†’x_i)^T Ï†(â†’x_j)$
- Repeat until convergence
  - Update the coordinates, either according to a full gradient update:
    - $â†’Î² â† â†’Î² + Î±(â†’t-Kâ†’Î²)$
  - or alternatively use single-batch SGD, arriving at:
    - for $i$ in random permutation of $\{1, â€¦, N\}$:
      - $Î²_i â† Î²_i + Î±\Big(t_i - âˆ‘\nolimits_j Î²_j K(â†’x_i, â†’x_j)\Big)$
</div>

~~~
The predictions are then performed by computing
$$y(â†’x) = â†’w^T Ï†(â†’x) = âˆ‘\nolimits_i Î²_i â†’Ï†(â†’x_i)^T â†’Ï†(â†’x).$$

---
section: Kernels
# Kernel Trick

A single SGD update of all $Î²_i$ then takes $ğ“(N^2)$, given that we can
evaluate scalar dot product of $Ï†(â†’x_j)^T Ï†(â†’x_i)$ quickly.

~~~
$$\begin{aligned}
Ï†(â†’x)^T Ï†(â†’z) =& 1 + âˆ‘_i x_i z_i + âˆ‘_{i,j} x_i x_j z_i z_j + âˆ‘_{i,j,k} x_i x_j x_k z_i z_j z_k \\
              =& 1 + âˆ‘_i x_i z_i + \Big(âˆ‘_i x_i z_i\Big)^2 + \Big(âˆ‘_i x_i z_i\Big)^3 \\
              =& 1 + â†’x^T â†’z + \big(â†’x^T â†’z\big)^2 + \big(â†’x^T â†’z\big)^3.
\end{aligned}$$

---
# Kernels

We define a _kernel_ corresponding to a feature map $Ï†$ as a function
$$K(â†’x, â†’z) â‰ Ï†(â†’x)^t Ï†(â†’z).$$

~~~
There is quite a lot of theory behind kernel construction. The most often used
kernels are:

~~~
- polynomial kernel or degree $d$
  $$K(â†’x, â†’z) = (Î³ â†’x^Tâ†’z)^d,$$
  which corresponds to a feature map generating all combinations of exactly
  $d$ input features;
~~~
- polynomial kernel or degree at most $d$
  $$K(â†’x, â†’z) = (Î³ â†’x^Tâ†’z + 1)^d,$$
  which corresponds to a feature map generating all combinations of up to
  $d$ input features;

---
# Kernels

- Gaussian (or RBF) kernel
  $$K(â†’x, â†’z) = e^{-Î³||â†’x-â†’z||^2},$$
  corresponding to a scalar product in an infinite-dimensional space (it is
  in a sense a combination of polynomial kernels of all degrees, details will
  appear later).

---
section: SVM
# Support Vector Machines

Let us return to a binary classification task. The perceptron algorithm
guaranteed finding some separating hyperplane if it existed; we now consider
finding the one with _maximum margin_.

![w=100%,h=center](svm_margin.svgz)

---
# Support Vector Machines

Assume we have a dataset $â‡‰X âˆˆ â„^{NÃ—D}$, $â†’t âˆˆ \{-1, 1\}^N$, feature map $Ï†$ and model
$$y(â†’x) â‰ â†’Ï†(â†’x)^T â†’w + b.$$

~~~
![w=30%,f=right](../03/binary_classification.svgz)

We already know that the distance of a point $â†’x_i$ to the decision boundary is
$$\frac{|y(â†’x_i)|}{||â†’w||} = \frac{t_i y(â†’x_i)}{||â†’w||}.$$

~~~
We therefore want to maximize
$$\argmax_{w,b} \frac{1}{||â†’w||} \min_i \big[t_i (â†’Ï†(â†’x)^T â†’w + b)\big].$$

However, this problem is difficult to optimize directly.

---
# Support Vector Machines

Because the model is invariant to multiplying $â†’w$ and $b$ by a constant, we can
say that for the points closest to the decision boundary, it will hold that
$$t_i y(â†’x_i) = 1.$$

~~~
Then for all the points we will have $t_i y(â†’x_i) â‰¥ 1$ and we can simplify
$$\argmax_{w,b} \frac{1}{||â†’w||} \min_i \big[t_i (â†’Ï†(â†’x)^T â†’w + b)\big]$$
to
$$\argmin_{w,b} \frac{1}{2} ||â†’w||^2 \textrm{~given that~~}t_i y(â†’x_i) â‰¥ 1.$$

---
section: KKT
# Lagrange Multipliers â€“ Inequality Constraints

Given a funtion $J(â†’x)$, we can find a maximum with respect to a vector
$â†’x âˆˆ â„^d$, by investigating the critical points $âˆ‡_â†’x J(â†’x) = 0$.

We even know how to incorporate constraints of form $g(â†’x) = 0$.

~~~
![w=25%,f=right](lagrange_inequalities.svgz)

We now describe how to include inequality constraints $g(â†’x) â‰¥ 0$.

~~~
The optimum can either be attained for $g(â†’x) > 0$, when the constraint is said
to be _inactive_, or for $g(â†’x) = 0$, when the constraint is sayd to be
_active_.

~~~
In the inactive case, the maximum is again a critical point of the Langrangian,
with $Î»=0$.
~~~
When maximum is on boundary, it corresponds to a critical point
with $Î»â‰ 0$ â€“ but note that this time the sign of the multiplier matters, because
maximum is attained only when gradient of $f(â†’x)$ is oriented away from the region
$g(â†’x) â‰¥ 0$. We therefore require $âˆ‡f(â†’x) = - Î»âˆ‡g(â†’x)$ for $Î»>0$.

~~~
In both cases, $Î» g(â†’x) = 0$.

---
section: KKT
# Karush-Khun-Tucker Conditions

![w=25%,f=right](lagrange_inequalities.svgz)

Therefore, the solution to a maximization problem of $f(x)$ subject to $g(â†’x)â‰¥0$
can be found by inspecting all points where the derivation of the Lagrangian is zero,
subject to the following conditions:
$$\begin{aligned}
g(â†’x) &â‰¥ 0 \\
Î» &â‰¥ 0 \\
Î» g(â†’x) &= 0
\end{aligned}$$

~~~
# Necessary and Sufficient KKT Conditions

The above conditions are necessary conditions for a minimum. However, it can be
proven that in the following settings, the conditions are also **sufficient**:
- if the objective to optimize is a _convex_ function,
~~~
- the inequality constraings are continuously differentiable convex functions,
~~~
- the equality constraints are affine functions (linear functions with an
  offset).


---
section: Dual SVM Formulation
# Support Vector Machines

In order to solve the constrained problem of
$$\argmin_{w,b} \frac{1}{2} ||â†’w||^2 \textrm{~given that~~}t_i y(â†’x_i) â‰¥ 1,$$
~~~
we write the Lagrangian with multipliers $â†’a=(a_1, â€¦, a_N)$ as
$$L = \frac{1}{2} ||â†’w||^2 - âˆ‘_i a_i \big[t_i y(â†’x_i) - 1\big].$$

~~~
Setting the derivatives with respect to $â†’w$ and $b$ to zero, we get
$$\begin{aligned}
â†’w =& âˆ‘_i a_i t_iÏ†(â†’x_i) \\
 0 =& âˆ‘_i a_i t_i \\
\end{aligned}$$

---
# Support Vector Machines

Substituting these to the Lagrangian, we get
$$L = âˆ‘_i a_i -  \frac{1}{2} âˆ‘_i âˆ‘_j a_i a_j t_i t_j K(â†’x_i, â†’x_j)$$
with respect to the constraints $âˆ€_i: a_i â‰¥ 0$, $âˆ‘_i a_i t_i = 0$
and kernel $K(â†’x, â†’z) = Ï†(â†’x)^T Ï†(â†’z).$

~~~
The solution of this Lagrangian will fulfil the KKT conditions, meaning that
$$\begin{aligned}
a_i &â‰¥ 0 \\
t_i y(â†’x_i) - 1 &â‰¥ 0 \\
a_i \big(t_i y(â†’x_i) - 1\big) &= 0.
\end{aligned}$$

~~~
Therefore, either a point is on a boundary, or $a_i=0$. Given that the
predictions for point $â†’x$ are given by $y(â†’x) = âˆ‘ a_i t_i K(â†’x, â†’x_i) + b$,
we need to keep only the points on the boundary, the so-called **support vectors**.

---
# Support Vector Machines

The dual formulation allows us to use non-linear kernels.

![w=100%](svm_gaussian.svgz)
