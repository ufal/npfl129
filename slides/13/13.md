title: NPFL129, Lecture 13
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Statistical Hypothesis Testing, Model Comparison

## Milan Straka

### January 04, 2021

---
section: StatisticalTesting
# Statistical Hypothesis Testing

Variation of a famous saying states, that there are various kinds of truth:

~~~
- truth,

~~~
- half-truth,
~~~
- lie,
~~~
- disgusting lie,
~~~
- and statistics.

---
# Statistical Hypothesis Testing

Assume we have a hypothesis testable using observed data of random variables.

~~~
There are two slightly differing views on statistical hypothesis testing:

~~~
1. In the first one, we assume we have a **null hypothesis** $H_0$, and we
   are interested in whether we can **reject it** using the observed data.

~~~
   The result is **statistically significant**, if it is very unlikely
   that the observed data have occurred given the null hypothesis.

~~~
   The **significance level** of a test is the threshold of this unlikeliness.

~~~
2. In the second view, we have two hypotheses, a null hypothesis $H_0$
   and an **alternative hypothesis** $H_1$, and we want to distinguish among
   them.

~~~
   We consider only two outcomes of the test:
   - either we “reject” the null hypothesis, if the data is very unlikely
   to have occurred given the null hypothesis; or
~~~
   - we cannot reject the null hypothesis.

~~~
   Note that we never “prove” the alternative hypothesis.

---
class: tablewide
# Type I and Type II Errors

Consider the _courtroom trial_ example, which is similar to a criminal trial,
where the defendant is considered not guilty until their guilt is proven.

~~~
In this setting, $H_0$ is “not guilty” and $H_1$ is “guilty”.

~~~
| | $H_0$ is true<br>Truly not guilty | $H_1$ is true<br>Truly guilty |
|-|-----------------------------------|-------------------------------|
| Not proven guilty<br>Not rejecting $H_0$ | Correct decision<br>True negative | Wrong decision<br>False negative<br>**Type II Error** |
| Proven guilty<br>Rejecting $H_0$ | Wrong decision<br>False positive<br>**Type I Error** | Correct decision<br>True positive |

~~~
Our goal is to limit the Type 1 errors – the test **significance level** is the
type 1 error rate.

---
# Match Analogy

I like the following analogy – if you have a theory and want to convince others
that it holds, you devise an _opponent_ for it and let them wrestle.

~~~
If your theory wins, it may be an indication that it really holds.

~~~
However, you must choose an appropriate opponent.

![w=49%](opponent_weak.png)
~~~
![w=44%,mw=49%,h=center](opponent_strong.png)

---
# Statistical Hypothesis Testing

The crucial part of a statistical test is the **test statistic**. It is some
summary of the observed data, very often a single value (like mean), which
can be used to distinguish the null and the alternative hypothesis.

~~~
It is crucial to be able to compute the distribution of the test statistic,
which allows the **p-values** to be calculated.

~~~
A **p-value** is the probability of obtaining test statistic value at least as extreme
as the one actually observed, assuming validity of the null hypothesis.
A very small p-value indicates that the observed data are very unlikely under
the null hypothesis.

~~~
Given a test statistic, we usually perform one of

![w=13%,f=right](test_right_tail.svgz)

- a one-sided right-tail test, when the p-value of $t$ is $P(\mathrm{test~statistic} > t | H_0)$;

~~~
![w=13%,f=right;clear:both](test_left_tail.svgz)

- a one-sided left-tail test, when the p-value of $t$ is $P(\mathrm{test~statistic} < t | H_0)$;
~~~
![w=13%,f=right;clear:both](test_two_sided.svgz)

- a two-sides test, when the p-value of $t$ is twice the minimum of
  $P(\mathrm{test~statistic} < t | H_0)$ and $P(\mathrm{test~statistic} > t | H_0)$.

---
# Statistical Hypothesis Testing

Therefore, the whole procedure consists of the following steps:
~~~
1. Formulate the null hypothesis $H_0$, and optionally the alternative
   hypothesis $H_1$.

~~~
1. Choose the test statistic.
~~~
1. Compute the observed value of the test statistic.
~~~
1. Calculate the p-value, which is the probability of a test
   statistic value at least as extreme as the observed one,
   under the null hypothesis $H_0$.
~~~
1. Reject the null hypothesis $H_0$ (in favor of the alternative hypothesis
   $H_1$), if the p-value is less than the chosen significance level $α$ (0.5%
   and 0.1% are common choices).

---
# Test Statistics

There are several kinds of test statistics:
~~~
- **one-sample tests**, where we sample values from one distribution.

~~~
  Common one-sample tests usually check for
  - the mean of the distribution to be greater/ than and/or equal to zero;
  - the goodness of fit (that the data comes from a normal or categorical
    distribution of given parameters).

~~~
- **two-sample tests**, where we sample independently from two distributions.

~~~
- **paired tests**, in which case we also sample from two distributions, but
  the samples are paired (i.e., evaluating several models on the same data).

~~~
  In paired tests, we usually compute the difference between the paired members
  and perform one-sample test on the mean of the differences.

---
# Test Statistics

There are many commonly used test statistics, with different requirements
and conditions. We only mention several commonly-used ones, but it is by no means
a comprehensive treatment.

~~~
- **Z-Test** is a test, where the test statistic can be approximated by a normal
  distribution. For example, it can be used when comparing a mean of samples
  _with known variance_ to a given value.

~~~
- In **Student's _t_-test** the test statistic follow a Student's _t_-distribution
  (where Student is the pseudonym used by the real author W. S. Gosset), which
  is the distribution of a sample mean of normally-distributed population _with
  unknown variance_.

~~~
  Therefore, the _t_-test is used when comparing a mean of
  samples with unknown variance to a given value, or to a mean of samples from
  another distribution with the same sample size and variance.

~~~
- **Chi-squared test** utilizes a test statistic with a chi-squared
  distribution, which is a distribution of a sum of squares of $k$
  independent normally distributed variables.

~~~
  The essential Pearson's chi-squared test can be used to evaluate a goodness of
  fit of $k$ random categorical samples with respect to a given categorical
  distribution.

---
section: MultipleComparisons
# Multiple Comparisons Problem

A **multiple comparisons problem** (or multiple testing problem) arises, if we
consider many statistical hypotheses tests using the same observed data.

~~~
![w=72%,h=center](xkcd_significant_1.png)

---
# Multiple Comparisons Problem

![w=70%,h=center](xkcd_significant_2.png)

---
# Multiple Comparisons Problem

![w=70%,h=center](xkcd_significant_3.png)

---
# Multiple Comparisons Problem

![w=90%,mw=45%,f=left](xkcd_significant_4.png)

~~~
Usually, the problem is that we perform many statistical tests and only report
the ones with statistically significant results.

~~~
![w=54%,mh=60%,v=bottom](spurious_correlation.svgz)

---
# Multiple Comparisons Problem: Post-Mortem Salmon Study

Even world-class researchers make mistakes in multiple comparisons problem.
Consider the paper
> _Neural Correlates of Interspecies Perspective Taking in the Post-Mortem
> Atlantic Salmon: An Argument For Proper Multiple Comparisons Correction_

~~~
Functional magnetic resonance imaging (fMRI) is a technique for monitoring brain
activity via measuring the changes in blood oxygenation. The measurement is
performed for every _voxel_ in the brain; the authors claim that 130k voxels are
common in a single fMRI measurement.

~~~
The correlation is usually computed for every voxel, and usually a cluster of
some number of neighboring voxels, all of which must pass statistically
significant test, is required.

~~~
However, with such a large number of voxels, a positive result can be caused by
chance without some multiple comparison correction.

---
# Multiple Comparisons Problem: Post-Mortem Salmon Study

The authors perform the following experiment. Citing:

> _One mature Atlantic Salmon (Salmo salar) participated in the fMRI study. The
> salmon measured approximately 18 inches long, weighed 3.8 lbs, and was not
> alive at the time of scanning. It is not known if the salmon was male or
> female, but given the post-mortem state of the subject this was not thought to
> be a critical variable._
~~~

> …

> _The task administered to the salmon involved completing an open-ended
> mentalizing task. The salmon was shown a series of photographs depicting human
> individuals in social situations with a specified emotional valence, either
> socially inclusive or socially exclusive. The salmon was asked to determine
> which emotion the individual in the photo must have been experiencing._

---
# Multiple Comparisons Problem: Post-Mortem Salmon Study

![w=56%,h=center](dead_salmon.svgz)

_A t-contrast was used to test for regions with significant BOLD signal change
during the presentation of photos as compared to rest. The relatively low extent threshold value was chosen due to
the small size of the salmon’s brain relative to voxel size. Several active
voxels were observed in a cluster located within the salmon’s brain cavity
(see Fig. 1). The size of this cluster was 81 mm$^3$ with a cluster-level
significance of $p = 0.001$._

---
# Multiple Comparisons Problem: Post-Mortem Salmon Study

The authors claim that:

> _Sadly, while methods for multiple comparisons correction are included in
> every major neuroimaging software package these techniques are not always
> invoked in the analysis of functional imaging data. For the year 2008 only 74%
> of articles in the journal NeuroImage reported results from a general linear
> model analysis of fMRI data that utilized multiple comparisons correction
> (193/260 studies). Other journals we examined were Cerebral Cortex (67.5%,
> 54/80 studies), Social Cognitive and Affective Neuroscience (60%, 15/25
> studies), Human Brain Mapping (75.4%, 43/57 studies), and the Journal of
> Cognitive Neuroscience (61.8%, 42/68 studies).
> …
> The issue is not limited to published articles, as proper multiple comparisons
> correction is somewhat rare during neuroimaging conference presentations.
> During one poster session at a recent neuroscience conference only 21% of the
> researchers used multiple comparisons correction in their research (9/42).
> A further, more insidious problem is that some researchers would apply
> correction to some contrasts but not to others depending on the results of
> each comparison._

---
section: FWER
# Family-Wise Error Rate

There are several ways to handle the multiple comparison problem; one of the
easiest (but often overly conservative) is to limit the **family-wise error
rate**, which is the probability of at least one type 1 error in the family.
$$\mathrm{FWER} = P\bigg(⋃_i \Big(p_i ≤ α\Big)\bigg).$$

~~~
One way of controlling the family-wise error rate is the **Bonferroni
correction**, which rejects the null hypothesis of a test in the family of
size $m$ when $p_i ≤ \frac{α}{m}$.

~~~
Assuming such a correction and utilizing the Boole's inequality
$P\big(⋃\nolimits_i A_i\big) ≤ ∑_i P(A_i)$, we get that
$$\mathrm{FWER} = P\bigg(⋃_i \Big(p_i ≤ \frac{α}{m}\Big)\bigg) ≤ ∑_i P\Big(p_i ≤ \frac{α}{m}\Big) = m \cdot \frac{α}{m} = m.$$

~~~
Note that there exist other more powerful methods like Holm-Bonferroni or Šidák correction.

---
section: ModelComparison
# Model Comparison

The goal of model comparison is to test whether some model will deliver better
perfomance on unseen data than another one.

~~~
However, we usually only have a single fixed-size test set. For the rest of the
lecture, we assume the test set instances are independently sampled from the
data generating distribution.

~~~
Even if comparing the models on the given test set is unbiased, we would like
to obtain some significance level of the result.

~~~
Therefore, we perform a statistical test with alternative hypothesis that
a model $y$ is better than a model $z$; therefore, the null hypothesis is that
the model $y$ is the same or worse than the model $z$.

~~~
However, we only have one sample (the result of a model on the test set). We
therefore turn to **bootstrap resampling**.

---
section: BootstrapResampling
# Bootstrap Resampling

In order to obtain multiple samples of model performance, we exploit the fact
that the test set consists of _multiple_ examples.

~~~
Therefore, we can generate different test sets by bootstrap resampling. Notably,
we obtain a same-sized test set by sampling the original test set examples _with
replacement_. Naturally, we can easily measure the performance of any given
model on such generated test set.

~~~

<div class="algorithm">

**Input**: Test set $\{(→x_1, t_1), …, (→x_N, t_N)\}$, model predictions $\{y(→x_1), …, y(→x_N)\}$,
a metric $E$, number of resamplings $R$.<br>
**Output**: $R$ samples of model performance.

~~~
- $\mathrm{performances} ← []$
- repeat $R$ times:
  - sample $N$ test set examples with replacements, together with corresponding
    model predictions
  - measure the performance of the sampled data using the metric $E$ and append the
    result to $\mathrm{performances}$
</div>

---
# Bootstrap Resampling – Confidence Intervals

When using bootstrap resampling on a single model, we can measure the
confidence intervals of model performance.

~~~
For a given confidence level (95\% is the most common value), the **confidence
interval** is an estimate of a value range of some unknown parameter (like
a mean performance of some model on unseen data), such that the confidence
interval contains the true value of the unknown parameter with the frequency
given by the confidence level.

~~~
When given the empirical distribution of model performances produced by
bootstrap resampling, we can estimate the 95% confidence interval as
a range from the 2.5 percentile and 97.5 percentile of the empirical
distribution (the so-called _percentile bootstrap_).

---
section: PairedBootstrapT
# Paired Bootstrap Test

To perform the model comparison statistical test, we could use a two-sample
test. However, such a test does not consider the fact that some of the inputs
might be more difficult than others, and takes into account cases when a
weaker model achieves higher performance on a simpler test set than a stronger
model on a more difficult test set.

~~~
Instead, we perform a paired bootstrap test. Our alternative hypothesis is that
the mean of the model performance differences is larger than zero, and the null
hypothesis is that it is less or equal to zero. We than repeatedly sample a test
set with repetition and compute the difference of the model performances on the
sampled test set, obtaining a distribution of differences _under the true distribution_.

~~~
However, to perform the statistical test, we require the distribution of the
differences _under the null hypothesis_. One way of obtaining it is to assume
that the distribution of differences is either normal or translation invariant,
under which assumption we obtain the wanted distribution as the mean-centered
bootstrap distribution. Finally, assuming symmetry, we can estimate the p-value
as the ratio of the bootstrapped differences which are less or equal to zero.
(See _permutation tests_ for a different way of estimating the p-values.)

---
# Paired Bootstrap Test

<div class="algorithm">

**Input**: Test set $\{(→x_1, t_1), …, (→x_N, t_N)\}$, model predictions $\{y(→x_1), …, y(→x_N)\}$,<br>
model predictions $\{z(→x_1), …, z(→x_N)\}$, a metric $E$, number of resamplings $R$.<br>
**Output**: Estimated p-value assuming that the model $y$ performance is worse or equal to $z$.

~~~
- $\mathrm{differences} ← []$
- repeat $R$ times:
  - sample $N$ test set examples with replacements, together with the
    corresponding predictions of the models
  - measure the performances of the models $y$ and $z$ on the sampled data using
    the metric $E$ and append their difference to $\mathrm{differences}$
- return the ratio of the $\mathrm{differences}$ which are less or equal to zero
</div>


---
# Paired Boostrap Test

For illustration, consider models for the `isnt_it_ironic` competition
utilizing either 3 (red) or 4 (green) in-word character n-grams. On the left,
there are distributions of the individual model performances, while on the right
there is a distribution of their differences.

![w=50%](bootstrap_char3_vs_char4_500.svgz)![w=48.5%](bootstrap_char3_vs_char4_500_differences.svgz)

The histograms are generated using 50 bins and 500 resamplings.

---
# Paired Boostrap Test

For illustration, consider models for the `isnt_it_ironic` competition
utilizing either 3 (red) or 4 (green) in-word character n-grams. On the left,
there are distributions of the individual model performances, while on the right
there is a distribution of their differences.

![w=50%](bootstrap_char3_vs_char4_5000.svgz)![w=49%](bootstrap_char3_vs_char4_5000_differences.svgz)

The histograms are generated using 50 bins and 5000 resamplings.

---
# Paired Boostrap Test

For illustration, consider models for the `isnt_it_ironic` competition
utilizing either 4 (red) or 5 (green) in-word character n-grams. On the left,
there are distributions of the individual model performances, while on the right
there is a distribution of their differences.

![w=50%](bootstrap_char4_vs_char5_5000.svgz)![w=49%](bootstrap_char4_vs_char5_5000_differences.svgz)

The histograms are generated using 50 bins and 5000 resamplings.
