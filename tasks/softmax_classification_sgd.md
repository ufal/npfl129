### Assignment: softmax_classification_sgd
#### Date: Deadline: Nov 11, 22:00
#### Points: 3 points
#### Tests: softmax_classification_sgd_tests
#### Examples: softmax_classification_sgd_examples

Starting with the [softmax_classification_sgd.py](https://github.com/ufal/npfl129/tree/past-2425/labs/04/softmax_classification_sgd.py),
implement minibatch SGD for multinomial logistic regression.

#### Tests Start: softmax_classification_sgd_tests
_Note that your results may be slightly different (because of varying floating point arithmetic on your CPU)._

1. `python3 softmax_classification_sgd.py --batch_size=10  --epochs=2 --learning_rate=0.005 --seed=244`
```
After epoch 1: train loss 0.4097 acc 87.6%, test loss 0.6056 acc 81.8%
After epoch 2: train loss 0.1842 acc 95.3%, test loss 0.2684 acc 92.6%
Learned weights:
  -0.04 0.04 0.06 0.09 -0.02 -0.12 -0.00 -0.10 0.06 0.04 ...
  -0.08 -0.09 -0.08 -0.08 -0.07 -0.02 -0.08 -0.01 0.07 -0.05 ...
  -0.05 -0.01 -0.01 0.04 -0.01 -0.08 0.07 0.09 0.03 0.04 ...
  -0.02 -0.07 0.08 0.02 -0.02 0.08 0.04 -0.00 -0.01 0.06 ...
  -0.09 -0.06 -0.02 -0.16 0.04 -0.15 -0.06 -0.09 -0.07 -0.08 ...
  -0.06 0.08 0.14 0.08 -0.04 0.12 0.02 -0.09 0.07 -0.01 ...
  0.10 -0.09 0.02 0.02 -0.08 -0.02 -0.02 -0.06 -0.03 -0.07 ...
  0.03 0.02 0.03 -0.05 0.06 0.01 0.08 0.09 -0.09 -0.05 ...
  0.03 0.07 0.02 -0.09 -0.05 -0.02 -0.08 0.09 -0.03 0.05 ...
  0.00 -0.09 0.10 -0.02 -0.05 -0.01 -0.04 -0.09 -0.04 -0.03 ...
```

2. `python3 softmax_classification_sgd.py --batch_size=1   --epochs=1 --learning_rate=0.005 --test_size=1597  --seed=244`
```
After epoch 1: train loss 1.3350 acc 77.5%, test loss 1.7405 acc 75.2%
Learned weights:
  -0.04 0.04 0.05 0.09 -0.00 -0.15 0.01 -0.10 0.06 0.04 ...
  -0.08 -0.09 -0.15 -0.18 -0.09 -0.02 -0.13 -0.01 0.08 -0.13 ...
  -0.05 0.04 0.14 0.16 0.00 -0.04 0.07 0.10 0.04 0.17 ...
  -0.02 -0.08 0.13 0.12 -0.01 0.08 0.06 -0.01 -0.01 0.10 ...
  -0.09 -0.07 -0.12 -0.21 -0.00 -0.25 -0.08 -0.09 -0.07 -0.14 ...
  -0.06 0.05 0.15 0.06 0.04 0.27 0.20 -0.08 0.07 -0.12 ...
  0.10 -0.08 0.02 -0.01 -0.17 -0.04 -0.02 -0.06 -0.03 -0.08 ...
  0.03 0.03 0.16 0.04 0.14 0.09 0.12 0.09 -0.09 0.01 ...
  0.03 0.05 -0.11 -0.23 -0.17 -0.13 -0.13 0.10 -0.03 0.03 ...
  0.00 -0.10 0.06 0.02 0.02 -0.02 -0.16 -0.10 -0.04 0.00 ...
```

3. `python3 softmax_classification_sgd.py --batch_size=100 --epochs=3 --learning_rate=0.05  --seed=244`
```
After epoch 1: train loss 1.8101 acc 79.4%, test loss 2.3757 acc 75.2%
After epoch 2: train loss 1.8213 acc 79.1%, test loss 2.3803 acc 75.0%
After epoch 3: train loss 0.2346 acc 93.6%, test loss 0.3357 acc 91.5%
Learned weights:
  -0.04 0.03 0.04 0.11 -0.01 -0.14 -0.01 -0.11 0.06 0.02 ...
  -0.08 -0.09 -0.18 -0.15 -0.14 0.02 -0.07 -0.01 0.07 -0.09 ...
  -0.05 0.00 0.06 0.08 -0.01 -0.14 0.05 0.09 0.03 0.09 ...
  -0.02 -0.07 0.18 0.07 0.04 0.18 0.08 -0.00 -0.01 0.14 ...
  -0.09 -0.06 -0.09 -0.25 0.04 -0.26 -0.11 -0.09 -0.07 -0.11 ...
  -0.06 0.09 0.21 0.10 0.01 0.25 0.08 -0.09 0.07 -0.01 ...
  0.10 -0.09 -0.03 0.03 -0.11 -0.06 -0.02 -0.06 -0.03 -0.10 ...
  0.03 0.03 0.04 -0.01 0.12 0.09 0.13 0.10 -0.09 -0.04 ...
  0.03 0.06 0.00 -0.13 -0.09 -0.11 -0.14 0.09 -0.03 0.02 ...
  0.00 -0.09 0.12 0.00 -0.08 -0.02 -0.03 -0.09 -0.04 -0.03 ...
```
#### Tests End:
#### Examples Start: softmax_classification_sgd_examples
_Note that your results may be slightly different (because of varying floating point arithmetic on your CPU)._

- `python3 softmax_classification_sgd.py --batch_size=10 --epochs=10 --learning_rate=0.005 --seed=244`
```
After epoch 1: train loss 0.4097 acc 87.6%, test loss 0.6056 acc 81.8%
After epoch 2: train loss 0.1842 acc 95.3%, test loss 0.2684 acc 92.6%
After epoch 3: train loss 0.1589 acc 95.4%, test loss 0.2366 acc 92.7%
After epoch 4: train loss 0.1509 acc 95.9%, test loss 0.2568 acc 91.2%
After epoch 5: train loss 0.1184 acc 96.6%, test loss 0.2067 acc 92.6%
After epoch 6: train loss 0.1052 acc 96.7%, test loss 0.1756 acc 94.6%
After epoch 7: train loss 0.0839 acc 97.2%, test loss 0.1704 acc 94.5%
After epoch 8: train loss 0.0898 acc 97.8%, test loss 0.1826 acc 94.0%
After epoch 9: train loss 0.0910 acc 97.3%, test loss 0.2099 acc 92.7%
After epoch 10: train loss 0.0717 acc 98.4%, test loss 0.1574 acc 95.4%
Learned weights:
  -0.04 0.04 0.06 0.10 -0.02 -0.14 -0.02 -0.11 0.06 0.03 ...
  -0.08 -0.09 -0.06 -0.03 -0.16 0.07 -0.07 -0.01 0.07 -0.07 ...
  -0.05 -0.00 0.02 0.02 -0.00 -0.05 0.07 0.09 0.03 0.07 ...
  -0.02 -0.07 0.04 0.03 0.06 0.10 0.03 -0.00 -0.01 0.08 ...
  -0.09 -0.06 -0.01 -0.21 -0.02 -0.19 -0.08 -0.09 -0.07 -0.07 ...
  -0.06 0.08 0.19 0.09 -0.03 0.16 0.06 -0.09 0.07 -0.01 ...
  0.10 -0.09 -0.01 -0.00 -0.08 -0.06 -0.02 -0.06 -0.03 -0.08 ...
  0.03 0.02 0.04 -0.05 0.07 0.03 0.11 0.10 -0.09 -0.05 ...
  0.03 0.07 -0.01 -0.09 -0.01 -0.06 -0.11 0.09 -0.03 0.02 ...
  0.00 -0.09 0.09 0.01 -0.05 -0.05 -0.03 -0.09 -0.04 -0.02 ...
```

- `python3 softmax_classification_sgd.py --batch_size=1 --epochs=10 --learning_rate=0.005 --test_size=1597 --seed=244`
```
After epoch 1: train loss 1.3350 acc 77.5%, test loss 1.7405 acc 75.2%
After epoch 2: train loss 0.4239 acc 89.0%, test loss 1.3121 acc 83.5%
After epoch 3: train loss 0.1724 acc 94.0%, test loss 0.8195 acc 85.7%
After epoch 4: train loss 0.4967 acc 91.0%, test loss 1.1503 acc 82.8%
After epoch 5: train loss 0.2448 acc 94.0%, test loss 0.6896 acc 88.0%
After epoch 6: train loss 0.0123 acc 99.5%, test loss 0.5975 acc 91.3%
After epoch 7: train loss 0.0113 acc 99.5%, test loss 0.5783 acc 91.0%
After epoch 8: train loss 0.2608 acc 92.5%, test loss 1.0018 acc 86.7%
After epoch 9: train loss 0.0182 acc 99.5%, test loss 0.5316 acc 91.7%
After epoch 10: train loss 0.0321 acc 99.0%, test loss 0.5806 acc 90.8%
Learned weights:
  -0.04 0.04 0.05 0.08 0.01 -0.18 -0.02 -0.10 0.06 0.01 ...
  -0.08 -0.11 0.03 -0.08 -0.48 0.12 -0.10 -0.01 0.08 -0.25 ...
  -0.05 0.02 0.12 0.14 -0.04 -0.13 0.04 0.10 0.04 0.14 ...
  -0.02 -0.05 0.05 0.03 0.24 0.33 0.17 -0.01 -0.01 0.14 ...
  -0.09 -0.07 -0.19 -0.29 -0.02 -0.33 -0.23 -0.11 -0.07 -0.15 ...
  -0.06 0.07 0.35 0.11 0.18 0.39 0.18 -0.08 0.07 -0.06 ...
  0.10 -0.08 -0.07 -0.17 -0.11 -0.03 -0.02 -0.06 -0.03 -0.10 ...
  0.03 0.03 0.15 0.08 0.22 0.12 0.24 0.12 -0.09 0.02 ...
  0.03 0.05 -0.32 -0.31 -0.25 -0.24 -0.16 0.10 -0.03 0.08 ...
  0.00 -0.11 0.17 0.26 0.02 -0.25 -0.18 -0.11 -0.04 0.06 ...
```

- `python3 softmax_classification_sgd.py --batch_size=100 --epochs=10 --learning_rate=0.05 --seed=244`
```
After epoch 1: train loss 1.8101 acc 79.4%, test loss 2.3757 acc 75.2%
After epoch 2: train loss 1.8213 acc 79.1%, test loss 2.3803 acc 75.0%
After epoch 3: train loss 0.2346 acc 93.6%, test loss 0.3357 acc 91.5%
After epoch 4: train loss 0.2589 acc 93.4%, test loss 0.4630 acc 88.5%
After epoch 5: train loss 0.1534 acc 96.0%, test loss 0.2766 acc 93.2%
After epoch 6: train loss 0.1057 acc 96.7%, test loss 0.2160 acc 93.7%
After epoch 7: train loss 0.1069 acc 96.7%, test loss 0.2495 acc 93.4%
After epoch 8: train loss 0.0955 acc 97.0%, test loss 0.2198 acc 93.7%
After epoch 9: train loss 0.0866 acc 97.5%, test loss 0.2234 acc 94.6%
After epoch 10: train loss 0.1401 acc 96.3%, test loss 0.2675 acc 93.0%
Learned weights:
  -0.04 0.03 0.04 0.11 -0.02 -0.16 -0.03 -0.11 0.06 0.01 ...
  -0.08 -0.09 -0.12 -0.05 -0.21 0.09 -0.07 -0.01 0.07 -0.09 ...
  -0.05 0.00 0.05 0.05 -0.00 -0.10 0.05 0.09 0.04 0.10 ...
  -0.02 -0.07 0.10 0.05 0.09 0.21 0.06 -0.00 -0.01 0.13 ...
  -0.09 -0.06 -0.06 -0.29 -0.03 -0.28 -0.14 -0.09 -0.07 -0.10 ...
  -0.06 0.09 0.25 0.10 -0.01 0.23 0.11 -0.09 0.07 -0.00 ...
  0.10 -0.09 -0.05 -0.02 -0.10 -0.08 -0.02 -0.06 -0.03 -0.11 ...
  0.03 0.03 0.05 0.00 0.13 0.08 0.14 0.11 -0.09 -0.04 ...
  0.03 0.06 -0.03 -0.13 -0.02 -0.10 -0.15 0.09 -0.03 0.02 ...
  0.00 -0.10 0.11 0.04 -0.06 -0.07 -0.02 -0.10 -0.04 -0.03 ...
```
#### Examples End:
