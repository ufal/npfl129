### Assignment: softmax_classification_sgd
#### Date: Deadline: Nov 8, 23:59
#### Points: 3 points
#### Examples: softmax_classification_sgd_examples
#### Tests: softmax_classification_sgd_tests

Starting with the [softmax_classification_sgd.py](https://github.com/ufal/npfl129/tree/master/labs/04/softmax_classification_sgd.py),
implement minibatch SGD for multinomial logistic regression.

#### Examples Start: softmax_classification_sgd_examples
_Note that your results may be slightly different (because of varying floating point arithmetic on your CPU)._
- `python3 softmax_classification_sgd.py --batch_size=10 --iterations=10 --learning_rate=0.005`
```
After iteration 1: train loss 0.3130 acc 90.8%, test loss 0.3529 acc 88.7%
After iteration 2: train loss 0.2134 acc 93.9%, test loss 0.2450 acc 92.5%
After iteration 3: train loss 0.1366 acc 96.8%, test loss 0.1735 acc 94.6%
After iteration 4: train loss 0.1374 acc 96.2%, test loss 0.1705 acc 94.0%
After iteration 5: train loss 0.1169 acc 97.2%, test loss 0.1667 acc 95.1%
After iteration 6: train loss 0.0978 acc 97.5%, test loss 0.1340 acc 96.1%
After iteration 7: train loss 0.0878 acc 98.0%, test loss 0.1366 acc 95.9%
After iteration 8: train loss 0.0889 acc 97.5%, test loss 0.1515 acc 95.1%
After iteration 9: train loss 0.0819 acc 98.0%, test loss 0.1336 acc 96.5%
After iteration 10: train loss 0.0801 acc 97.9%, test loss 0.1342 acc 96.4%
Learned weights:
  -0.03 -0.10 0.01 0.08 -0.05 0.01 -0.06 0.05 0.07 -0.10 ...
  0.09 0.07 -0.15 -0.02 -0.21 0.13 -0.01 -0.06 0.02 -0.07 ...
  0.05 0.08 0.01 -0.03 -0.05 0.06 0.04 -0.10 -0.03 0.09 ...
  0.02 -0.03 -0.02 0.11 0.16 0.09 -0.06 0.06 -0.09 0.05 ...
  -0.07 -0.07 -0.10 -0.07 -0.10 -0.13 -0.09 0.03 -0.04 0.02 ...
  -0.07 -0.04 0.20 0.05 -0.02 0.12 0.06 0.04 -0.04 0.01 ...
  -0.09 -0.04 -0.14 -0.09 -0.02 -0.08 -0.09 0.05 0.05 -0.03 ...
  0.07 0.01 0.05 -0.01 0.06 -0.01 0.13 -0.04 0.03 -0.02 ...
  0.02 -0.02 0.01 -0.08 0.03 0.01 -0.10 -0.03 0.08 -0.05 ...
  0.04 -0.05 -0.07 0.09 -0.00 -0.05 0.10 -0.09 -0.01 0.01 ...
```
- `python3 softmax_classification_sgd.py --batch_size=1 --iterations=10 --learning_rate=0.005 --test_size=1597`
```
After iteration 1: train loss 1.7683 acc 73.5%, test loss 2.0028 acc 72.2%
After iteration 2: train loss 0.7731 acc 88.5%, test loss 1.5349 acc 77.8%
After iteration 3: train loss 1.2201 acc 82.5%, test loss 2.0733 acc 73.7%
After iteration 4: train loss 1.1711 acc 89.5%, test loss 2.3419 acc 79.0%
After iteration 5: train loss 0.1996 acc 97.5%, test loss 0.7725 acc 89.1%
After iteration 6: train loss 0.6093 acc 92.5%, test loss 1.6294 acc 81.0%
After iteration 7: train loss 0.1480 acc 97.5%, test loss 0.8089 acc 87.6%
After iteration 8: train loss 0.0691 acc 97.5%, test loss 1.3602 acc 83.4%
After iteration 9: train loss 0.0346 acc 99.0%, test loss 0.7549 acc 89.9%
After iteration 10: train loss 0.0631 acc 98.5%, test loss 0.7471 acc 90.6%
Learned weights:
  -0.03 -0.10 0.04 0.14 0.10 0.01 -0.07 0.05 0.07 -0.15 ...
  0.09 0.10 -0.30 -0.23 -0.35 0.29 -0.13 -0.06 0.02 -0.12 ...
  0.05 0.07 0.08 -0.01 -0.13 0.02 0.02 -0.10 -0.04 0.33 ...
  0.02 -0.01 0.15 0.27 0.16 0.21 -0.11 0.06 -0.08 0.19 ...
  -0.07 -0.10 -0.09 -0.32 -0.25 -0.58 -0.22 0.04 -0.04 0.07 ...
  -0.07 -0.08 0.37 0.15 0.06 0.46 0.12 0.04 -0.03 0.11 ...
  -0.09 -0.06 -0.34 -0.10 0.12 0.02 -0.09 0.05 0.05 -0.11 ...
  0.07 0.02 0.07 0.08 0.15 0.09 0.25 -0.08 0.03 -0.07 ...
  0.02 -0.01 -0.08 -0.07 -0.06 -0.16 -0.15 -0.03 0.08 -0.22 ...
  0.04 -0.04 -0.10 0.09 0.01 -0.21 0.31 -0.08 -0.01 -0.12 ...
```
- `python3 softmax_classification_sgd.py --batch_size=100 --iterations=10 --learning_rate=0.05`
```
After iteration 1: train loss 4.1126 acc 77.8%, test loss 4.2883 acc 75.5%
After iteration 2: train loss 0.4290 acc 90.5%, test loss 0.5414 acc 89.8%
After iteration 3: train loss 0.6189 acc 88.0%, test loss 0.5752 acc 89.2%
After iteration 4: train loss 0.3084 acc 91.9%, test loss 0.3482 acc 91.3%
After iteration 5: train loss 0.2757 acc 93.2%, test loss 0.3792 acc 91.3%
After iteration 6: train loss 0.2559 acc 92.7%, test loss 0.3718 acc 91.8%
After iteration 7: train loss 0.1164 acc 96.8%, test loss 0.1761 acc 95.1%
After iteration 8: train loss 0.2891 acc 91.5%, test loss 0.4110 acc 90.2%
After iteration 9: train loss 0.1256 acc 96.4%, test loss 0.1977 acc 94.9%
After iteration 10: train loss 0.1239 acc 96.3%, test loss 0.1847 acc 95.0%
Learned weights:
  -0.03 -0.10 -0.05 0.07 -0.08 -0.04 -0.06 0.05 0.07 -0.12 ...
  0.09 0.05 -0.24 -0.03 -0.25 0.16 -0.01 -0.06 0.02 -0.13 ...
  0.05 0.10 0.05 -0.02 -0.06 0.04 0.03 -0.10 -0.03 0.16 ...
  0.02 -0.03 0.03 0.15 0.25 0.13 -0.09 0.06 -0.09 0.11 ...
  -0.07 -0.08 -0.13 -0.10 -0.10 -0.18 -0.11 0.03 -0.04 0.00 ...
  -0.07 -0.02 0.32 0.06 0.03 0.23 0.10 0.04 -0.03 0.03 ...
  -0.09 -0.04 -0.18 -0.12 -0.01 -0.12 -0.10 0.05 0.04 -0.06 ...
  0.07 0.01 0.10 0.00 0.05 0.05 0.20 -0.02 0.03 -0.02 ...
  0.02 -0.03 -0.04 -0.12 0.02 -0.02 -0.15 -0.04 0.08 -0.08 ...
  0.04 -0.06 -0.06 0.12 -0.04 -0.10 0.12 -0.09 -0.01 0.01 ...
```
#### Examples End:
#### Tests Start: softmax_classification_sgd_tests
_Note that your results may be slightly different (because of varying floating point arithmetic on your CPU)._
- `python3 softmax_classification_sgd.py --batch_size=10  --iterations=2 --learning_rate=0.005`
```
After iteration 1: train loss 0.3130 acc 90.8%, test loss 0.3529 acc 88.7%
After iteration 2: train loss 0.2134 acc 93.9%, test loss 0.2450 acc 92.5%
Learned weights:
  -0.03 -0.10 0.01 0.06 -0.07 0.04 -0.05 0.05 0.07 -0.10 ...
  0.09 0.08 -0.12 -0.08 -0.10 0.09 -0.03 -0.06 0.02 -0.01 ...
  0.05 0.07 0.01 -0.03 -0.05 0.06 0.04 -0.10 -0.03 0.08 ...
  0.02 -0.05 -0.01 0.10 0.11 0.09 -0.05 0.06 -0.09 0.04 ...
  -0.07 -0.07 -0.10 -0.01 -0.06 -0.07 -0.08 0.04 -0.04 0.01 ...
  -0.07 -0.05 0.14 0.06 0.02 0.14 0.05 0.04 -0.04 0.03 ...
  -0.09 -0.04 -0.11 -0.06 -0.04 -0.10 -0.09 0.05 0.05 -0.01 ...
  0.07 0.01 0.02 -0.04 0.04 -0.01 0.11 -0.06 0.03 -0.03 ...
  0.02 -0.02 0.01 -0.03 0.00 -0.03 -0.09 -0.03 0.08 -0.07 ...
  0.04 -0.04 -0.05 0.05 -0.04 -0.05 0.09 -0.08 -0.01 -0.04 ...
```
- `python3 softmax_classification_sgd.py --batch_size=1   --iterations=1 --learning_rate=0.005 --test_size=1597`
```
After iteration 1: train loss 1.7683 acc 73.5%, test loss 2.0028 acc 72.2%
Learned weights:
  -0.03 -0.10 0.03 0.08 0.03 0.03 -0.07 0.05 0.07 -0.15 ...
  0.09 0.08 -0.25 -0.15 -0.17 0.11 -0.00 -0.06 0.02 -0.05 ...
  0.05 0.06 0.07 0.04 -0.12 0.11 0.07 -0.10 -0.03 0.16 ...
  0.02 -0.03 0.03 0.14 0.03 0.08 -0.09 0.06 -0.09 0.09 ...
  -0.07 -0.08 -0.22 -0.07 -0.11 -0.27 -0.13 0.04 -0.04 -0.00 ...
  -0.07 -0.08 0.17 0.16 0.17 0.39 0.07 0.04 -0.03 0.03 ...
  -0.09 -0.04 -0.13 -0.10 -0.03 -0.16 -0.09 0.05 0.05 -0.03 ...
  0.07 0.02 0.10 0.03 0.09 -0.05 0.13 -0.08 0.03 -0.05 ...
  0.02 0.00 0.02 -0.17 -0.01 -0.04 -0.12 -0.03 0.08 -0.09 ...
  0.04 -0.04 -0.02 0.06 -0.07 -0.05 0.16 -0.08 -0.01 -0.01 ...
```
- `python3 softmax_classification_sgd.py --batch_size=100 --iterations=3 --learning_rate=0.05`
```
After iteration 1: train loss 4.1126 acc 77.8%, test loss 4.2883 acc 75.5%
After iteration 2: train loss 0.4290 acc 90.5%, test loss 0.5414 acc 89.8%
After iteration 3: train loss 0.6189 acc 88.0%, test loss 0.5752 acc 89.2%
Learned weights:
  -0.03 -0.10 -0.04 0.08 -0.07 -0.02 -0.05 0.05 0.07 -0.12 ...
  0.09 0.06 -0.23 -0.08 -0.12 0.11 -0.04 -0.06 0.02 -0.09 ...
  0.05 0.09 0.07 -0.01 -0.08 0.01 0.02 -0.10 -0.03 0.16 ...
  0.02 -0.04 0.03 0.15 0.18 0.09 -0.09 0.06 -0.09 0.10 ...
  -0.07 -0.07 -0.14 -0.07 -0.07 -0.13 -0.11 0.03 -0.04 -0.01 ...
  -0.07 -0.03 0.28 0.07 0.06 0.29 0.11 0.04 -0.03 0.08 ...
  -0.09 -0.04 -0.16 -0.09 -0.04 -0.13 -0.09 0.05 0.04 -0.05 ...
  0.07 0.01 0.06 -0.01 0.05 0.06 0.18 -0.04 0.03 -0.03 ...
  0.02 -0.03 -0.03 -0.08 -0.00 -0.03 -0.12 -0.04 0.08 -0.12 ...
  0.04 -0.05 -0.04 0.06 -0.09 -0.09 0.12 -0.09 -0.01 -0.03 ...
```
#### Tests End:
