### Lecture: 11. SVD, PCA, k-means
#### Date: Dec 9
#### Slides: https://ufal.mff.cuni.cz/~courses/npfl129/2425/slides/?11
#### Reading: https://ufal.mff.cuni.cz/~courses/npfl129/2425/slides.pdf/npfl129-2425-11.pdf,PDF Slides
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl129/2425/npfl129-2425-11-czech.mp4, CS Lecture
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl129/2425/npfl129-2425-11-english.mp4, EN Lecture
#### Lecture assignment: pca
#### Lecture assignment: kmeans
#### Lecture assignment: nli_competition
#### Questions: #lecture_11_questions

**Learning objectives.** After the lecture you should be able to

- Theoretically explain Singular Value Decomposition (SVD), prove it exists and explain what the Eckart-Young theorem says.

- Theoretically explain Principal Component Analysis (PCA) and say how it explains the variance in the data based on SVD.

- Use SVD or PCA for dimensionality reduction, data visualization and data whitening.

- Implement the $k$-means algorithm and use it for clustering.

**Covered topics** and where to find more:

- Singular value decomposition [Gilbert Strang: Linear Algebra and Learning from Data. Wellesley- Cambridge Press, 2019. Chapters I.8, I.9.]

   - Lectures 4-7 in the [corresponding course from MIT Courseware](https://www.youtube.com/playlist?list=PLUl4u3cNGP63oMNUHXqIUcrkS2PivhN3k)

- Principal component analysis [Sections 12.1 and 12.4.2 of PRML]

- Power iteration algorithm

- K-Means clustering [Section 9.1 of PRML]

After the lecture: short and non-comprehensive [**recap quiz**](http://quest.ms.mff.cuni.cz/class-quiz/quiz/ml_intro_lect11).
