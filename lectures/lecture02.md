### Lecture: 2. Linear Regression, SGD
#### Date: Oct 7
#### Slides: https://ufal.mff.cuni.cz/~courses/npfl129/2425/slides/?02
#### Reading: https://ufal.mff.cuni.cz/~courses/npfl129/2425/slides.pdf/npfl129-2425-02.pdf, PDF Slides
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl129/2425/npfl129-2425-02-czech.mp4, CS Lecture
#### Lecture assignment: linear_regression_l2
#### Lecture assignment: linear_regression_sgd
#### Lecture assignment: feature_engineering
#### Lecture assignment: rental_competition
#### Questions: #lecture_2_questions

**Learning objectives.** After the lecture you should be able to

- Reason about **overfitting** in terms of **model capacity**.
- Use **$L^2$-regularization** to control model capacity.
- Explain what the difference between **parameters and hyperparameters** is.
- Tell what the **basic probability concepts** are (joint, marginal, conditional probability; expected value, mean, variance).
- Mathematically describe and implement the **stochastic gradient descent** algorithm.
- Use both **numerical and categorical features** in linear regression.

**Covered topics** and where to find more:

- L2 regularization in linear regression [Section 1.1, 3.1.4 of PRML]
- Random variables and probability distributions [Section 1.2, 1.2.1 of PRML]
- Expectation and variance [Section 1.2.2 of PRML]
- Gradient descent [Section 5.2.4 of PRML]
  - Stochastic gradient descent solution of linear regression
- [Linear regression demo](https://mlu-explain.github.io/linear-regression) by Jared Willber
- [Why Momentum Really Works](https://distill.pub/2017/momentum/) by Gabriel Goh
- [IPython notebook on momentum](https://github.com/ufal/npfl129/blob/past-2425/notebooks/gradient.ipynb)

Due to technical issues when processing the video from the English lecture, the video is unavailable. Please watch the video from the previous runs of the course.

After the lecture: short and non-comprehensive [**recap quiz**](http://quest.ms.mff.cuni.cz/class-quiz/quiz/ml_intro_lect02).
