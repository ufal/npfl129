### Lecture: 9. Decision Trees, Random Forests
#### Date: Nov 28
#### Slides: https://ufal.mff.cuni.cz/~courses/npfl129/2425/slides/?09
#### Reading: https://ufal.mff.cuni.cz/~courses/npfl129/2425/slides.pdf/npfl129-2425-09.pdf,PDF Slides
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl129/2425/npfl129-2425-09-english.mp4, EN Lecture
#### Lecture assignment: decision_tree
#### Lecture assignment: random_forest
#### Questions: #lecture_9_questions

**Learning objectives.** After the lecture you should be able to

- Implement Decision Trees and Random Forests for classification and regression

- Explain how the splitting criterion depend on optimized loss function

- Tell how Random Forests differ from Gradient Boosted Decision Trees

**Covered topics** and where to find more:

- Decision trees [Section 14.4 of PRML]
  - [Decision trees demo](https://mlu-explain.github.io/decision-tree/) by Jared Wilber & Lucía Santamaría

- Random forests
  - [Random forets demo](https://mlu-explain.github.io/random-forest/) by Jenny Yeon & Jared Wilber

Recording of the Czech lecture failed, please use the English recording instead. Assignments for the 9th lecture were discussed jointly with the assignments from the 8th lecture and discussed in the video from the previous practicals.

After the lecture: short and non-comprehensive [**recap quiz**](http://quest.ms.mff.cuni.cz/class-quiz/quiz/ml_intro_lect09).
